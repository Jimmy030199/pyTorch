import os
# ç”¨ä¾†è§£æå‘½ä»¤åˆ—åƒæ•¸ï¼Œè®“ç¨‹å¼åŸ·è¡Œæ™‚å¯ä»¥ç”¨ä¸åŒåƒæ•¸è¨­å®š
import argparse 
from torchvision import datasets, transforms
from torch.utils.data import DataLoader
import torch
import torch.nn as nn
import torch.optim as optim
from cnn import CNN   # å‡è¨­ä½ æŠŠ CNN é¡åˆ¥å­˜æˆ cnn.py
import time
from torch.utils.data import random_split, DataLoader


def main():
    # ===== Step1: è®“å‘½ä»¤åˆ— å¯ä»¥è¼¸å…¥åƒæ•¸ =====
    # é€™æ®µç¨‹å¼ç¢¼çš„åŠŸèƒ½æ˜¯è®“ä½ çš„ç¨‹å¼å¯ä»¥å¾å‘½ä»¤åˆ—æ¥æ”¶ è³‡æ–™å¤¾è·¯å¾‘ã€è¨“ç·´è¨­å®šï¼ˆepochs, batch_size, lrï¼‰ ç­‰åƒæ•¸ï¼Œ
    # ä¸¦è‡ªå‹•å»ºç«‹éœ€è¦çš„è³‡æ–™å¤¾ã€‚é€™æ¨£å¯ä»¥é¿å…å¯«æ­»åƒæ•¸ï¼Œè®“ç¨‹å¼æ›´éˆæ´»ã€‚

    #  é€™è¡Œä½¿ç”¨ argparse å¥—ä»¶ï¼Œå»ºç«‹ä¸€å€‹è§£æå™¨ç‰©ä»¶ï¼Œå¯ä»¥è®“ä½ çš„ç¨‹å¼æ”¯æ´ å‘½ä»¤åˆ—åƒæ•¸ã€‚
    # ğŸ‘‰ ä¾‹å¦‚ä½ åŸ·è¡Œç¨‹å¼æ™‚å¯ä»¥é€™æ¨£è¼¸å…¥ï¼špython train.py --epochs 10 --lr 0.01
    parser = argparse.ArgumentParser()


    #é€™è£¡å®šç¾©äº† 5 å€‹åƒæ•¸ï¼š
    # --data_dirï¼šå­—ä¸²å‹åˆ¥ï¼Œé è¨­å€¼ "data"ï¼Œç”¨ä¾†æŒ‡å®šå­˜æ”¾æˆ–è®€å–è³‡æ–™é›†çš„ç›®éŒ„ã€‚
    # --out_dirï¼šå­—ä¸²å‹åˆ¥ï¼Œé è¨­å€¼ "result"ï¼Œç”¨ä¾†æŒ‡å®šè¼¸å‡ºæ¨¡å‹æˆ–è¨“ç·´çµæœçš„ç›®éŒ„ã€‚
    # --epochsï¼šæ•´æ•¸å‹åˆ¥ï¼Œé è¨­å€¼ 5ï¼Œè¡¨ç¤ºè¨“ç·´è¿´åœˆè¦è·‘å¹¾æ¬¡ã€‚
    # --batch_sizeï¼šæ•´æ•¸å‹åˆ¥ï¼Œé è¨­å€¼ 128ï¼Œä¸€å€‹è¨“ç·´æ‰¹æ¬¡çš„è³‡æ–™é‡ã€‚
    # --lrï¼šæµ®é»æ•¸å‹åˆ¥ï¼Œé è¨­å€¼ 1e-3 (ä¹Ÿå°±æ˜¯ 0.001)ï¼Œå­¸ç¿’ç‡ã€‚
    # ğŸ“Œ help åƒæ•¸çš„æ–‡å­—æœƒåœ¨ä½¿ç”¨ python train.py --help æ™‚é¡¯ç¤ºï¼Œæ–¹ä¾¿ä½¿ç”¨è€…ç†è§£
    parser.add_argument('--data_dir', type=str, default='data', help='è³‡æ–™ä¸‹è¼‰/è®€å–ç›®éŒ„')
    parser.add_argument('--out_dir', type=str, default='result', help='è¨“ç·´è¼¸å‡ºç›®éŒ„')
    parser.add_argument('--epochs', type=int, default=10,help='è¨“ç·´å›åˆæ•¸')
    parser.add_argument('--batch_size', type=int, default=128,help='æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--lr', type=float, default=1e-3,help='å­¸ç¿’ç‡')

    # é€™è¡Œæœƒè®€å–ä½ åœ¨å‘½ä»¤åˆ—è¼¸å…¥çš„åƒæ•¸ï¼Œä¸¦å­˜é€² args ç‰©ä»¶ã€‚
    args = parser.parse_args()

    os.makedirs(args.data_dir, exist_ok=True)
    os.makedirs(args.out_dir, exist_ok=True)

    print(f"data_dir={args.data_dir}, out_dir={args.out_dir}")



    # ===== Step2: è¼‰å…¥è³‡æ–™é›† =====
    # torchvision.transforms
    # é€™å€‹æ¨¡çµ„æä¾›äº†è³‡æ–™è½‰æ› (preprocessing/augmentation) çš„å·¥å…·ï¼Œé€šå¸¸æœƒåœ¨è®€å…¥åœ–ç‰‡æ™‚åŒæ™‚åšè™•ç†ã€‚

    # å®šç¾©è³‡æ–™å‰è™•ç† (transforms)

    # transforms.ToTensor()
    # æŠŠ PIL åœ–ç‰‡æˆ– numpy array è½‰æˆ PyTorch Tensorã€‚
    # åŒæ™‚æœƒæŠŠåƒç´ å€¼å¾ 0~255 ç¸®æ”¾åˆ° 0~1 çš„ç¯„åœã€‚
    # ä¾‹å¦‚ï¼šåŸå§‹åƒç´  128 â†’ 128/255 â‰ˆ 0.502ã€‚

    # transforms.Normalize((0.5,), (0.5,))
    # é€™æ¨£åšçš„ç›®çš„æ˜¯ï¼šè®“è³‡æ–™ å¹³å‡å€¼é è¿‘ 0ã€åˆ†å¸ƒå¤§è‡´åœ¨ [-1,1]ï¼Œæœ‰åŠ©æ–¼æ¨¡å‹æ›´å¿«æ”¶æ–‚ã€‚
    # RGB å½©è‰²åœ–ç‰‡ (3 å€‹é€šé“)ï¼šå°±è¦æä¾›ä¸‰å€‹å€¼ã€‚
    # ä¾‹å¦‚æœ€å¸¸è¦‹çš„è¨­å®šï¼š
    # transforms.Normalize((0.5, 0.5, 0.5),   # æ¯å€‹é€šé“çš„ mean
    #                      (0.5, 0.5, 0.5))   # æ¯å€‹é€šé“çš„ std
    # é€™æ¨£ R/G/B ä¸‰å€‹é€šé“éƒ½æœƒåš (x - 0.5)/0.5 â†’ [-1, 1]ã€‚
    tfm = transforms.Compose([
        transforms.ToTensor(),                       # å°‡åœ–ç‰‡è½‰æˆ tensor (0~1 æµ®é»æ•¸)
        transforms.Normalize((0.5,), (0.5,))         # ç°éšå–®é€šé“æ¨™æº–åŒ– (å¹³å‡0.5, æ¨™æº–å·®0.5)
    ])

    # # è¼‰å…¥ Fashion-MNIST è¨“ç·´è³‡æ–™é›†
    # train_dataset = datasets.FashionMNIST(
    #     root=args.data_dir,       # è³‡æ–™å­˜æ”¾ä½ç½® (å‰é¢ Step1 å®šç¾©çš„åƒæ•¸)
    #     train=True,               # è¼‰å…¥è¨“ç·´é›† (True=è¨“ç·´, False=æ¸¬è©¦)
    #     download=False,           # è‹¥æ²’æœ‰è³‡æ–™å°±è‡ªå‹•ä¸‹è¼‰
    #     transform=tfm             # å¥—ç”¨å‰è™•ç† (tensor + normalize)
    # )

    # # å°å‡ºè¨“ç·´è³‡æ–™é›†å¤§å° & å­˜æ”¾ä½ç½®
    # print(f"è¨“ç·´é›†ç­†æ•¸: {len(train_dataset)} (ä¸‹è¼‰ä½ç½®: {args.data_dir})")

    # è¼‰å…¥ Fashion-MNIST è¨“ç·´é›†
    full_train = datasets.FashionMNIST(
        root=args.data_dir,
        train=True,
        download=True,
        transform=tfm
    )

    # åˆ†å‰²è³‡æ–™é›†ç‚ºè¨“ç·´èˆ‡é©—è­‰
    val_len = 10000
    train_len = len(full_train) - val_len
    train_set, val_set = random_split(full_train, [train_len, val_len])

    # æŒ‡å®šé‹ç®—è£ç½®
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    # ===== Step3: å»ºç«‹ DataLoader =====

    # num_workers=2
    # ç”¨å¤šå°‘å€‹å­ç¨‹åºå¹³è¡Œè¼‰å…¥è³‡æ–™ã€‚
    # æ•¸å­—è¶Šå¤§ â†’ è³‡æ–™è®€å–è¶Šå¿«ï¼Œä½†ä¹Ÿåƒæ›´å¤š CPUã€‚
    # åœ¨ Windows ä¸Šæœ‰æ™‚è¦å°å¿ƒï¼Œnum_workers>0 éœ€è¦ if __name__ == "__main__": åŒ…èµ·ä¾†ï¼Œä¸ç„¶å¯èƒ½æœƒå¡ä½

    # pin_memory=True
    # ç•¶ä½ åœ¨ GPU ä¸Šè¨“ç·´æ™‚ï¼ŒæœƒæŠŠ batch å›ºå®šåœ¨ page-locked memoryï¼ŒåŠ é€Ÿ CPU â†’ GPU çš„è³‡æ–™å‚³è¼¸ã€‚
    # åœ¨ CUDA ç’°å¢ƒä¸‹é€šå¸¸å»ºè­°é–‹å•Ÿã€‚

    # train_loader = DataLoader(
    #     train_dataset,
    #     batch_size=args.batch_size,
    #     shuffle=True,       # æ¯å€‹ epoch æ‰“äº‚è³‡æ–™
    #     num_workers=2,      # ä½¿ç”¨å…©å€‹å­ç¨‹åºåŠ é€Ÿè®€å–
    #     pin_memory=True     # åœ¨ GPU ä¸Šè¨“ç·´æ™‚å»ºè­°é–‹å•Ÿ
    # )
    # print(f"DataLoader å°±ç·’ (batch_size={args.batch_size})")

    # å»ºç«‹ DataLoader
    train_loader = DataLoader(
        train_set, batch_size=args.batch_size,
        shuffle=True, num_workers=2, pin_memory=True
    )

    val_loader = DataLoader(
        val_set, batch_size=args.batch_size,
        shuffle=False, num_workers=2, pin_memory=True
    )

    # é¸æ“‡è£ç½® (GPU å„ªå…ˆï¼Œå¦å‰‡ç”¨ CPU)
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    # å»ºç«‹æ¨¡å‹ (Fashion-MNIST æœ‰ 10 é¡)
    model = CNN(num_classes=10).to(device)
    print(f"ä½¿ç”¨è£ç½®: {device}")

    # æå¤±å‡½æ•¸ (CrossEntropyLoss: çµåˆ softmax + NLLLoss)
    criterion = nn.CrossEntropyLoss()

    # å„ªåŒ–å™¨ (Adam)
    optimizer = optim.Adam(model.parameters(), lr=args.lr)

    print("[S5] Loss/Optimizer æº–å‚™å®Œæˆ")

    # è¨“ç·´æå¤±è¼¸å‡º CSV æª”æ¡ˆ
    loss_csv = os.path.join(args.out_dir, 'loss.csv')
    with open(loss_csv, 'w', encoding='utf-8') as f:
        f.write('epoch,train_loss\n')
    best_val_loss=float('inf')
    best_path=os.path.join(args.out_dir,'best_cnn.pth')
    
    train_acc = 0.0
    # é–‹å§‹è¨“ç·´
    for epoch in range(1, args.epochs + 1):
        t0 = time.time()
        model.train()    # åˆ‡æ›åˆ°è¨“ç·´æ¨¡å¼ (å•Ÿç”¨ dropout, BN)
        running_loss, n = 0.0, 0

        for images, labels in train_loader:
            images = images.to(device)
            labels = labels.to(device).long()   # CrossEntropyLoss éœ€è¦ int64

            # Forward
            logits = model(images)              # æ¨¡å‹è¼¸å‡º [batch, 10] è¼¸å‡ºçš„ã€ŒåŸå§‹åˆ†æ•¸ã€
            loss = criterion(logits, labels)    # è¨ˆç®—æå¤± (æœª softmax)

            # Backward
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            # ç´¯ç©æå¤± (ä¹˜ batch sizeï¼Œæ›ç®—æˆç¸½å’Œ)

            # labels.size(0) å–å¾—é€™å€‹ batch çš„å¤§å°ï¼ˆbatch sizeï¼‰ã€‚
            # å‡è¨­ labels.shape = [128]ï¼Œé‚£ bs = 128ã€‚
            # ğŸ‘‰ é€™æ¨£ä¸ç®¡æœ€å¾Œ batch æœ‰å¹¾ç­†ï¼Œéƒ½èƒ½å‹•æ…‹å–å¾—æ•¸é‡ã€‚
            bs = labels.size(0)

            
            # å‡è¨­ batch loss = 0.5678
            # lossï¼štensor(0.5678, grad_fn=<NllLossBackward0>)ï¼ˆå¯ä»¥åš backwardï¼‰
            # loss.item()ï¼š0.5678ï¼ˆç´”æ•¸å­—ï¼Œå¯ä»¥åŠ ç¸½çµ±è¨ˆï¼‰
            running_loss += loss.item() * bs
            n += bs
        
        
        
        # è¨ˆç®—å¹³å‡è¨“ç·´æå¤±
        train_loss = running_loss / n
        train_acc=train_acc / n
        # --- validate ---
        model.eval()
        val_loss, val_acc = 0.0, 0.0

        with torch.no_grad():
            for images, labels in val_loader:
                images, labels = images.to(device), labels.to(device)

                logits = model(images)
                loss = criterion(logits, labels)

                # è¨ˆç®— batch çš„ loss èˆ‡æ­£ç¢ºç‡
                val_loss += loss.item()
                val_acc += (logits.argmax(1) == labels).float().sum().item()

        # å¹³å‡ lossã€acc
        val_loss /= len(val_loader)
        val_acc /= len(val_loader.dataset)

        # å°‡çµæœå¯«å…¥ csv æª”
        with open(loss_csv, 'a', encoding='utf-8') as f:
            f.write(f"{epoch},{train_loss:.6f},{val_loss:.6f},{train_acc:.4f},{val_acc:.4f}\n")

        # é¡¯ç¤ºç›®å‰ epoch çš„çµæœ
        print(f"[Epoch {epoch:02d}] "
            f"train_loss={train_loss:.4f} val_loss={val_loss:.4f} "
            f"train_acc={train_acc:.4f} val_acc={val_acc:.4f}")

        # è‹¥ validation loss ä¸‹é™ï¼Œå‰‡å„²å­˜æœ€å¥½çš„æ¨¡å‹
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            torch.save({
                'epoch': epoch,
                'model_state': model.state_dict(),
                'optimizer_state': optimizer.state_dict(),
                'val_loss': val_loss,
                'val_acc': val_acc,
            }, best_path)
            print(f"--> Saved new best to {best_path}")


        # è¨˜éŒ„åˆ° CSV
        with open(loss_csv, 'a', encoding='utf-8') as f:
            f.write(f'{epoch},{train_loss:.6f}\n')

        # å°å‡ºæ¯å€‹ epoch çµæœ
        dt = time.time() - t0
        print(f"[Epoch {epoch:02d}] train_loss={train_loss:.4f} ({dt:.1f}s)")

    print(f"âœ… è¨“ç·´å®Œæˆï¼Œloss å·²å¯«å…¥: {loss_csv}")


if __name__ == '__main__':
    main()






