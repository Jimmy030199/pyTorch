{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Step1 載入資料與探索\n",
      "\n",
      " 前5筆資料\n",
      "   sepal_length  sepal_width  petal_length  petal_width  target\n",
      "0           5.1          3.5           1.4          0.2       0\n",
      "1           4.9          3.0           1.4          0.2       0\n",
      "2           4.7          3.2           1.3          0.2       0\n",
      "3           4.6          3.1           1.5          0.2       0\n",
      "4           5.0          3.6           1.4          0.2       0\n",
      "\n",
      " 類別分布:\n",
      "0=setosa     : 50 筆\n",
      "1=versicolor : 50 筆\n",
      "2=virginica  : 50 筆\n",
      "\n",
      "[原始資料(未標準化)]\n",
      "sepal_length   mean=  5.8433 std=  0.8253\n",
      "sepal_width    mean=  3.0573 std=  0.4344\n",
      "petal_length   mean=  3.7580 std=  1.7594\n",
      "petal_width    mean=  1.1993 std=  0.7597\n",
      "\n",
      "->已存取20筆預覽 :iris_course\\artifacts\\iris_preview.csv\n",
      "STEP1 完成\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing  import List\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "ROOT= \"iris_course\"\n",
    "ARTIFACTS = os.path.join(ROOT,'artifacts')\n",
    "os.makedirs(ARTIFACTS,exist_ok=True)\n",
    "\n",
    "# axis=0 與 axis=1 差別\n",
    "# print(\"axis=0：每欄平均\", X.mean(axis=0))\n",
    "# print(\"axis=1：每列平均\", X.mean(axis=1))\n",
    "\n",
    "# 這段的作用\n",
    "# 這段是 把特徵名稱 (names)、平均值 (m)、標準差 (s) 串在一起逐一輸出。\n",
    "\n",
    "# 📌 詳細拆解\n",
    "# zip(names, m, s)\n",
    "# names 是特徵名稱的字串清單，例如：\n",
    "# [\"sepal_length\",\"sepal_width\",\"petal_length\",\"petal_width\"]\n",
    "\n",
    "# m 是各欄的平均值陣列，例如：\n",
    "# [5.84, 3.05, 3.76, 1.20]\n",
    "\n",
    "# s 是各欄的標準差陣列，例如：\n",
    "# [0.82, 0.43, 1.76, 0.76]\n",
    "\n",
    "# zip() 會把它們一一對應打包成 tuple：\n",
    "# [\n",
    "#   (\"sepal_length\", 5.84, 0.82),\n",
    "#   (\"sepal_width\",  3.05, 0.43),\n",
    "#   ...\n",
    "# ]\n",
    "\n",
    "# 格式\t     說明\n",
    "# {n:<14s}\t左對齊字串，寬度 14\n",
    "# {mi:8.4f}\t浮點數，總寬度 8，顯示 4 位小數\n",
    "# {sd:8.4f}\t同上\n",
    "\n",
    "def describe_stats(X:np.ndarray,names:List[str],title:str):\n",
    "    m,s = X.mean(axis=0), X.std(axis=0)\n",
    "    print(f\"\\n[{title}]\")\n",
    "    for n, mi, sd in zip(names, m,s):\n",
    "        print(f\"{n:<14s} mean={mi:8.4f} std={sd:8.4f}\" )\n",
    "\n",
    "print(\"***Step1 載入資料與探索\")\n",
    "iris =load_iris()\n",
    "# print(iris)\n",
    "\n",
    "\n",
    "# 載入資料集：\n",
    "# x 是 (150,4) 的數值矩陣\n",
    "# y 是 (150,) 的標籤（0,1,2）\n",
    "# 這四個特徵分別是：花萼長寬、花瓣長寬\n",
    "x,y = iris.data,iris.target\n",
    "\n",
    "# sepal 萼片\n",
    "# petal 花瓣\n",
    "feature_names = [\"sepal_length\",\"sepal_width\",\"petal_length\",\"petal_width\"]\n",
    "target_names=iris.target_names.tolist()\n",
    "# print(target_names)\n",
    "\n",
    "# 這行會用 pandas 把 x 轉成一個 DataFrame（表格格式），欄名就是特徵名稱：\n",
    "df=pd.DataFrame(x,columns=feature_names)\n",
    "df[\"target\"] = y\n",
    "print(\"\\n 前5筆資料\");print(df.head())\n",
    "print(\"\\n 類別分布:\")\n",
    "\n",
    "\n",
    "# enumerate() 是 Python 內建函式\n",
    "# 它的作用是：在迴圈中同時取得「索引」和「元素」\n",
    "\n",
    "# 小總結\n",
    "#表達式\t                           意思\t結果型態\n",
    "# y\t類別標籤陣列\t           ndarray of int\n",
    "# y == i\t          元素逐一是否等於 i\tndarray of bool\n",
    "# (y == i).sum()\t      有幾個等於 i\tint（計數結果）\n",
    "\n",
    "# ✅ 所以雖然 y 是個整數陣列 (ndarray)，\n",
    "# 用 == 去比對數值時，會自動對每個元素做比較，這就是 NumPy 的「向量化運算」。\n",
    "\n",
    "# target_names：只會是 三個獨特的名稱 → ['setosa','versicolor','virginica']\n",
    "for i ,name in enumerate(target_names):\n",
    "    print(f\"{i}={name:<10s} : {(y==i).sum()} 筆\")\n",
    "describe_stats(x,feature_names,\"原始資料(未標準化)\")\n",
    "\n",
    "out_csv=os.path.join(ARTIFACTS,\"iris_preview.csv\")\n",
    "# index=False 表示 不要輸出 DataFrame 的索引欄位（只保留資料本身）\n",
    "df.head(20).to_csv(out_csv,index=False)\n",
    "\n",
    "print(f\"\\n->已存取20筆預覽 :{out_csv}\")\n",
    "print(\"STEP1 完成\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b2949c15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===STEP2 | 切分 Train/Val/Test===\n",
      "切分形狀: train=(96, 4) val=(24, 4) test=(30, 4)\n",
      "STEP2 完成\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "print(\"\\n===STEP2 | 切分 Train/Val/Test===\")\n",
    "\n",
    "\n",
    "\n",
    "# x：所有的特徵資料（numpy.ndarray，shape = (150, 4)）\n",
    "# y：所有的標籤（numpy.ndarray，shape = (150,)）\n",
    "# train_test_split 是 scikit-learn 提供的\n",
    "# train_test_split 函式，用來隨機把資料拆成兩組\n",
    "# 參數解釋\n",
    "# 參數\t                            用途\n",
    "# x, y\t             輸入資料與標籤\n",
    "# test_size=0.2\t    指定 20% 當「測試集」，剩下 80% 是「訓練+驗證」\n",
    "# random_state=42\t設定隨機種子，讓結果可重現\n",
    "# stratify(分層)=y\t    分層抽樣：確保三個類別在兩組中比例一致\n",
    "\n",
    "X_trainval,X_test,y_trainval,y_test =train_test_split(\n",
    "    x,y,test_size=0.2,random_state=42,stratify=y\n",
    ")\n",
    "X_train,X_val,y_train,y_val =train_test_split(\n",
    "    X_trainval,y_trainval,test_size=0.2,random_state=42,stratify=y_trainval\n",
    ")\n",
    "\n",
    "print(f\"切分形狀: train={X_train.shape} val={X_val.shape} test={X_test.shape}\")\n",
    "print(\"STEP2 完成\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "18de2a5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\====STEP 3 | 標準化(只用訓練集fit)並存檔)\n",
      "\n",
      "[訓練集(標準化前)]\n",
      "sepal_length   mean=  5.8333 std=  0.8516\n",
      "sepal_width    mean=  3.0083 std=  0.4264\n",
      "petal_length   mean=  3.7500 std=  1.7530\n",
      "petal_width    mean=  1.1875 std=  0.7463\n",
      "\n",
      "[訓練集(標準化後)]\n",
      "sepal_length   mean=  0.0000 std=  1.0000\n",
      "sepal_width    mean=  0.0000 std=  1.0000\n",
      "petal_length   mean=  0.0000 std=  1.0000\n",
      "petal_width    mean=  0.0000 std=  1.0000\n",
      "->已存標準化資料:iris_course\\artifacts\\train_val_test_scaled.npz\n",
      "->已存標準化器:iris_course\\artifacts\\scaler.pkl\n",
      "STEP3 完成\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import joblib\n",
    "\n",
    "print('\\====STEP 3 | 標準化(只用訓練集fit)並存檔)')\n",
    "\n",
    "# StandardScaler 是一個「轉換器 (transformer)」物件\n",
    "# 它會計算：\n",
    "# 每個欄位的平均值 μ\n",
    "# 每個欄位的標準差 σ\n",
    "# 然後把資料套用公式：\n",
    "\n",
    "# 𝑧 = (x - 𝜇) / 𝜎\n",
    "# 讓 每個特徵（欄）都變成平均 0、標準差 1\t​\n",
    "\n",
    "\n",
    "# 第 1 行：先用訓練資料「學習平均與標準差」\n",
    "# .fit(X_train) 會計算：\n",
    "# 每一欄的平均值 mean_\n",
    "# 每一欄的標準差 scale_\n",
    "# 這一步「只用訓練集」是為了避免資料洩漏（不能偷看驗證或測試資料）\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "\n",
    "# 第 2 行：把訓練資料做標準化\n",
    "# 用剛剛算出的 mean_ 和 scale_ 把資料轉換成：\n",
    "# (原值 - 平均) / 標準差\n",
    "# 結果：每一欄的平均會變成 0、標準差變成 1\n",
    "X_train_sc = scaler.transform(X_train)\n",
    "\n",
    "# 第 3～4 行：用同一個 scaler 處理驗證與測試資料\n",
    "# 這裡 不能再 fit 一次，要用 訓練集的平均與標準差 來轉換\n",
    "# 這樣才確保模型在驗證/測試時使用完全相同的尺度\n",
    "X_val_sc = scaler.transform(X_val)\n",
    "X_test_sc = scaler.transform(X_test)\n",
    "\n",
    "describe_stats(X_train, feature_names,\"訓練集(標準化前)\")\n",
    "describe_stats(X_train_sc, feature_names,\"訓練集(標準化後)\")\n",
    "\n",
    "npz_path= os.path.join(ARTIFACTS,\"train_val_test_scaled.npz\")\n",
    "# .npz 就是 把很多 NumPy 陣列一起打包壓縮存檔\n",
    "# → 讓你之後可以 一次存、一包讀，很方便。\n",
    "\n",
    "# 這行會建立一個 train_val_test_scaled.npz 檔，裡面包含：\n",
    "\n",
    "# 存進去的名稱\t內容\n",
    "# X_train_sc\t標準化後的訓練特徵資料\n",
    "# y_train\t訓練標籤\n",
    "# X_val_sc\t標準化後的驗證特徵資料\n",
    "# y_val\t驗證標籤\n",
    "# X_test_sc\t標準化後的測試特徵資料\n",
    "# y_test\t測試標籤\n",
    "# feature_names\t特徵名稱清單（轉成陣列存）\n",
    "# target_names\t類別名稱清單（轉成陣列存）\n",
    "\n",
    "np.savez(npz_path,\n",
    "         X_train_sc=X_train_sc,y_train=y_train,\n",
    "         X_val_sc=X_val_sc,y_val=y_val,\n",
    "         X_test_sc=X_test_sc,y_test=y_test,\n",
    "         feature_names=np.array(feature_names,dtype=object),\n",
    "         target_names=np.array(target_names,dtype=object),\n",
    ")\n",
    "\n",
    "# 這兩行的目的\n",
    "\n",
    "# 把你訓練好的 StandardScaler 物件\n",
    "# 存成一個檔案（scaler.pkl），\n",
    "# 以後要用時可以直接載回來，不用重新 .fit() 一次。\n",
    "\n",
    "# 把物件存起來\n",
    "# joblib.dump(scaler, scaler_path)\n",
    "# 使用 joblib 的 dump 函式\n",
    "# 把 scaler（也就是你用 X_train .fit() 過的 StandardScaler）存成 .pkl 檔案\n",
    "# .pkl 是「pickle」格式，用來存整個 Python 物件\n",
    "\n",
    "scaler_path =os.path.join(ARTIFACTS,\"scaler.pkl\")\n",
    "joblib.dump(scaler,scaler_path)\n",
    "\n",
    "print(f\"->已存標準化資料:{npz_path}\")\n",
    "print(f\"->已存標準化器:{scaler_path}\")\n",
    "print(\"STEP3 完成\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f154bf7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\====STEP 4 | Tensor 與 DataLoader\n",
      "第一個 batch:xb.shape=torch.Size([16, 4]), yb.shape=torch.Size([16])\n",
      "   xb[0](標準化後)=[-1.0960394144058228, -1.1921887397766113, 0.42784440517425537, 0.6867437362670898]\n",
      "   yb[0](類別)=2\n",
      "->已存batch 預覽iris_course\\artifacts\\batch_preview.csv\n",
      "STEP4 完成\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset,DataLoader\n",
    "# 整體目標\n",
    "# 這段是 STEP4：把資料轉成 torch.Tensor，再包進 DataLoader，用來訓練模型\n",
    "# 是 PyTorch 的標準資料處理流程。\n",
    "# 簡單來說：\n",
    "# 把資料(numpy.ndarray) → 轉成 Tensor → 用 DataLoader 分成小批次（batch） → 之後可以丟給模型訓練\n",
    "print('\\====STEP 4 | Tensor 與 DataLoader')\n",
    "\n",
    "\n",
    "X_train_t = torch.tensor(X_train_sc,dtype=torch.float32)\n",
    "y_train_t = torch.tensor(y_train,dtype=torch.long)\n",
    "X_val_t = torch.tensor(X_val_sc,dtype=torch.float32)\n",
    "y_val_t = torch.tensor(y_val,dtype=torch.long)\n",
    "\n",
    "\n",
    "# TensorDataset(X, y)：把 X 和 y 包成一筆一筆的資料\n",
    "# DataLoader(..., batch_size=16)：每次會吐出 16 筆資料（小批次）\n",
    "# shuffle=True：訓練集會隨機打亂順序（避免模型記住順序）\n",
    "# shuffle=False：驗證集保持原順序\n",
    "# 為什麼驗證/測試集不要 shuffle\n",
    "# 驗證時只是「評估」模型表現，不需要也不應打亂\n",
    "# 保持固定順序 → 方便對照預測與真實標籤\n",
    "# 每次評估結果一致，避免隨機性影響評估\n",
    "train_loader =DataLoader(TensorDataset(X_train_t,y_train_t),batch_size=16,shuffle=True)\n",
    "val_loader =DataLoader(TensorDataset(X_val_t,y_val_t),batch_size=16,shuffle=False)\n",
    "\n",
    "\n",
    "# 目的\n",
    "\n",
    "# 從 DataLoader（PyTorch）\n",
    "# 取出「第一個 batch（小批次）」的資料，\n",
    "# 用來 檢查資料長相是否正確。\n",
    "\n",
    "\n",
    "# train_loader 是你剛建立的 DataLoader，裡面有所有訓練資料（已分好 batch）\n",
    "# iter(train_loader) → 建立一個「迭代器」\n",
    "# next(...) → 從迭代器中取出第一組 (特徵, 標籤)\n",
    "# 結果會是：\n",
    "# xb = 一個 batch 的特徵資料\n",
    "# shape 通常是 (batch_size, 特徵數)\n",
    "# 例如 (16, 4)\n",
    "# yb = 一個 batch 的標籤資料\n",
    "# shape 是 (batch_size,)\n",
    "# 例如 (16,)\n",
    "\n",
    "# 重點觀念\n",
    "# shuffle=True 的 打亂時機不是在你建立 DataLoader 的當下，\n",
    "# 而是在你第一次呼叫 iter(train_loader)（開始一個 epoch）時才打亂資料順序。\n",
    "\n",
    "xb,yb = next(iter(train_loader))\n",
    "print(f\"第一個 batch:xb.shape={xb.shape}, yb.shape={yb.shape}\")\n",
    "\n",
    "# 取出 batch 裡第一筆資料的標籤\n",
    "print(f\"   xb[0](標準化後)={xb[0].tolist()}\")\n",
    "print(f\"   yb[0](類別)={yb[0].item()}\")\n",
    "\n",
    "\n",
    "# 功能（一句話）\n",
    "# 把你剛從 DataLoader 取出的那一個 batch（小批次）\n",
    "# 轉成表格（pandas.DataFrame），加上標籤欄位，\n",
    "# 再存成 CSV 檔，方便你用 Excel 或其他工具檢查。\n",
    "batch_preview=os.path.join(ARTIFACTS,\"batch_preview.csv\")\n",
    "pd.DataFrame(xb.numpy(),columns=feature_names).assign(label=yb.numpy()).to_csv(batch_preview,index=False)\n",
    "print(f\"->已存batch 預覽{batch_preview}\")\n",
    "print(\"STEP4 完成\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "70183213",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\====STEP 5 | 定義模型與參數量\n",
      "IrisMLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=4, out_features=64, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.2, inplace=False)\n",
      "    (3): Linear(in_features=64, out_features=32, bias=True)\n",
      "    (4): ReLU()\n",
      "    (5): Dropout(p=0.2, inplace=False)\n",
      "    (6): Linear(in_features=32, out_features=3, bias=True)\n",
      "  )\n",
      ")\n",
      "可訓練參數量:2,499\n",
      "-> 已存結構描述:iris_course\\models\\model_arch.txt\n",
      "STEP5 完成\n"
     ]
    }
   ],
   "source": [
    "from torch import nn \n",
    "MODELS = os.path.join(ROOT,\"models\")\n",
    "os.makedirs(MODELS,exist_ok=True)\n",
    "\n",
    "print('\\====STEP 5 | 定義模型與參數量')\n",
    "\n",
    "class IrisMLP(nn.Module):\n",
    "    def __init__(self, in_dim=4, hidden1=64,hidden2=32,out_dim=3,dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim,hidden1),nn.ReLU(),nn.Dropout(dropout),\n",
    "            nn.Linear(hidden1,hidden2),nn.ReLU(),nn.Dropout(dropout),\n",
    "            nn.Linear(hidden2,out_dim)\n",
    "        )\n",
    "\n",
    "\n",
    "    #     forward：定義前向傳遞\n",
    "    # def forward(self, x):\n",
    "    #     return self.net(x)\n",
    "\n",
    "\n",
    "    # 告訴 PyTorch 資料要怎麼經過網路\n",
    "\n",
    "    # 只要呼叫 model(x)，就會自動跑這段\n",
    "    def forward(self,x): return self.net(x)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 計算 PyTorch 模型中「可訓練參數」的總數\n",
    "# 也就是：這個模型裡 所有需要更新的權重參數 一共有幾個數值（weights / biases）。\n",
    "\n",
    "# 你問的 -> int 是 Type Hint（型別註解） 的一種，\n",
    "# 不是程式功能的一部分，只是「告訴人或工具：這個函式會回傳什麼型別」。\n",
    "def count_trainable_params(model: nn.Module) -> int:\n",
    "\n",
    "    #     p.numel()\n",
    "    # 回傳這個 Tensor 裡「有幾個元素」\n",
    "    # 例如：\n",
    "    # p.shape = (64, 4) → p.numel() = 256\n",
    "    # p.shape = (64,)   → p.numel() = 64\n",
    "\n",
    "    #     (p.numel() for p in ...) 是 生成器，不是陣列，\n",
    "    # 它會「一個一個產生數字」，讓 sum() 去加總，\n",
    "    # 而不會先把所有數字存在記憶體裡。\n",
    "\n",
    "    # requires_grad 是什麼\n",
    "    # 它的意思是：\n",
    "    # 這個張量是否要在反向傳播（backpropagation）時計算梯度\n",
    "    # 有些參數可能：\n",
    "    # 是凍結的（不想訓練）\n",
    "    # 是固定的 embedding 或預訓練權重\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "\n",
    "# 檢查你的電腦有沒有 NVIDIA GPU（CUDA）可以用，\n",
    "# 然後設定一個 torch.device 物件，\n",
    "# 讓你之後可以把模型或資料移到 GPU 或 CPU 執行。\n",
    "# device =torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# 建立模型物件\n",
    "# 把模型搬到指定的裝置（CPU / GPU）上\n",
    "model = IrisMLP().to(device)\n",
    "print(model)\n",
    "print(f\"可訓練參數量:{count_trainable_params(model):,}\")\n",
    "\n",
    "arch_txt =os.path.join(MODELS,\"model_arch.txt\")\n",
    "\n",
    "# 功能總覽\n",
    "\n",
    "# 把 模型的結構 和 可訓練參數總數\n",
    "# 寫進一個文字檔（arch_txt）\n",
    "\n",
    "# 用 Python 內建的檔案操作\n",
    "# \"w\" → 以「寫入模式」開啟檔案\n",
    "# encoding=\"utf-8\" → 用 UTF-8 編碼（可以正常寫中文）\n",
    "# as f → 建立檔案物件 f\n",
    "# with 會在寫完後自動關閉檔案\n",
    "\n",
    "with open(arch_txt,\"w\",encoding=\"utf-8\") as f:\n",
    "    f.write(str(model) + \"\\n\")\n",
    "    f.write(f\"trainable_params={count_trainable_params(model)}\\n\")\n",
    "print(f\"-> 已存結構描述:{arch_txt}\")\n",
    "print(\"STEP5 完成\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "48be2f56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== STEP 6 訓練 (含早停) ===\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== STEP 6 訓練 (含早停) ===\")\n",
    "\n",
    "# 這是定義「損失函式 (Loss Function)」的部分\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# 這是定義「優化器 (Optimizer)」，也就是更新模型參數的演算法。\n",
    "# Adam：一種改良版的 SGD，會根據歷史梯度的大小自動調整學習率 (learning rate)，收斂速度通常比單純的 SGD 快。\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "def evaluate(m, loader):\n",
    "\n",
    "    # 1️⃣ m.eval()\n",
    "    # 把模型切換到「推論模式」。\n",
    "    # 會停用 Dropout、BatchNorm 等僅訓練用的機制，確保驗證結果穩定。\n",
    "    m.eval()\n",
    "\n",
    "    total, correct, loss_sum = 0, 0, 0.0\n",
    "\n",
    "    # 2️⃣ torch.no_grad()\n",
    "    # 在這個區塊內不會記錄梯度，節省記憶體和運算量。\n",
    "    # 驗證或推論時不需要反向傳播，所以關掉 gradient tracking。\n",
    "    with torch.no_grad():\n",
    "        # 3️⃣ for xb, yb in loader\n",
    "        # 從 DataLoader 一批一批讀資料。\n",
    "        # xb = 輸入 (features)，yb = 標籤 (labels)。\n",
    "        for xb, yb in loader:\n",
    "            # 4️⃣ xb, yb = xb.to(device), yb.to(device)\n",
    "            # 把資料搬到 device（可能是 GPU 或 CPU）。\n",
    "            # 確保資料和模型在同一裝置上，不然會報錯。\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "\n",
    "            #5️⃣ logits = m(xb)\n",
    "            #m → 你的模型 (例如一個 nn.Module 定義的神經網路)。\n",
    "            # xb → 一個 batch 的輸入資料 (features)，通常 shape = [batch_size, in_dim]。\n",
    "            # logits = 模型輸出的「原始分數」(linear output)，還沒有經過 softmax 轉換成機率。\n",
    "            # 前向傳播 (forward pass)，輸出 logits（未經 softmax 的分數）。\n",
    "            # shape 通常是 [batch_size, num_classes]。\n",
    "            logits = m(xb)\n",
    "\n",
    "            print('logits',logits)\n",
    "            print('yb',yb)\n",
    "\n",
    "            # 7️⃣ 損失計算\n",
    "            # loss_sum += criterion(logits, yb).item() * xb.size(0)\n",
    "            # criterion(logits, yb) = 算這個 batch 的平均 loss。\n",
    "            # .item() 取出 Python float。\n",
    "            # 乘上 xb.size(0)（batch size），轉成「總 loss」，方便最後做加權平均。\n",
    "            loss_sum += criterion(logits, yb).item() * xb.size(0)\n",
    "\n",
    "            # 8️⃣ 正確率計算\n",
    "            # correct += (logits.argmax(1) == yb).sum().item()\n",
    "            # argmax = argument of the maximum\n",
    "            # 就是「找出一個序列裡 最大值的位置 (索引)」。\n",
    "            # logits.argmax(1) → 找出每一筆資料預測的類別索引。\n",
    "            # == yb → 和真實標籤比對，回傳 True/False tensor。\n",
    "            # .sum() → 計算這個 batch 預測正確的數量。\n",
    "            correct += (logits.argmax(1) == yb).sum().item()\n",
    "\n",
    "            #9️⃣ 累積樣本數\n",
    "            # total += yb.size(0)\n",
    "            # 紀錄一共看過多少筆樣本。\n",
    "            # 最後才能算平均 loss 和正確率。\n",
    "            total += yb.size(0)\n",
    "    return loss_sum / total, correct / total\n",
    "\n",
    "\n",
    "# 各變數的用途：\n",
    "# best_state = None\n",
    "# 用來存模型目前「最佳狀態」(通常是 state_dict() 的複本)。\n",
    "# 剛開始還沒有最佳模型，所以是 None。\n",
    "\n",
    "# best_val = -1.0\n",
    "# 用來記錄「驗證集 (validation) 的最佳表現」\n",
    "# 設成 -1.0 是因為我們希望後面第一次驗證結果一定會比它好（假設準確率 ≥ 0）。\n",
    "\n",
    "# patience = 15\n",
    "# 代表「容忍連續多少次表現沒有進步」\n",
    "# 如果超過 15 次 epoch 都沒有改善，就觸發 early stopping，停止訓練。\n",
    "\n",
    "# bad = 0\n",
    "# 記錄「已經連續幾次沒有進步」\n",
    "# 每當 val_acc 沒有變好，就 bad += 1；有變好就 bad = 0。\n",
    "best_state, best_val, patience, bad = None, -1.0, 15, 0\n",
    "hist = {\"tr_loss\": [], \"tr_acc\": [], \"va_loss\": [], \"va_acc\": []}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "34056bd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits tensor([[-0.3058, -0.0048,  0.1059],\n",
      "        [-0.1441,  0.0519, -0.0442],\n",
      "        [-0.1442,  0.0864, -0.0293],\n",
      "        [-0.2625, -0.0158,  0.0164],\n",
      "        [ 0.3275, -0.0046, -0.1956],\n",
      "        [-0.3512, -0.0322,  0.1340],\n",
      "        [ 0.3038, -0.0171, -0.1763],\n",
      "        [-0.2666,  0.0081,  0.0818],\n",
      "        [ 0.1845,  0.0703, -0.1612],\n",
      "        [-0.1050,  0.0419, -0.0637],\n",
      "        [-0.0844,  0.0860, -0.1062],\n",
      "        [-0.1384,  0.0228, -0.0238],\n",
      "        [ 0.2918, -0.0391, -0.1539],\n",
      "        [-0.5315, -0.0209, -0.0087],\n",
      "        [-0.0315,  0.1546, -0.0969],\n",
      "        [-0.4474, -0.0102, -0.0110]])\n",
      "yb tensor([2, 1, 2, 1, 0, 2, 0, 2, 0, 1, 1, 1, 0, 2, 1, 2])\n",
      "logits tensor([[ 0.2448,  0.0699, -0.1733],\n",
      "        [ 0.2152,  0.0338, -0.1785],\n",
      "        [-0.2530,  0.0175,  0.0403],\n",
      "        [-0.2758, -0.0062,  0.0820],\n",
      "        [-0.1488,  0.0265, -0.0156],\n",
      "        [ 0.2631,  0.0658, -0.1719],\n",
      "        [ 0.2426,  0.0263, -0.1847],\n",
      "        [-0.0220,  0.1573, -0.0973]])\n",
      "yb tensor([0, 0, 2, 2, 1, 0, 0, 1])\n",
      "Epoch 001 | train_loss=1.0005 acc=0.729 | val_loss=0.9564 acc=0.875\n",
      "logits tensor([[-4.1246e-01, -2.6632e-02,  2.2820e-01],\n",
      "        [-2.0052e-01,  4.1729e-02,  2.4316e-04],\n",
      "        [-2.2426e-01,  7.7610e-02,  3.7291e-02],\n",
      "        [-3.5801e-01, -3.5631e-02,  9.5741e-02],\n",
      "        [ 5.2452e-01, -5.7623e-02, -2.0238e-01],\n",
      "        [-4.8212e-01, -6.1363e-02,  2.5748e-01],\n",
      "        [ 5.0381e-01, -7.6007e-02, -1.7706e-01],\n",
      "        [-3.5713e-01, -1.1838e-02,  1.8663e-01],\n",
      "        [ 3.1380e-01,  4.4402e-02, -1.9359e-01],\n",
      "        [-1.4186e-01,  3.3297e-02, -2.8266e-02],\n",
      "        [-1.1309e-01,  8.8289e-02, -9.0672e-02],\n",
      "        [-1.7580e-01,  7.3823e-03,  2.5179e-02],\n",
      "        [ 4.9367e-01, -1.0574e-01, -1.4729e-01],\n",
      "        [-7.1076e-01, -5.6086e-02,  1.4681e-01],\n",
      "        [-5.0184e-02,  1.6689e-01, -1.0364e-01],\n",
      "        [-6.0225e-01, -4.2490e-02,  1.1564e-01]])\n",
      "yb tensor([2, 1, 2, 1, 0, 2, 0, 2, 0, 1, 1, 1, 0, 2, 1, 2])\n",
      "logits tensor([[ 0.3870,  0.0385, -0.1826],\n",
      "        [ 0.3510,  0.0022, -0.1950],\n",
      "        [-0.3533, -0.0099,  0.1234],\n",
      "        [-0.3882, -0.0396,  0.1878],\n",
      "        [-0.2127,  0.0084,  0.0484],\n",
      "        [ 0.4173,  0.0356, -0.1956],\n",
      "        [ 0.3842, -0.0068, -0.1991],\n",
      "        [-0.0461,  0.1695, -0.1012]])\n",
      "yb tensor([0, 0, 2, 2, 1, 0, 0, 1])\n",
      "Epoch 002 | train_loss=0.9412 acc=0.802 | val_loss=0.8868 acc=0.833\n",
      "logits tensor([[-0.5652, -0.0561,  0.3857],\n",
      "        [-0.2821,  0.0276,  0.0598],\n",
      "        [-0.3312,  0.0661,  0.1244],\n",
      "        [-0.4904, -0.0577,  0.1977],\n",
      "        [ 0.7352, -0.1071, -0.2085],\n",
      "        [-0.6457, -0.0828,  0.4280],\n",
      "        [ 0.7157, -0.1273, -0.1828],\n",
      "        [-0.4797, -0.0410,  0.3221],\n",
      "        [ 0.4407,  0.0209, -0.2331],\n",
      "        [-0.2040,  0.0223,  0.0250],\n",
      "        [-0.1535,  0.0852, -0.0611],\n",
      "        [-0.2390, -0.0126,  0.0983],\n",
      "        [ 0.7039, -0.1665, -0.1410],\n",
      "        [-0.9108, -0.0724,  0.3691],\n",
      "        [-0.0827,  0.1827, -0.1011],\n",
      "        [-0.7829, -0.0624,  0.2982]])\n",
      "yb tensor([2, 1, 2, 1, 0, 2, 0, 2, 0, 1, 1, 1, 0, 2, 1, 2])\n",
      "logits tensor([[ 0.5366,  0.0229, -0.2089],\n",
      "        [ 0.5011, -0.0151, -0.2210],\n",
      "        [-0.4826, -0.0421,  0.2238],\n",
      "        [-0.5237, -0.0620,  0.3267],\n",
      "        [-0.3003, -0.0124,  0.1308],\n",
      "        [ 0.5587,  0.0096, -0.2371],\n",
      "        [ 0.5476, -0.0273, -0.2237],\n",
      "        [-0.0846,  0.1850, -0.0951]])\n",
      "yb tensor([0, 0, 2, 2, 1, 0, 0, 1])\n",
      "Epoch 003 | train_loss=0.8684 acc=0.885 | val_loss=0.8150 acc=0.792\n",
      "logits tensor([[-0.7317, -0.0789,  0.5623],\n",
      "        [-0.3703,  0.0150,  0.1212],\n",
      "        [-0.4486,  0.0524,  0.2092],\n",
      "        [-0.6330, -0.0867,  0.3017],\n",
      "        [ 0.9814, -0.1570, -0.2411],\n",
      "        [-0.8228, -0.1117,  0.6080],\n",
      "        [ 0.9609, -0.1826, -0.2130],\n",
      "        [-0.6205, -0.0694,  0.4664],\n",
      "        [ 0.5886, -0.0024, -0.2879],\n",
      "        [-0.2772,  0.0109,  0.0755],\n",
      "        [-0.1990,  0.0863, -0.0334],\n",
      "        [-0.3124, -0.0385,  0.1711],\n",
      "        [ 0.9478, -0.2306, -0.1585],\n",
      "        [-1.1423, -0.0920,  0.5924],\n",
      "        [-0.1210,  0.1998, -0.1049],\n",
      "        [-0.9888, -0.0791,  0.4847]])\n",
      "yb tensor([2, 1, 2, 1, 0, 2, 0, 2, 0, 1, 1, 1, 0, 2, 1, 2])\n",
      "logits tensor([[ 7.1354e-01, -6.2500e-04, -2.4826e-01],\n",
      "        [ 6.7707e-01, -3.9392e-02, -2.5772e-01],\n",
      "        [-6.0751e-01, -6.5241e-02,  3.3343e-01],\n",
      "        [-6.7170e-01, -8.1316e-02,  4.7578e-01],\n",
      "        [-4.0045e-01, -3.0277e-02,  2.1488e-01],\n",
      "        [ 7.1876e-01, -1.6472e-02, -2.9580e-01],\n",
      "        [ 7.3529e-01, -5.4818e-02, -2.6123e-01],\n",
      "        [-1.2993e-01,  2.0194e-01, -9.4982e-02]])\n",
      "yb tensor([0, 0, 2, 2, 1, 0, 0, 1])\n",
      "Epoch 004 | train_loss=0.8078 acc=0.875 | val_loss=0.7442 acc=0.792\n",
      "logits tensor([[-0.9125, -0.1122,  0.7397],\n",
      "        [-0.4629,  0.0118,  0.1911],\n",
      "        [-0.5665,  0.0445,  0.2975],\n",
      "        [-0.7832, -0.1011,  0.4223],\n",
      "        [ 1.2683, -0.2124, -0.3037],\n",
      "        [-1.0212, -0.1470,  0.7989],\n",
      "        [ 1.2503, -0.2456, -0.2761],\n",
      "        [-0.7685, -0.1036,  0.6271],\n",
      "        [ 0.7534, -0.0319, -0.3546],\n",
      "        [-0.3536,  0.0080,  0.1315],\n",
      "        [-0.2525,  0.0908, -0.0034],\n",
      "        [-0.3850, -0.0726,  0.2460],\n",
      "        [ 1.2271, -0.3044, -0.2043],\n",
      "        [-1.4019, -0.1212,  0.8031],\n",
      "        [-0.1677,  0.2163, -0.1119],\n",
      "        [-1.2153, -0.1014,  0.6666]])\n",
      "yb tensor([2, 1, 2, 1, 0, 2, 0, 2, 0, 1, 1, 1, 0, 2, 1, 2])\n",
      "logits tensor([[ 0.9107, -0.0465, -0.3072],\n",
      "        [ 0.8713, -0.0824, -0.3151],\n",
      "        [-0.7455, -0.0829,  0.4572],\n",
      "        [-0.8371, -0.1058,  0.6294],\n",
      "        [-0.4951, -0.0457,  0.3124],\n",
      "        [ 0.9094, -0.0574, -0.3646],\n",
      "        [ 0.9528, -0.0956, -0.3172],\n",
      "        [-0.1840,  0.2187, -0.0973]])\n",
      "yb tensor([0, 0, 2, 2, 1, 0, 0, 1])\n",
      "Epoch 005 | train_loss=0.7509 acc=0.865 | val_loss=0.6767 acc=0.792\n",
      "logits tensor([[-1.1186, -0.1444,  0.9306],\n",
      "        [-0.5714,  0.0137,  0.2585],\n",
      "        [-0.7042,  0.0419,  0.3914],\n",
      "        [-0.9425, -0.1069,  0.5419],\n",
      "        [ 1.5692, -0.2773, -0.3824],\n",
      "        [-1.2483, -0.1761,  1.0035],\n",
      "        [ 1.5591, -0.3105, -0.3664],\n",
      "        [-0.9395, -0.1334,  0.7926],\n",
      "        [ 0.9261, -0.0714, -0.4257],\n",
      "        [-0.4405,  0.0136,  0.1900],\n",
      "        [-0.3178,  0.1007,  0.0265],\n",
      "        [-0.4630, -0.0896,  0.3301],\n",
      "        [ 1.5296, -0.3817, -0.2789],\n",
      "        [-1.6926, -0.1386,  1.0249],\n",
      "        [-0.2220,  0.2353, -0.1186],\n",
      "        [-1.4684, -0.1144,  0.8516]])\n",
      "yb tensor([2, 1, 2, 1, 0, 2, 0, 2, 0, 1, 1, 1, 0, 2, 1, 2])\n",
      "logits tensor([[ 1.1189, -0.1016, -0.3746],\n",
      "        [ 1.0756, -0.1347, -0.3810],\n",
      "        [-0.9033, -0.0955,  0.5890],\n",
      "        [-1.0274, -0.1253,  0.7949],\n",
      "        [-0.6051, -0.0567,  0.4128],\n",
      "        [ 1.1185, -0.1118, -0.4393],\n",
      "        [ 1.1747, -0.1521, -0.3868],\n",
      "        [-0.2467,  0.2380, -0.0990]])\n",
      "yb tensor([0, 0, 2, 2, 1, 0, 0, 1])\n",
      "Epoch 006 | train_loss=0.6856 acc=0.802 | val_loss=0.6159 acc=0.792\n",
      "logits tensor([[-1.3498, -0.1718,  1.1289],\n",
      "        [-0.6980,  0.0208,  0.3184],\n",
      "        [-0.8678,  0.0473,  0.4822],\n",
      "        [-1.1280, -0.1030,  0.6583],\n",
      "        [ 1.8912, -0.3439, -0.4859],\n",
      "        [-1.4990, -0.2035,  1.2161],\n",
      "        [ 1.8845, -0.3770, -0.4789],\n",
      "        [-1.1312, -0.1584,  0.9631],\n",
      "        [ 1.1046, -0.1178, -0.5103],\n",
      "        [-0.5405,  0.0252,  0.2456],\n",
      "        [-0.4021,  0.1177,  0.0504],\n",
      "        [-0.5479, -0.0985,  0.4165],\n",
      "        [ 1.8617, -0.4586, -0.3867],\n",
      "        [-2.0386, -0.1470,  1.2464],\n",
      "        [-0.3014,  0.2640, -0.1321],\n",
      "        [-1.7656, -0.1178,  1.0373]])\n",
      "yb tensor([2, 1, 2, 1, 0, 2, 0, 2, 0, 1, 1, 1, 0, 2, 1, 2])\n",
      "logits tensor([[ 1.3337, -0.1633, -0.4611],\n",
      "        [ 1.2877, -0.1922, -0.4623],\n",
      "        [-1.0870, -0.1013,  0.7229],\n",
      "        [-1.2433, -0.1386,  0.9657],\n",
      "        [-0.7298, -0.0638,  0.5143],\n",
      "        [ 1.3331, -0.1726, -0.5333],\n",
      "        [ 1.4066, -0.2140, -0.4711],\n",
      "        [-0.3355,  0.2671, -0.1060]])\n",
      "yb tensor([0, 0, 2, 2, 1, 0, 0, 1])\n",
      "Epoch 007 | train_loss=0.6122 acc=0.823 | val_loss=0.5622 acc=0.792\n",
      "logits tensor([[-1.5889, -0.1769,  1.3159],\n",
      "        [-0.8285,  0.0483,  0.3602],\n",
      "        [-1.0452,  0.0775,  0.5478],\n",
      "        [-1.3331, -0.0717,  0.7502],\n",
      "        [ 2.2181, -0.4278, -0.6247],\n",
      "        [-1.7762, -0.2024,  1.4064],\n",
      "        [ 2.2188, -0.4564, -0.6070],\n",
      "        [-1.3272, -0.1660,  1.1222],\n",
      "        [ 1.3032, -0.1724, -0.6109],\n",
      "        [-0.6474,  0.0494,  0.2823],\n",
      "        [-0.4841,  0.1511,  0.0555],\n",
      "        [-0.6379, -0.0921,  0.4903],\n",
      "        [ 2.2063, -0.5412, -0.5197],\n",
      "        [-2.4214, -0.1057,  1.4215],\n",
      "        [-0.3694,  0.3124, -0.1720],\n",
      "        [-2.0948, -0.0772,  1.1799]])\n",
      "yb tensor([2, 1, 2, 1, 0, 2, 0, 2, 0, 1, 1, 1, 0, 2, 1, 2])\n",
      "logits tensor([[ 1.5634, -0.2348, -0.5658],\n",
      "        [ 1.5126, -0.2605, -0.5648],\n",
      "        [-1.2894, -0.0825,  0.8414],\n",
      "        [-1.4789, -0.1338,  1.1163],\n",
      "        [-0.8615, -0.0554,  0.6004],\n",
      "        [ 1.5685, -0.2433, -0.6445],\n",
      "        [ 1.6499, -0.2882, -0.5805],\n",
      "        [-0.4219,  0.3122, -0.1344]])\n",
      "yb tensor([0, 0, 2, 2, 1, 0, 0, 1])\n",
      "Epoch 008 | train_loss=0.5624 acc=0.823 | val_loss=0.5160 acc=0.792\n",
      "logits tensor([[-1.8584, -0.1491,  1.4648],\n",
      "        [-0.9575,  0.0878,  0.3857],\n",
      "        [-1.2247,  0.1220,  0.5951],\n",
      "        [-1.5393, -0.0245,  0.8197],\n",
      "        [ 2.5678, -0.5339, -0.7974],\n",
      "        [-2.0703, -0.1706,  1.5624],\n",
      "        [ 2.5766, -0.5538, -0.7800],\n",
      "        [-1.5412, -0.1426,  1.2498],\n",
      "        [ 1.5267, -0.2400, -0.7304],\n",
      "        [-0.7551,  0.0880,  0.3018],\n",
      "        [-0.5623,  0.1928,  0.0453],\n",
      "        [-0.7319, -0.0665,  0.5400],\n",
      "        [ 2.5804, -0.6355, -0.7032],\n",
      "        [-2.8102, -0.0386,  1.5630],\n",
      "        [-0.4282,  0.3691, -0.2314],\n",
      "        [-2.4305, -0.0151,  1.2949]])\n",
      "yb tensor([2, 1, 2, 1, 0, 2, 0, 2, 0, 1, 1, 1, 0, 2, 1, 2])\n",
      "logits tensor([[ 1.8182, -0.3192, -0.6897],\n",
      "        [ 1.7611, -0.3413, -0.6863],\n",
      "        [-1.4982, -0.0510,  0.9348],\n",
      "        [-1.7298, -0.1023,  1.2354],\n",
      "        [-1.0035, -0.0295,  0.6647],\n",
      "        [ 1.8314, -0.3279, -0.7762],\n",
      "        [ 1.9178, -0.3759, -0.7101],\n",
      "        [-0.4924,  0.3685, -0.1844]])\n",
      "yb tensor([0, 0, 2, 2, 1, 0, 0, 1])\n",
      "Epoch 009 | train_loss=0.5193 acc=0.854 | val_loss=0.4759 acc=0.792\n",
      "logits tensor([[-2.1419, -0.1236,  1.6229],\n",
      "        [-1.0947,  0.1262,  0.4168],\n",
      "        [-1.4188,  0.1622,  0.6581],\n",
      "        [-1.7498,  0.0250,  0.8927],\n",
      "        [ 2.9019, -0.6498, -0.9678],\n",
      "        [-2.3803, -0.1393,  1.7253],\n",
      "        [ 2.9155, -0.6645, -0.9497],\n",
      "        [-1.7774, -0.1200,  1.3777],\n",
      "        [ 1.7426, -0.3093, -0.8551],\n",
      "        [-0.8693,  0.1252,  0.3235],\n",
      "        [-0.6483,  0.2344,  0.0390],\n",
      "        [-0.8324, -0.0378,  0.5835],\n",
      "        [ 2.9419, -0.7326, -0.9052],\n",
      "        [-3.2101,  0.0260,  1.7244],\n",
      "        [-0.5013,  0.4271, -0.2764],\n",
      "        [-2.7772,  0.0445,  1.4281]])\n",
      "yb tensor([2, 1, 2, 1, 0, 2, 0, 2, 0, 1, 1, 1, 0, 2, 1, 2])\n",
      "logits tensor([[ 2.0667, -0.4046, -0.8187],\n",
      "        [ 2.0024, -0.4231, -0.8129],\n",
      "        [-1.7189, -0.0188,  1.0334],\n",
      "        [-1.9919, -0.0719,  1.3656],\n",
      "        [-1.1599, -0.0041,  0.7317],\n",
      "        [ 2.0884, -0.4155, -0.9123],\n",
      "        [ 2.1778, -0.4638, -0.8456],\n",
      "        [-0.5790,  0.4269, -0.2243]])\n",
      "yb tensor([0, 0, 2, 2, 1, 0, 0, 1])\n",
      "Epoch 010 | train_loss=0.5008 acc=0.865 | val_loss=0.4433 acc=0.792\n",
      "logits tensor([[-2.4135, -0.1127,  1.7830],\n",
      "        [-1.2312,  0.1618,  0.4484],\n",
      "        [-1.6191,  0.1962,  0.7282],\n",
      "        [-1.9458,  0.0715,  0.9589],\n",
      "        [ 3.2232, -0.7613, -1.1429],\n",
      "        [-2.6730, -0.1223,  1.8874],\n",
      "        [ 3.2372, -0.7710, -1.1247],\n",
      "        [-2.0010, -0.1100,  1.5079],\n",
      "        [ 1.9426, -0.3744, -0.9882],\n",
      "        [-0.9754,  0.1623,  0.3396],\n",
      "        [-0.7348,  0.2779,  0.0294],\n",
      "        [-0.9282, -0.0138,  0.6216],\n",
      "        [ 3.2816, -0.8332, -1.1022],\n",
      "        [-3.5937,  0.0825,  1.8812],\n",
      "        [-0.5856,  0.4909, -0.3228],\n",
      "        [-3.1101,  0.0965,  1.5586]])\n",
      "yb tensor([2, 1, 2, 1, 0, 2, 0, 2, 0, 1, 1, 1, 0, 2, 1, 2])\n",
      "logits tensor([[ 2.3039, -0.4867, -0.9516],\n",
      "        [ 2.2317, -0.5016, -0.9434],\n",
      "        [-1.9291,  0.0046,  1.1320],\n",
      "        [-2.2422, -0.0531,  1.4966],\n",
      "        [-1.3087,  0.0146,  0.8001],\n",
      "        [ 2.3327, -0.4998, -1.0527],\n",
      "        [ 2.4258, -0.5481, -0.9853],\n",
      "        [-0.6746,  0.4916, -0.2637]])\n",
      "yb tensor([0, 0, 2, 2, 1, 0, 0, 1])\n",
      "Epoch 011 | train_loss=0.4487 acc=0.885 | val_loss=0.4167 acc=0.792\n",
      "logits tensor([[-2.6752, -0.0941,  1.9190],\n",
      "        [-1.3586,  0.2055,  0.4652],\n",
      "        [-1.8162,  0.2349,  0.7847],\n",
      "        [-2.1178,  0.1342,  0.9965],\n",
      "        [ 3.5282, -0.8673, -1.3208],\n",
      "        [-2.9537, -0.0962,  2.0229],\n",
      "        [ 3.5363, -0.8733, -1.3042],\n",
      "        [-2.2141, -0.0925,  1.6137],\n",
      "        [ 2.1366, -0.4382, -1.1232],\n",
      "        [-1.0742,  0.2051,  0.3430],\n",
      "        [-0.8178,  0.3264,  0.0103],\n",
      "        [-1.0168,  0.0172,  0.6441],\n",
      "        [ 3.5972, -0.9345, -1.2917],\n",
      "        [-3.9705,  0.1550,  2.0000],\n",
      "        [-0.6683,  0.5577, -0.3859],\n",
      "        [-3.4386,  0.1630,  1.6544]])\n",
      "yb tensor([2, 1, 2, 1, 0, 2, 0, 2, 0, 1, 1, 1, 0, 2, 1, 2])\n",
      "logits tensor([[ 2.5298, -0.5652, -1.0870],\n",
      "        [ 2.4497, -0.5765, -1.0761],\n",
      "        [-2.1317,  0.0353,  1.2119],\n",
      "        [-2.4833, -0.0270,  1.6058],\n",
      "        [-1.4477,  0.0407,  0.8542],\n",
      "        [ 2.5667, -0.5809, -1.1959],\n",
      "        [ 2.6610, -0.6284, -1.1273],\n",
      "        [-0.7681,  0.5620, -0.3148]])\n",
      "yb tensor([0, 0, 2, 2, 1, 0, 0, 1])\n",
      "Epoch 012 | train_loss=0.4315 acc=0.812 | val_loss=0.3935 acc=0.792\n",
      "logits tensor([[-2.9110, -0.0943,  2.0609],\n",
      "        [-1.4653,  0.2477,  0.4739],\n",
      "        [-1.9989,  0.2664,  0.8436],\n",
      "        [-2.2729,  0.1876,  1.0283],\n",
      "        [ 3.8178, -0.9615, -1.5014],\n",
      "        [-3.2085, -0.0901,  2.1662],\n",
      "        [ 3.8219, -0.9643, -1.4895],\n",
      "        [-2.4048, -0.0910,  1.7239],\n",
      "        [ 2.3267, -0.4909, -1.2616],\n",
      "        [-1.1610,  0.2444,  0.3437],\n",
      "        [-0.8944,  0.3783, -0.0186],\n",
      "        [-1.0924,  0.0413,  0.6665],\n",
      "        [ 3.8884, -1.0245, -1.4834],\n",
      "        [-4.3251,  0.2072,  2.1215],\n",
      "        [-0.7508,  0.6292, -0.4611],\n",
      "        [-3.7483,  0.2129,  1.7516]])\n",
      "yb tensor([2, 1, 2, 1, 0, 2, 0, 2, 0, 1, 1, 1, 0, 2, 1, 2])\n",
      "logits tensor([[ 2.7483, -0.6349, -1.2258],\n",
      "        [ 2.6602, -0.6423, -1.2122],\n",
      "        [-2.3171,  0.0531,  1.2967],\n",
      "        [-2.7006, -0.0159,  1.7168],\n",
      "        [-1.5631,  0.0616,  0.9017],\n",
      "        [ 2.7945, -0.6521, -1.3429],\n",
      "        [ 2.8871, -0.6994, -1.2727],\n",
      "        [-0.8610,  0.6342, -0.3824]])\n",
      "yb tensor([0, 0, 2, 2, 1, 0, 0, 1])\n",
      "Epoch 013 | train_loss=0.3885 acc=0.854 | val_loss=0.3728 acc=0.792\n",
      "logits tensor([[-3.1341, -0.0897,  2.1954],\n",
      "        [-1.5689,  0.2996,  0.4674],\n",
      "        [-2.1762,  0.3058,  0.8945],\n",
      "        [-2.4197,  0.2537,  1.0320],\n",
      "        [ 4.0615, -1.0300, -1.6722],\n",
      "        [-3.4476, -0.0788,  2.3006],\n",
      "        [ 4.0609, -1.0287, -1.6655],\n",
      "        [-2.5808, -0.0823,  1.8244],\n",
      "        [ 2.4897, -0.5284, -1.3899],\n",
      "        [-1.2441,  0.2906,  0.3368],\n",
      "        [-0.9749,  0.4353, -0.0590],\n",
      "        [-1.1677,  0.0747,  0.6811],\n",
      "        [ 4.1316, -1.0876, -1.6642],\n",
      "        [-4.6597,  0.2795,  2.2118],\n",
      "        [-0.8354,  0.7065, -0.5411],\n",
      "        [-4.0323,  0.2844,  1.8136]])\n",
      "yb tensor([2, 1, 2, 1, 0, 2, 0, 2, 0, 1, 1, 1, 0, 2, 1, 2])\n",
      "logits tensor([[ 2.9339e+00, -6.8632e-01, -1.3567e+00],\n",
      "        [ 2.8381e+00, -6.8986e-01, -1.3408e+00],\n",
      "        [-2.4924e+00,  8.0635e-02,  1.3698e+00],\n",
      "        [-2.9062e+00, -4.2219e-04,  1.8216e+00],\n",
      "        [-1.6701e+00,  9.0476e-02,  9.3859e-01],\n",
      "        [ 2.9882e+00, -7.0453e-01, -1.4807e+00],\n",
      "        [ 3.0781e+00, -7.5058e-01, -1.4103e+00],\n",
      "        [-9.5603e-01,  7.1185e-01, -4.5467e-01]])\n",
      "yb tensor([0, 0, 2, 2, 1, 0, 0, 1])\n",
      "Epoch 014 | train_loss=0.3913 acc=0.896 | val_loss=0.3532 acc=0.792\n",
      "logits tensor([[-3.3553, -0.0743,  2.3145],\n",
      "        [-1.6762,  0.3560,  0.4510],\n",
      "        [-2.3590,  0.3510,  0.9320],\n",
      "        [-2.5521,  0.3302,  1.0115],\n",
      "        [ 4.2805, -1.0851, -1.8315],\n",
      "        [-3.6748, -0.0537,  2.4120],\n",
      "        [ 4.2757, -1.0799, -1.8287],\n",
      "        [-2.7525, -0.0631,  1.9082],\n",
      "        [ 2.6328, -0.5554, -1.5063],\n",
      "        [-1.3290,  0.3432,  0.3219],\n",
      "        [-1.0569,  0.4957, -0.1026],\n",
      "        [-1.2429,  0.1167,  0.6871],\n",
      "        [ 4.3509, -1.1369, -1.8331],\n",
      "        [-4.9598,  0.3685,  2.2682],\n",
      "        [-0.9255,  0.7901, -0.6187],\n",
      "        [-4.2945,  0.3673,  1.8532]])\n",
      "yb tensor([2, 1, 2, 1, 0, 2, 0, 2, 0, 1, 1, 1, 0, 2, 1, 2])\n",
      "logits tensor([[ 3.0992, -0.7268, -1.4778],\n",
      "        [ 2.9961, -0.7266, -1.4597],\n",
      "        [-2.6582,  0.1180,  1.4246],\n",
      "        [-3.0973,  0.0289,  1.9017],\n",
      "        [-1.7791,  0.1256,  0.9688],\n",
      "        [ 3.1588, -0.7448, -1.6068],\n",
      "        [ 3.2485, -0.7906, -1.5379],\n",
      "        [-1.0550,  0.7951, -0.5242]])\n",
      "yb tensor([0, 0, 2, 2, 1, 0, 0, 1])\n",
      "Epoch 015 | train_loss=0.3737 acc=0.865 | val_loss=0.3348 acc=0.833\n",
      "logits tensor([[-3.5497, -0.0901,  2.4749],\n",
      "        [-1.7824,  0.3951,  0.4717],\n",
      "        [-2.5265,  0.3737,  1.0103],\n",
      "        [-2.6768,  0.3764,  1.0485],\n",
      "        [ 4.4900, -1.1365, -1.9820],\n",
      "        [-3.8847, -0.0678,  2.5796],\n",
      "        [ 4.4815, -1.1282, -1.9817],\n",
      "        [-2.9155, -0.0771,  2.0421],\n",
      "        [ 2.7575, -0.5721, -1.6284],\n",
      "        [-1.4137,  0.3805,  0.3344],\n",
      "        [-1.1409,  0.5457, -0.1234],\n",
      "        [-1.3116,  0.1416,  0.7213],\n",
      "        [ 4.5634, -1.1841, -1.9895],\n",
      "        [-5.2469,  0.4031,  2.4242],\n",
      "        [-1.0266,  0.8678, -0.6787],\n",
      "        [-4.5469,  0.4041,  1.9791]])\n",
      "yb tensor([2, 1, 2, 1, 0, 2, 0, 2, 0, 1, 1, 1, 0, 2, 1, 2])\n",
      "logits tensor([[ 3.2547, -0.7631, -1.5946],\n",
      "        [ 3.1446, -0.7593, -1.5738],\n",
      "        [-2.8072,  0.1300,  1.5164],\n",
      "        [-3.2815,  0.0226,  2.0368],\n",
      "        [-1.8839,  0.1389,  1.0338],\n",
      "        [ 3.3175, -0.7799, -1.7284],\n",
      "        [ 3.4092, -0.8266, -1.6600],\n",
      "        [-1.1622,  0.8702, -0.5744]])\n",
      "yb tensor([0, 0, 2, 2, 1, 0, 0, 1])\n",
      "Epoch 016 | train_loss=0.3245 acc=0.896 | val_loss=0.3215 acc=0.833\n",
      "早停：15 epochs 未提升\n"
     ]
    }
   ],
   "source": [
    "# 訓練迴圈\n",
    "# for ep in range(1, 201):\n",
    "# 訓練最多 200 個 epoch。\n",
    "# 如果提早 early stopping，就會 break。\n",
    "for ep in range(1, 201):\n",
    "    # 🏋️‍♂️ 模型訓練 (train mode)\n",
    "    # model.train()\n",
    "    # total, correct, loss_sum = 0, 0, 0.0\n",
    "    # 切換到「訓練模式」(啟用 dropout、BN 等)。\n",
    "    # total：累計訓練樣本數\n",
    "    # correct：累計預測正確數\n",
    "    # loss_sum：累計總 loss\n",
    "    model.train()\n",
    "    total, correct, loss_sum = 0, 0, 0.0\n",
    "\n",
    "    # 一個 batch 的訓練\n",
    "    # for xb, yb in train_loader:\n",
    "    #     xb, yb = xb.to(device), yb.to(device)\n",
    "    #     logits = model(xb)\n",
    "    #     loss = criterion(logits, yb)\n",
    "\n",
    "    # 取一個 batch (xb, yb)\n",
    "    # 丟進 GPU/CPU\n",
    "    # 前向傳播 → 得到 logits (分數)\n",
    "    # 計算 loss (CrossEntropyLoss)\n",
    "    for xb, yb in train_loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        logits = model(xb)\n",
    "        loss = criterion(logits, yb)\n",
    "\n",
    "        #⬅️ 反向傳播 + 參數更新\n",
    "        # optimizer.zero_grad()\n",
    "        #loss.backward()\n",
    "        #optimizer.step()\n",
    "\n",
    "        # zero_grad()：清掉上一輪的梯度\n",
    "        # loss.backward()：反向傳播，算出梯度\n",
    "        # step()：用 optimizer (Adam) 更新參數\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        loss_sum += loss.item() * xb.size(0)\n",
    "        correct += (logits.argmax(1) == yb).sum().item()\n",
    "        total += yb.size(0)\n",
    "    \n",
    "    tr_loss, tr_acc = loss_sum / total, correct / total\n",
    "\n",
    "    # 驗證 (validation)\n",
    "    # va_loss, va_acc = evaluate(model, val_loader)\n",
    "    # 用剛才寫的 evaluate 函數算 validation loss 和 accuracy。\n",
    "    va_loss, va_acc = evaluate(model, val_loader)\n",
    "    \n",
    "    hist[\"tr_loss\"].append(tr_loss)\n",
    "    hist[\"tr_acc\"].append(tr_acc)\n",
    "    hist[\"va_loss\"].append(va_loss)\n",
    "    hist[\"va_acc\"].append(va_acc)\n",
    "    \n",
    "    print(f\"Epoch {ep:03d} | train_loss={tr_loss:.4f} acc={tr_acc:.3f} | val_loss={va_loss:.4f} acc={va_acc:.3f}\")\n",
    "\n",
    "\n",
    "    # Early Stopping\n",
    "    # 解釋：\n",
    "    # 如果 validation accuracy 變好 → 更新 best_val，存下模型狀態 best_state，並 reset bad = 0。\n",
    "    # 如果沒有進步 → bad += 1\n",
    "    # 如果連續 patience (15) 次都沒進步 → 早停 (early stopping)，直接結束訓練。\n",
    "    #這一行：\n",
    "    # {k: v.cpu() for k, v in model.state_dict().items()}\n",
    "    # 等價於：\n",
    "    # 取出模型所有參數 (state_dict)\n",
    "    # 把每個參數從 GPU 搬到 CPU\n",
    "    # 存成一個新的字典\n",
    "    # 這個新字典就被存在 best_state 裡，用來保存「最佳模型」的參數。\n",
    "    if va_acc > best_val:\n",
    "     best_val, best_state,bad = va_acc, {k: v.cpu() for k, v in model.state_dict().items()},0\n",
    "    else:\n",
    "        bad += 1\n",
    "        if bad >= patience:\n",
    "            print(f\"早停：{patience} epochs 未提升\")\n",
    "            break\n",
    "\n",
    "\n",
    "# 載回最佳\n",
    "if best_state is not None:\n",
    "    model.load_state_dict(best_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ecbd0f27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 已存訓練曲線: iris_course\\plots\\curves.png\n",
      "✅ 已存最佳權重: iris_course\\models\\best.pt\n",
      "STEP 6 ✅ 完成\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# === 畫訓練/驗證曲線 ===\n",
    "xs = np.arange(1, len(hist[\"tr_loss\"]) + 1)\n",
    "plt.figure(figsize=(8, 4))\n",
    "\n",
    "# loss 曲線\n",
    "plt.plot(xs, hist[\"tr_loss\"], label=\"train_loss\")\n",
    "plt.plot(xs, hist[\"va_loss\"], label=\"val_loss\")\n",
    "\n",
    "# acc 曲線\n",
    "plt.plot(xs, hist[\"tr_acc\"], label=\"train_acc\")\n",
    "plt.plot(xs, hist[\"va_acc\"], label=\"val_acc\")\n",
    "\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"value\")\n",
    "plt.title(\"Training Curves\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "\n",
    "PLOTS = os.path.join(ROOT, \"plots\")\n",
    "\n",
    "# 確保資料夾存在\n",
    "os.makedirs(PLOTS, exist_ok=True)\n",
    "\n",
    "# 儲存圖表\n",
    "curve_path = os.path.join(PLOTS, \"curves.png\")\n",
    "plt.savefig(curve_path, dpi=150)\n",
    "plt.close()\n",
    "print(f\"✅ 已存訓練曲線: {curve_path}\")\n",
    "\n",
    "# === 儲存最佳模型權重 ===\n",
    "best_path = os.path.join(MODELS, \"best.pt\")\n",
    "torch.save(model.state_dict(), best_path)\n",
    "print(f\"✅ 已存最佳權重: {best_path}\")\n",
    "\n",
    "print(\"STEP 6 ✅ 完成\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4037ebf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== STEP 7 測試集評估 ===\n",
      "Test Accuracy = 0.800\n",
      "\n",
      "分類報告：\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      setosa      1.000     1.000     1.000        10\n",
      "  versicolor      1.000     0.400     0.571        10\n",
      "   virginica      0.625     1.000     0.769        10\n",
      "\n",
      "    accuracy                          0.800        30\n",
      "   macro avg      0.875     0.800     0.780        30\n",
      "weighted avg      0.875     0.800     0.780        30\n",
      "\n",
      "->已存混淆矩陣: iris_course\\plots\\confusion_matrix.png\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "print(\"\\n=== STEP 7 測試集評估 ===\")\n",
    "\n",
    "# 把模型切換到 評估模式 (evaluation mode)。\n",
    "# 關掉 dropout、batch normalization 的隨機行為，讓測試結果穩定。\n",
    "# 模型訓練好 不會自己關掉 Dropout/BN，因為 PyTorch 不知道你要做「驗證」還是「繼續訓練」。\n",
    "model.eval()\n",
    "\n",
    "# 關閉梯度計算，節省記憶體和運算量。\n",
    "# 測試時只要 forward，不需要 backward。\n",
    "with torch.no_grad():\n",
    "\n",
    "    # X_test_sc → 測試集的輸入特徵 (通常是標準化後的資料)。\n",
    "    # torch.tensor(..., dtype=torch.float32) → 把 numpy array 轉成 PyTorch tensor，並指定浮點數型別。\n",
    "    # .to(device) → 把資料搬到跟模型相同的裝置（CPU 或 GPU）。\n",
    "    # model(...) → 前向傳播，得到 logits（未經 softmax 的分數）。\n",
    "    # 假設測試集有 100 筆資料，類別數 = 3，\n",
    "    # 那 logits.shape = [100, 3]。\n",
    "    logits = model(torch.tensor(X_test_sc, dtype=torch.float32).to(device))\n",
    "\n",
    "\n",
    "    # logits.argmax(1) → 沿著類別維度找最大值的索引，也就是「模型預測的類別」。\n",
    "    # e.g. [2.0, 1.0, -0.5] → argmax = 0 (預測 class 0)\n",
    "    # .cpu() → 把結果移回 CPU（因為有可能在 GPU 上運算）。\n",
    "    # .numpy() → 轉成 numpy array，方便後續用 sklearn.metrics 計算 accuracy、混淆矩陣等。\n",
    "    # 最後 y_pred 是一個長度 = 測試資料筆數的 array，例如：\n",
    "    # array([0, 2, 1, 0, 1, 2, ...])\n",
    "    y_pred = logits.argmax(1).cpu().numpy()\n",
    "\n",
    "# 計算準確率\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(f\"Test Accuracy = {acc:.3f}\\n\")\n",
    "\n",
    "# 分類報告\n",
    "print(\"分類報告：\")\n",
    "print(classification_report(y_test, y_pred, target_names=target_names, digits=3))\n",
    "    \n",
    "# === 混淆矩陣 ===\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(4.5, 4))\n",
    "plt.imshow(cm, interpolation=\"nearest\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.colorbar()\n",
    "\n",
    "ticks = np.arange(len(target_names))\n",
    "plt.xticks(ticks, target_names, rotation=30)\n",
    "plt.yticks(ticks, target_names)\n",
    "\n",
    "# 在矩陣格子中填上數字\n",
    "for i in range(cm.shape[0]):\n",
    "    for j in range(cm.shape[1]):\n",
    "        plt.text(j, i, str(cm[i, j]), ha=\"center\", va=\"center\")\n",
    "\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.tight_layout()\n",
    "\n",
    "cm_path = os.path.join(PLOTS, \"confusion_matrix.png\")\n",
    "plt.savefig(cm_path, dpi=150)\n",
    "plt.close()\n",
    "print(f\"->已存混淆矩陣: {cm_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "83ea9f23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   sepal_length  sepal_width  petal_length  petal_width        True  \\\n",
      "0           4.4          3.0           1.3          0.2      setosa   \n",
      "1           6.1          3.0           4.9          1.8   virginica   \n",
      "2           4.9          2.4           3.3          1.0  versicolor   \n",
      "3           5.0          2.3           3.3          1.0  versicolor   \n",
      "4           4.4          3.2           1.3          0.2      setosa   \n",
      "5           6.3          3.3           4.7          1.6  versicolor   \n",
      "6           4.6          3.6           1.0          0.2      setosa   \n",
      "7           5.4          3.4           1.7          0.2      setosa   \n",
      "8           6.5          3.0           5.2          2.0   virginica   \n",
      "9           5.4          3.0           4.5          1.5  versicolor   \n",
      "\n",
      "         Pred  \n",
      "0      setosa  \n",
      "1   virginica  \n",
      "2  versicolor  \n",
      "3  versicolor  \n",
      "4      setosa  \n",
      "5   virginica  \n",
      "6      setosa  \n",
      "7      setosa  \n",
      "8   virginica  \n",
      "9   virginica  \n",
      "已存樣本對照(原單位): iris_course\\artifacts\\test_samples.csv\n",
      "STEP 7 完成 ✅ （全流程）\n"
     ]
    }
   ],
   "source": [
    "# === 回到原單位顯示幾筆 ===\n",
    "\n",
    "# X_test_sc → 之前經過標準化 (scaler) 的測試集特徵。\n",
    "# scaler.inverse_transform(...) → 把標準化後的資料還原成原本的數值單位。\n",
    "# 例如：花瓣長度原本是 3.5 cm，標準化後可能是 -0.23，現在還原回 3.5。\n",
    "X_test_orig = scaler.inverse_transform(X_test_sc)\n",
    "\n",
    "# 最多顯示 10 筆樣本。\n",
    "# 如果測試資料少於 10，就顯示全部。\n",
    "show_n = min(10, len(X_test_orig))\n",
    "\n",
    "tbl = pd.DataFrame(X_test_orig[:show_n], columns=feature_names)\n",
    "tbl[\"True\"]  = [target_names[i] for i in y_test[:show_n]]\n",
    "tbl[\"Pred\"]  = [target_names[i] for i in y_pred[:show_n]]\n",
    "print(tbl)\n",
    "samples_csv = os.path.join(ARTIFACTS, \"test_samples.csv\")\n",
    "tbl.to_csv(samples_csv, index=False)\n",
    "print(f\"已存樣本對照(原單位): {samples_csv}\")\n",
    "\n",
    "print(\"STEP 7 完成 ✅ （全流程）\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyTorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
