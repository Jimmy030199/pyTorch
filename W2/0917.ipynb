{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Step1 è¼‰å…¥è³‡æ–™èˆ‡æ¢ç´¢\n",
      "\n",
      " å‰5ç­†è³‡æ–™\n",
      "   sepal_length  sepal_width  petal_length  petal_width  target\n",
      "0           5.1          3.5           1.4          0.2       0\n",
      "1           4.9          3.0           1.4          0.2       0\n",
      "2           4.7          3.2           1.3          0.2       0\n",
      "3           4.6          3.1           1.5          0.2       0\n",
      "4           5.0          3.6           1.4          0.2       0\n",
      "\n",
      " é¡åˆ¥åˆ†å¸ƒ:\n",
      "0=setosa     : 50 ç­†\n",
      "1=versicolor : 50 ç­†\n",
      "2=virginica  : 50 ç­†\n",
      "\n",
      "[åŸå§‹è³‡æ–™(æœªæ¨™æº–åŒ–)]\n",
      "sepal_length   mean=  5.8433 std=  0.8253\n",
      "sepal_width    mean=  3.0573 std=  0.4344\n",
      "petal_length   mean=  3.7580 std=  1.7594\n",
      "petal_width    mean=  1.1993 std=  0.7597\n",
      "\n",
      "->å·²å­˜å–20ç­†é è¦½ :iris_course\\artifacts\\iris_preview.csv\n",
      "STEP1 å®Œæˆ\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing  import List\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "ROOT= \"iris_course\"\n",
    "ARTIFACTS = os.path.join(ROOT,'artifacts')\n",
    "os.makedirs(ARTIFACTS,exist_ok=True)\n",
    "\n",
    "# axis=0 èˆ‡ axis=1 å·®åˆ¥\n",
    "# print(\"axis=0ï¼šæ¯æ¬„å¹³å‡\", X.mean(axis=0))\n",
    "# print(\"axis=1ï¼šæ¯åˆ—å¹³å‡\", X.mean(axis=1))\n",
    "\n",
    "# é€™æ®µçš„ä½œç”¨\n",
    "# é€™æ®µæ˜¯ æŠŠç‰¹å¾µåç¨± (names)ã€å¹³å‡å€¼ (m)ã€æ¨™æº–å·® (s) ä¸²åœ¨ä¸€èµ·é€ä¸€è¼¸å‡ºã€‚\n",
    "\n",
    "# ğŸ“Œ è©³ç´°æ‹†è§£\n",
    "# zip(names, m, s)\n",
    "# names æ˜¯ç‰¹å¾µåç¨±çš„å­—ä¸²æ¸…å–®ï¼Œä¾‹å¦‚ï¼š\n",
    "# [\"sepal_length\",\"sepal_width\",\"petal_length\",\"petal_width\"]\n",
    "\n",
    "# m æ˜¯å„æ¬„çš„å¹³å‡å€¼é™£åˆ—ï¼Œä¾‹å¦‚ï¼š\n",
    "# [5.84, 3.05, 3.76, 1.20]\n",
    "\n",
    "# s æ˜¯å„æ¬„çš„æ¨™æº–å·®é™£åˆ—ï¼Œä¾‹å¦‚ï¼š\n",
    "# [0.82, 0.43, 1.76, 0.76]\n",
    "\n",
    "# zip() æœƒæŠŠå®ƒå€‘ä¸€ä¸€å°æ‡‰æ‰“åŒ…æˆ tupleï¼š\n",
    "# [\n",
    "#   (\"sepal_length\", 5.84, 0.82),\n",
    "#   (\"sepal_width\",  3.05, 0.43),\n",
    "#   ...\n",
    "# ]\n",
    "\n",
    "# æ ¼å¼\t     èªªæ˜\n",
    "# {n:<14s}\tå·¦å°é½Šå­—ä¸²ï¼Œå¯¬åº¦ 14\n",
    "# {mi:8.4f}\tæµ®é»æ•¸ï¼Œç¸½å¯¬åº¦ 8ï¼Œé¡¯ç¤º 4 ä½å°æ•¸\n",
    "# {sd:8.4f}\tåŒä¸Š\n",
    "\n",
    "def describe_stats(X:np.ndarray,names:List[str],title:str):\n",
    "    m,s = X.mean(axis=0), X.std(axis=0)\n",
    "    print(f\"\\n[{title}]\")\n",
    "    for n, mi, sd in zip(names, m,s):\n",
    "        print(f\"{n:<14s} mean={mi:8.4f} std={sd:8.4f}\" )\n",
    "\n",
    "print(\"***Step1 è¼‰å…¥è³‡æ–™èˆ‡æ¢ç´¢\")\n",
    "iris =load_iris()\n",
    "# print(iris)\n",
    "\n",
    "\n",
    "# è¼‰å…¥è³‡æ–™é›†ï¼š\n",
    "# x æ˜¯ (150,4) çš„æ•¸å€¼çŸ©é™£\n",
    "# y æ˜¯ (150,) çš„æ¨™ç±¤ï¼ˆ0,1,2ï¼‰\n",
    "# é€™å››å€‹ç‰¹å¾µåˆ†åˆ¥æ˜¯ï¼šèŠ±è¼é•·å¯¬ã€èŠ±ç“£é•·å¯¬\n",
    "x,y = iris.data,iris.target\n",
    "\n",
    "# sepal è¼ç‰‡\n",
    "# petal èŠ±ç“£\n",
    "feature_names = [\"sepal_length\",\"sepal_width\",\"petal_length\",\"petal_width\"]\n",
    "target_names=iris.target_names.tolist()\n",
    "# print(target_names)\n",
    "\n",
    "# é€™è¡Œæœƒç”¨ pandas æŠŠ x è½‰æˆä¸€å€‹ DataFrameï¼ˆè¡¨æ ¼æ ¼å¼ï¼‰ï¼Œæ¬„åå°±æ˜¯ç‰¹å¾µåç¨±ï¼š\n",
    "df=pd.DataFrame(x,columns=feature_names)\n",
    "df[\"target\"] = y\n",
    "print(\"\\n å‰5ç­†è³‡æ–™\");print(df.head())\n",
    "print(\"\\n é¡åˆ¥åˆ†å¸ƒ:\")\n",
    "\n",
    "\n",
    "# enumerate() æ˜¯ Python å…§å»ºå‡½å¼\n",
    "# å®ƒçš„ä½œç”¨æ˜¯ï¼šåœ¨è¿´åœˆä¸­åŒæ™‚å–å¾—ã€Œç´¢å¼•ã€å’Œã€Œå…ƒç´ ã€\n",
    "\n",
    "# å°ç¸½çµ\n",
    "#è¡¨é”å¼\t                           æ„æ€\tçµæœå‹æ…‹\n",
    "# y\té¡åˆ¥æ¨™ç±¤é™£åˆ—\t           ndarray of int\n",
    "# y == i\t          å…ƒç´ é€ä¸€æ˜¯å¦ç­‰æ–¼ i\tndarray of bool\n",
    "# (y == i).sum()\t      æœ‰å¹¾å€‹ç­‰æ–¼ i\tintï¼ˆè¨ˆæ•¸çµæœï¼‰\n",
    "\n",
    "# âœ… æ‰€ä»¥é›–ç„¶ y æ˜¯å€‹æ•´æ•¸é™£åˆ— (ndarray)ï¼Œ\n",
    "# ç”¨ == å»æ¯”å°æ•¸å€¼æ™‚ï¼Œæœƒè‡ªå‹•å°æ¯å€‹å…ƒç´ åšæ¯”è¼ƒï¼Œé€™å°±æ˜¯ NumPy çš„ã€Œå‘é‡åŒ–é‹ç®—ã€ã€‚\n",
    "\n",
    "# target_namesï¼šåªæœƒæ˜¯ ä¸‰å€‹ç¨ç‰¹çš„åç¨± â†’ ['setosa','versicolor','virginica']\n",
    "for i ,name in enumerate(target_names):\n",
    "    print(f\"{i}={name:<10s} : {(y==i).sum()} ç­†\")\n",
    "describe_stats(x,feature_names,\"åŸå§‹è³‡æ–™(æœªæ¨™æº–åŒ–)\")\n",
    "\n",
    "out_csv=os.path.join(ARTIFACTS,\"iris_preview.csv\")\n",
    "# index=False è¡¨ç¤º ä¸è¦è¼¸å‡º DataFrame çš„ç´¢å¼•æ¬„ä½ï¼ˆåªä¿ç•™è³‡æ–™æœ¬èº«ï¼‰\n",
    "df.head(20).to_csv(out_csv,index=False)\n",
    "\n",
    "print(f\"\\n->å·²å­˜å–20ç­†é è¦½ :{out_csv}\")\n",
    "print(\"STEP1 å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b2949c15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===STEP2 | åˆ‡åˆ† Train/Val/Test===\n",
      "åˆ‡åˆ†å½¢ç‹€: train=(96, 4) val=(24, 4) test=(30, 4)\n",
      "STEP2 å®Œæˆ\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "print(\"\\n===STEP2 | åˆ‡åˆ† Train/Val/Test===\")\n",
    "\n",
    "\n",
    "\n",
    "# xï¼šæ‰€æœ‰çš„ç‰¹å¾µè³‡æ–™ï¼ˆnumpy.ndarrayï¼Œshape = (150, 4)ï¼‰\n",
    "# yï¼šæ‰€æœ‰çš„æ¨™ç±¤ï¼ˆnumpy.ndarrayï¼Œshape = (150,)ï¼‰\n",
    "# train_test_split æ˜¯ scikit-learn æä¾›çš„\n",
    "# train_test_split å‡½å¼ï¼Œç”¨ä¾†éš¨æ©ŸæŠŠè³‡æ–™æ‹†æˆå…©çµ„\n",
    "# åƒæ•¸è§£é‡‹\n",
    "# åƒæ•¸\t                            ç”¨é€”\n",
    "# x, y\t             è¼¸å…¥è³‡æ–™èˆ‡æ¨™ç±¤\n",
    "# test_size=0.2\t    æŒ‡å®š 20% ç•¶ã€Œæ¸¬è©¦é›†ã€ï¼Œå‰©ä¸‹ 80% æ˜¯ã€Œè¨“ç·´+é©—è­‰ã€\n",
    "# random_state=42\tè¨­å®šéš¨æ©Ÿç¨®å­ï¼Œè®“çµæœå¯é‡ç¾\n",
    "# stratify(åˆ†å±¤)=y\t    åˆ†å±¤æŠ½æ¨£ï¼šç¢ºä¿ä¸‰å€‹é¡åˆ¥åœ¨å…©çµ„ä¸­æ¯”ä¾‹ä¸€è‡´\n",
    "\n",
    "X_trainval,X_test,y_trainval,y_test =train_test_split(\n",
    "    x,y,test_size=0.2,random_state=42,stratify=y\n",
    ")\n",
    "X_train,X_val,y_train,y_val =train_test_split(\n",
    "    X_trainval,y_trainval,test_size=0.2,random_state=42,stratify=y_trainval\n",
    ")\n",
    "\n",
    "print(f\"åˆ‡åˆ†å½¢ç‹€: train={X_train.shape} val={X_val.shape} test={X_test.shape}\")\n",
    "print(\"STEP2 å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "18de2a5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\====STEP 3 | æ¨™æº–åŒ–(åªç”¨è¨“ç·´é›†fit)ä¸¦å­˜æª”)\n",
      "\n",
      "[è¨“ç·´é›†(æ¨™æº–åŒ–å‰)]\n",
      "sepal_length   mean=  5.8333 std=  0.8516\n",
      "sepal_width    mean=  3.0083 std=  0.4264\n",
      "petal_length   mean=  3.7500 std=  1.7530\n",
      "petal_width    mean=  1.1875 std=  0.7463\n",
      "\n",
      "[è¨“ç·´é›†(æ¨™æº–åŒ–å¾Œ)]\n",
      "sepal_length   mean=  0.0000 std=  1.0000\n",
      "sepal_width    mean=  0.0000 std=  1.0000\n",
      "petal_length   mean=  0.0000 std=  1.0000\n",
      "petal_width    mean=  0.0000 std=  1.0000\n",
      "->å·²å­˜æ¨™æº–åŒ–è³‡æ–™:iris_course\\artifacts\\train_val_test_scaled.npz\n",
      "->å·²å­˜æ¨™æº–åŒ–å™¨:iris_course\\artifacts\\scaler.pkl\n",
      "STEP3 å®Œæˆ\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import joblib\n",
    "\n",
    "print('\\====STEP 3 | æ¨™æº–åŒ–(åªç”¨è¨“ç·´é›†fit)ä¸¦å­˜æª”)')\n",
    "\n",
    "# StandardScaler æ˜¯ä¸€å€‹ã€Œè½‰æ›å™¨ (transformer)ã€ç‰©ä»¶\n",
    "# å®ƒæœƒè¨ˆç®—ï¼š\n",
    "# æ¯å€‹æ¬„ä½çš„å¹³å‡å€¼ Î¼\n",
    "# æ¯å€‹æ¬„ä½çš„æ¨™æº–å·® Ïƒ\n",
    "# ç„¶å¾ŒæŠŠè³‡æ–™å¥—ç”¨å…¬å¼ï¼š\n",
    "\n",
    "# ğ‘§ = (x - ğœ‡) / ğœ\n",
    "# è®“ æ¯å€‹ç‰¹å¾µï¼ˆæ¬„ï¼‰éƒ½è®Šæˆå¹³å‡ 0ã€æ¨™æº–å·® 1\tâ€‹\n",
    "\n",
    "\n",
    "# ç¬¬ 1 è¡Œï¼šå…ˆç”¨è¨“ç·´è³‡æ–™ã€Œå­¸ç¿’å¹³å‡èˆ‡æ¨™æº–å·®ã€\n",
    "# .fit(X_train) æœƒè¨ˆç®—ï¼š\n",
    "# æ¯ä¸€æ¬„çš„å¹³å‡å€¼ mean_\n",
    "# æ¯ä¸€æ¬„çš„æ¨™æº–å·® scale_\n",
    "# é€™ä¸€æ­¥ã€Œåªç”¨è¨“ç·´é›†ã€æ˜¯ç‚ºäº†é¿å…è³‡æ–™æ´©æ¼ï¼ˆä¸èƒ½å·çœ‹é©—è­‰æˆ–æ¸¬è©¦è³‡æ–™ï¼‰\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "\n",
    "# ç¬¬ 2 è¡Œï¼šæŠŠè¨“ç·´è³‡æ–™åšæ¨™æº–åŒ–\n",
    "# ç”¨å‰›å‰›ç®—å‡ºçš„ mean_ å’Œ scale_ æŠŠè³‡æ–™è½‰æ›æˆï¼š\n",
    "# (åŸå€¼ - å¹³å‡) / æ¨™æº–å·®\n",
    "# çµæœï¼šæ¯ä¸€æ¬„çš„å¹³å‡æœƒè®Šæˆ 0ã€æ¨™æº–å·®è®Šæˆ 1\n",
    "X_train_sc = scaler.transform(X_train)\n",
    "\n",
    "# ç¬¬ 3ï½4 è¡Œï¼šç”¨åŒä¸€å€‹ scaler è™•ç†é©—è­‰èˆ‡æ¸¬è©¦è³‡æ–™\n",
    "# é€™è£¡ ä¸èƒ½å† fit ä¸€æ¬¡ï¼Œè¦ç”¨ è¨“ç·´é›†çš„å¹³å‡èˆ‡æ¨™æº–å·® ä¾†è½‰æ›\n",
    "# é€™æ¨£æ‰ç¢ºä¿æ¨¡å‹åœ¨é©—è­‰/æ¸¬è©¦æ™‚ä½¿ç”¨å®Œå…¨ç›¸åŒçš„å°ºåº¦\n",
    "X_val_sc = scaler.transform(X_val)\n",
    "X_test_sc = scaler.transform(X_test)\n",
    "\n",
    "describe_stats(X_train, feature_names,\"è¨“ç·´é›†(æ¨™æº–åŒ–å‰)\")\n",
    "describe_stats(X_train_sc, feature_names,\"è¨“ç·´é›†(æ¨™æº–åŒ–å¾Œ)\")\n",
    "\n",
    "npz_path= os.path.join(ARTIFACTS,\"train_val_test_scaled.npz\")\n",
    "# .npz å°±æ˜¯ æŠŠå¾ˆå¤š NumPy é™£åˆ—ä¸€èµ·æ‰“åŒ…å£“ç¸®å­˜æª”\n",
    "# â†’ è®“ä½ ä¹‹å¾Œå¯ä»¥ ä¸€æ¬¡å­˜ã€ä¸€åŒ…è®€ï¼Œå¾ˆæ–¹ä¾¿ã€‚\n",
    "\n",
    "# é€™è¡Œæœƒå»ºç«‹ä¸€å€‹ train_val_test_scaled.npz æª”ï¼Œè£¡é¢åŒ…å«ï¼š\n",
    "\n",
    "# å­˜é€²å»çš„åç¨±\tå…§å®¹\n",
    "# X_train_sc\tæ¨™æº–åŒ–å¾Œçš„è¨“ç·´ç‰¹å¾µè³‡æ–™\n",
    "# y_train\tè¨“ç·´æ¨™ç±¤\n",
    "# X_val_sc\tæ¨™æº–åŒ–å¾Œçš„é©—è­‰ç‰¹å¾µè³‡æ–™\n",
    "# y_val\té©—è­‰æ¨™ç±¤\n",
    "# X_test_sc\tæ¨™æº–åŒ–å¾Œçš„æ¸¬è©¦ç‰¹å¾µè³‡æ–™\n",
    "# y_test\tæ¸¬è©¦æ¨™ç±¤\n",
    "# feature_names\tç‰¹å¾µåç¨±æ¸…å–®ï¼ˆè½‰æˆé™£åˆ—å­˜ï¼‰\n",
    "# target_names\té¡åˆ¥åç¨±æ¸…å–®ï¼ˆè½‰æˆé™£åˆ—å­˜ï¼‰\n",
    "\n",
    "np.savez(npz_path,\n",
    "         X_train_sc=X_train_sc,y_train=y_train,\n",
    "         X_val_sc=X_val_sc,y_val=y_val,\n",
    "         X_test_sc=X_test_sc,y_test=y_test,\n",
    "         feature_names=np.array(feature_names,dtype=object),\n",
    "         target_names=np.array(target_names,dtype=object),\n",
    ")\n",
    "\n",
    "# é€™å…©è¡Œçš„ç›®çš„\n",
    "\n",
    "# æŠŠä½ è¨“ç·´å¥½çš„ StandardScaler ç‰©ä»¶\n",
    "# å­˜æˆä¸€å€‹æª”æ¡ˆï¼ˆscaler.pklï¼‰ï¼Œ\n",
    "# ä»¥å¾Œè¦ç”¨æ™‚å¯ä»¥ç›´æ¥è¼‰å›ä¾†ï¼Œä¸ç”¨é‡æ–° .fit() ä¸€æ¬¡ã€‚\n",
    "\n",
    "# æŠŠç‰©ä»¶å­˜èµ·ä¾†\n",
    "# joblib.dump(scaler, scaler_path)\n",
    "# ä½¿ç”¨ joblib çš„ dump å‡½å¼\n",
    "# æŠŠ scalerï¼ˆä¹Ÿå°±æ˜¯ä½ ç”¨ X_train .fit() éçš„ StandardScalerï¼‰å­˜æˆ .pkl æª”æ¡ˆ\n",
    "# .pkl æ˜¯ã€Œpickleã€æ ¼å¼ï¼Œç”¨ä¾†å­˜æ•´å€‹ Python ç‰©ä»¶\n",
    "\n",
    "scaler_path =os.path.join(ARTIFACTS,\"scaler.pkl\")\n",
    "joblib.dump(scaler,scaler_path)\n",
    "\n",
    "print(f\"->å·²å­˜æ¨™æº–åŒ–è³‡æ–™:{npz_path}\")\n",
    "print(f\"->å·²å­˜æ¨™æº–åŒ–å™¨:{scaler_path}\")\n",
    "print(\"STEP3 å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f154bf7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\====STEP 4 | Tensor èˆ‡ DataLoader\n",
      "ç¬¬ä¸€å€‹ batch:xb.shape=torch.Size([16, 4]), yb.shape=torch.Size([16])\n",
      "   xb[0](æ¨™æº–åŒ–å¾Œ)=[-1.0960394144058228, -1.1921887397766113, 0.42784440517425537, 0.6867437362670898]\n",
      "   yb[0](é¡åˆ¥)=2\n",
      "->å·²å­˜batch é è¦½iris_course\\artifacts\\batch_preview.csv\n",
      "STEP4 å®Œæˆ\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset,DataLoader\n",
    "# æ•´é«”ç›®æ¨™\n",
    "# é€™æ®µæ˜¯ STEP4ï¼šæŠŠè³‡æ–™è½‰æˆ torch.Tensorï¼Œå†åŒ…é€² DataLoaderï¼Œç”¨ä¾†è¨“ç·´æ¨¡å‹\n",
    "# æ˜¯ PyTorch çš„æ¨™æº–è³‡æ–™è™•ç†æµç¨‹ã€‚\n",
    "# ç°¡å–®ä¾†èªªï¼š\n",
    "# æŠŠè³‡æ–™(numpy.ndarray) â†’ è½‰æˆ Tensor â†’ ç”¨ DataLoader åˆ†æˆå°æ‰¹æ¬¡ï¼ˆbatchï¼‰ â†’ ä¹‹å¾Œå¯ä»¥ä¸Ÿçµ¦æ¨¡å‹è¨“ç·´\n",
    "print('\\====STEP 4 | Tensor èˆ‡ DataLoader')\n",
    "\n",
    "\n",
    "X_train_t = torch.tensor(X_train_sc,dtype=torch.float32)\n",
    "y_train_t = torch.tensor(y_train,dtype=torch.long)\n",
    "X_val_t = torch.tensor(X_val_sc,dtype=torch.float32)\n",
    "y_val_t = torch.tensor(y_val,dtype=torch.long)\n",
    "\n",
    "\n",
    "# TensorDataset(X, y)ï¼šæŠŠ X å’Œ y åŒ…æˆä¸€ç­†ä¸€ç­†çš„è³‡æ–™\n",
    "# DataLoader(..., batch_size=16)ï¼šæ¯æ¬¡æœƒåå‡º 16 ç­†è³‡æ–™ï¼ˆå°æ‰¹æ¬¡ï¼‰\n",
    "# shuffle=Trueï¼šè¨“ç·´é›†æœƒéš¨æ©Ÿæ‰“äº‚é †åºï¼ˆé¿å…æ¨¡å‹è¨˜ä½é †åºï¼‰\n",
    "# shuffle=Falseï¼šé©—è­‰é›†ä¿æŒåŸé †åº\n",
    "# ç‚ºä»€éº¼é©—è­‰/æ¸¬è©¦é›†ä¸è¦ shuffle\n",
    "# é©—è­‰æ™‚åªæ˜¯ã€Œè©•ä¼°ã€æ¨¡å‹è¡¨ç¾ï¼Œä¸éœ€è¦ä¹Ÿä¸æ‡‰æ‰“äº‚\n",
    "# ä¿æŒå›ºå®šé †åº â†’ æ–¹ä¾¿å°ç…§é æ¸¬èˆ‡çœŸå¯¦æ¨™ç±¤\n",
    "# æ¯æ¬¡è©•ä¼°çµæœä¸€è‡´ï¼Œé¿å…éš¨æ©Ÿæ€§å½±éŸ¿è©•ä¼°\n",
    "train_loader =DataLoader(TensorDataset(X_train_t,y_train_t),batch_size=16,shuffle=True)\n",
    "val_loader =DataLoader(TensorDataset(X_val_t,y_val_t),batch_size=16,shuffle=False)\n",
    "\n",
    "\n",
    "# ç›®çš„\n",
    "\n",
    "# å¾ DataLoaderï¼ˆPyTorchï¼‰\n",
    "# å–å‡ºã€Œç¬¬ä¸€å€‹ batchï¼ˆå°æ‰¹æ¬¡ï¼‰ã€çš„è³‡æ–™ï¼Œ\n",
    "# ç”¨ä¾† æª¢æŸ¥è³‡æ–™é•·ç›¸æ˜¯å¦æ­£ç¢ºã€‚\n",
    "\n",
    "\n",
    "# train_loader æ˜¯ä½ å‰›å»ºç«‹çš„ DataLoaderï¼Œè£¡é¢æœ‰æ‰€æœ‰è¨“ç·´è³‡æ–™ï¼ˆå·²åˆ†å¥½ batchï¼‰\n",
    "# iter(train_loader) â†’ å»ºç«‹ä¸€å€‹ã€Œè¿­ä»£å™¨ã€\n",
    "# next(...) â†’ å¾è¿­ä»£å™¨ä¸­å–å‡ºç¬¬ä¸€çµ„ (ç‰¹å¾µ, æ¨™ç±¤)\n",
    "# çµæœæœƒæ˜¯ï¼š\n",
    "# xb = ä¸€å€‹ batch çš„ç‰¹å¾µè³‡æ–™\n",
    "# shape é€šå¸¸æ˜¯ (batch_size, ç‰¹å¾µæ•¸)\n",
    "# ä¾‹å¦‚ (16, 4)\n",
    "# yb = ä¸€å€‹ batch çš„æ¨™ç±¤è³‡æ–™\n",
    "# shape æ˜¯ (batch_size,)\n",
    "# ä¾‹å¦‚ (16,)\n",
    "\n",
    "# é‡é»è§€å¿µ\n",
    "# shuffle=True çš„ æ‰“äº‚æ™‚æ©Ÿä¸æ˜¯åœ¨ä½ å»ºç«‹ DataLoader çš„ç•¶ä¸‹ï¼Œ\n",
    "# è€Œæ˜¯åœ¨ä½ ç¬¬ä¸€æ¬¡å‘¼å« iter(train_loader)ï¼ˆé–‹å§‹ä¸€å€‹ epochï¼‰æ™‚æ‰æ‰“äº‚è³‡æ–™é †åºã€‚\n",
    "\n",
    "xb,yb = next(iter(train_loader))\n",
    "print(f\"ç¬¬ä¸€å€‹ batch:xb.shape={xb.shape}, yb.shape={yb.shape}\")\n",
    "\n",
    "# å–å‡º batch è£¡ç¬¬ä¸€ç­†è³‡æ–™çš„æ¨™ç±¤\n",
    "print(f\"   xb[0](æ¨™æº–åŒ–å¾Œ)={xb[0].tolist()}\")\n",
    "print(f\"   yb[0](é¡åˆ¥)={yb[0].item()}\")\n",
    "\n",
    "\n",
    "# åŠŸèƒ½ï¼ˆä¸€å¥è©±ï¼‰\n",
    "# æŠŠä½ å‰›å¾ DataLoader å–å‡ºçš„é‚£ä¸€å€‹ batchï¼ˆå°æ‰¹æ¬¡ï¼‰\n",
    "# è½‰æˆè¡¨æ ¼ï¼ˆpandas.DataFrameï¼‰ï¼ŒåŠ ä¸Šæ¨™ç±¤æ¬„ä½ï¼Œ\n",
    "# å†å­˜æˆ CSV æª”ï¼Œæ–¹ä¾¿ä½ ç”¨ Excel æˆ–å…¶ä»–å·¥å…·æª¢æŸ¥ã€‚\n",
    "batch_preview=os.path.join(ARTIFACTS,\"batch_preview.csv\")\n",
    "pd.DataFrame(xb.numpy(),columns=feature_names).assign(label=yb.numpy()).to_csv(batch_preview,index=False)\n",
    "print(f\"->å·²å­˜batch é è¦½{batch_preview}\")\n",
    "print(\"STEP4 å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "70183213",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\====STEP 5 | å®šç¾©æ¨¡å‹èˆ‡åƒæ•¸é‡\n",
      "IrisMLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=4, out_features=64, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.2, inplace=False)\n",
      "    (3): Linear(in_features=64, out_features=32, bias=True)\n",
      "    (4): ReLU()\n",
      "    (5): Dropout(p=0.2, inplace=False)\n",
      "    (6): Linear(in_features=32, out_features=3, bias=True)\n",
      "  )\n",
      ")\n",
      "å¯è¨“ç·´åƒæ•¸é‡:2,499\n",
      "-> å·²å­˜çµæ§‹æè¿°:iris_course\\models\\model_arch.txt\n",
      "STEP5 å®Œæˆ\n"
     ]
    }
   ],
   "source": [
    "from torch import nn \n",
    "MODELS = os.path.join(ROOT,\"models\")\n",
    "os.makedirs(MODELS,exist_ok=True)\n",
    "\n",
    "print('\\====STEP 5 | å®šç¾©æ¨¡å‹èˆ‡åƒæ•¸é‡')\n",
    "\n",
    "class IrisMLP(nn.Module):\n",
    "    def __init__(self, in_dim=4, hidden1=64,hidden2=32,out_dim=3,dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim,hidden1),nn.ReLU(),nn.Dropout(dropout),\n",
    "            nn.Linear(hidden1,hidden2),nn.ReLU(),nn.Dropout(dropout),\n",
    "            nn.Linear(hidden2,out_dim)\n",
    "        )\n",
    "\n",
    "\n",
    "    #     forwardï¼šå®šç¾©å‰å‘å‚³é\n",
    "    # def forward(self, x):\n",
    "    #     return self.net(x)\n",
    "\n",
    "\n",
    "    # å‘Šè¨´ PyTorch è³‡æ–™è¦æ€éº¼ç¶“éç¶²è·¯\n",
    "\n",
    "    # åªè¦å‘¼å« model(x)ï¼Œå°±æœƒè‡ªå‹•è·‘é€™æ®µ\n",
    "    def forward(self,x): return self.net(x)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# è¨ˆç®— PyTorch æ¨¡å‹ä¸­ã€Œå¯è¨“ç·´åƒæ•¸ã€çš„ç¸½æ•¸\n",
    "# ä¹Ÿå°±æ˜¯ï¼šé€™å€‹æ¨¡å‹è£¡ æ‰€æœ‰éœ€è¦æ›´æ–°çš„æ¬Šé‡åƒæ•¸ ä¸€å…±æœ‰å¹¾å€‹æ•¸å€¼ï¼ˆweights / biasesï¼‰ã€‚\n",
    "\n",
    "# ä½ å•çš„ -> int æ˜¯ Type Hintï¼ˆå‹åˆ¥è¨»è§£ï¼‰ çš„ä¸€ç¨®ï¼Œ\n",
    "# ä¸æ˜¯ç¨‹å¼åŠŸèƒ½çš„ä¸€éƒ¨åˆ†ï¼Œåªæ˜¯ã€Œå‘Šè¨´äººæˆ–å·¥å…·ï¼šé€™å€‹å‡½å¼æœƒå›å‚³ä»€éº¼å‹åˆ¥ã€ã€‚\n",
    "def count_trainable_params(model: nn.Module) -> int:\n",
    "\n",
    "    #     p.numel()\n",
    "    # å›å‚³é€™å€‹ Tensor è£¡ã€Œæœ‰å¹¾å€‹å…ƒç´ ã€\n",
    "    # ä¾‹å¦‚ï¼š\n",
    "    # p.shape = (64, 4) â†’ p.numel() = 256\n",
    "    # p.shape = (64,)   â†’ p.numel() = 64\n",
    "\n",
    "    #     (p.numel() for p in ...) æ˜¯ ç”Ÿæˆå™¨ï¼Œä¸æ˜¯é™£åˆ—ï¼Œ\n",
    "    # å®ƒæœƒã€Œä¸€å€‹ä¸€å€‹ç”¢ç”Ÿæ•¸å­—ã€ï¼Œè®“ sum() å»åŠ ç¸½ï¼Œ\n",
    "    # è€Œä¸æœƒå…ˆæŠŠæ‰€æœ‰æ•¸å­—å­˜åœ¨è¨˜æ†¶é«”è£¡ã€‚\n",
    "\n",
    "    # requires_grad æ˜¯ä»€éº¼\n",
    "    # å®ƒçš„æ„æ€æ˜¯ï¼š\n",
    "    # é€™å€‹å¼µé‡æ˜¯å¦è¦åœ¨åå‘å‚³æ’­ï¼ˆbackpropagationï¼‰æ™‚è¨ˆç®—æ¢¯åº¦\n",
    "    # æœ‰äº›åƒæ•¸å¯èƒ½ï¼š\n",
    "    # æ˜¯å‡çµçš„ï¼ˆä¸æƒ³è¨“ç·´ï¼‰\n",
    "    # æ˜¯å›ºå®šçš„ embedding æˆ–é è¨“ç·´æ¬Šé‡\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "\n",
    "# æª¢æŸ¥ä½ çš„é›»è…¦æœ‰æ²’æœ‰ NVIDIA GPUï¼ˆCUDAï¼‰å¯ä»¥ç”¨ï¼Œ\n",
    "# ç„¶å¾Œè¨­å®šä¸€å€‹ torch.device ç‰©ä»¶ï¼Œ\n",
    "# è®“ä½ ä¹‹å¾Œå¯ä»¥æŠŠæ¨¡å‹æˆ–è³‡æ–™ç§»åˆ° GPU æˆ– CPU åŸ·è¡Œã€‚\n",
    "# device =torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# å»ºç«‹æ¨¡å‹ç‰©ä»¶\n",
    "# æŠŠæ¨¡å‹æ¬åˆ°æŒ‡å®šçš„è£ç½®ï¼ˆCPU / GPUï¼‰ä¸Š\n",
    "model = IrisMLP().to(device)\n",
    "print(model)\n",
    "print(f\"å¯è¨“ç·´åƒæ•¸é‡:{count_trainable_params(model):,}\")\n",
    "\n",
    "arch_txt =os.path.join(MODELS,\"model_arch.txt\")\n",
    "\n",
    "# åŠŸèƒ½ç¸½è¦½\n",
    "\n",
    "# æŠŠ æ¨¡å‹çš„çµæ§‹ å’Œ å¯è¨“ç·´åƒæ•¸ç¸½æ•¸\n",
    "# å¯«é€²ä¸€å€‹æ–‡å­—æª”ï¼ˆarch_txtï¼‰\n",
    "\n",
    "# ç”¨ Python å…§å»ºçš„æª”æ¡ˆæ“ä½œ\n",
    "# \"w\" â†’ ä»¥ã€Œå¯«å…¥æ¨¡å¼ã€é–‹å•Ÿæª”æ¡ˆ\n",
    "# encoding=\"utf-8\" â†’ ç”¨ UTF-8 ç·¨ç¢¼ï¼ˆå¯ä»¥æ­£å¸¸å¯«ä¸­æ–‡ï¼‰\n",
    "# as f â†’ å»ºç«‹æª”æ¡ˆç‰©ä»¶ f\n",
    "# with æœƒåœ¨å¯«å®Œå¾Œè‡ªå‹•é—œé–‰æª”æ¡ˆ\n",
    "\n",
    "with open(arch_txt,\"w\",encoding=\"utf-8\") as f:\n",
    "    f.write(str(model) + \"\\n\")\n",
    "    f.write(f\"trainable_params={count_trainable_params(model)}\\n\")\n",
    "print(f\"-> å·²å­˜çµæ§‹æè¿°:{arch_txt}\")\n",
    "print(\"STEP5 å®Œæˆ\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "48be2f56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== STEP 6 è¨“ç·´ (å«æ—©åœ) ===\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== STEP 6 è¨“ç·´ (å«æ—©åœ) ===\")\n",
    "\n",
    "# é€™æ˜¯å®šç¾©ã€Œæå¤±å‡½å¼ (Loss Function)ã€çš„éƒ¨åˆ†\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# é€™æ˜¯å®šç¾©ã€Œå„ªåŒ–å™¨ (Optimizer)ã€ï¼Œä¹Ÿå°±æ˜¯æ›´æ–°æ¨¡å‹åƒæ•¸çš„æ¼”ç®—æ³•ã€‚\n",
    "# Adamï¼šä¸€ç¨®æ”¹è‰¯ç‰ˆçš„ SGDï¼Œæœƒæ ¹æ“šæ­·å²æ¢¯åº¦çš„å¤§å°è‡ªå‹•èª¿æ•´å­¸ç¿’ç‡ (learning rate)ï¼Œæ”¶æ–‚é€Ÿåº¦é€šå¸¸æ¯”å–®ç´”çš„ SGD å¿«ã€‚\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "def evaluate(m, loader):\n",
    "\n",
    "    # 1ï¸âƒ£ m.eval()\n",
    "    # æŠŠæ¨¡å‹åˆ‡æ›åˆ°ã€Œæ¨è«–æ¨¡å¼ã€ã€‚\n",
    "    # æœƒåœç”¨ Dropoutã€BatchNorm ç­‰åƒ…è¨“ç·´ç”¨çš„æ©Ÿåˆ¶ï¼Œç¢ºä¿é©—è­‰çµæœç©©å®šã€‚\n",
    "    m.eval()\n",
    "\n",
    "    total, correct, loss_sum = 0, 0, 0.0\n",
    "\n",
    "    # 2ï¸âƒ£ torch.no_grad()\n",
    "    # åœ¨é€™å€‹å€å¡Šå…§ä¸æœƒè¨˜éŒ„æ¢¯åº¦ï¼Œç¯€çœè¨˜æ†¶é«”å’Œé‹ç®—é‡ã€‚\n",
    "    # é©—è­‰æˆ–æ¨è«–æ™‚ä¸éœ€è¦åå‘å‚³æ’­ï¼Œæ‰€ä»¥é—œæ‰ gradient trackingã€‚\n",
    "    with torch.no_grad():\n",
    "        # 3ï¸âƒ£ for xb, yb in loader\n",
    "        # å¾ DataLoader ä¸€æ‰¹ä¸€æ‰¹è®€è³‡æ–™ã€‚\n",
    "        # xb = è¼¸å…¥ (features)ï¼Œyb = æ¨™ç±¤ (labels)ã€‚\n",
    "        for xb, yb in loader:\n",
    "            # 4ï¸âƒ£ xb, yb = xb.to(device), yb.to(device)\n",
    "            # æŠŠè³‡æ–™æ¬åˆ° deviceï¼ˆå¯èƒ½æ˜¯ GPU æˆ– CPUï¼‰ã€‚\n",
    "            # ç¢ºä¿è³‡æ–™å’Œæ¨¡å‹åœ¨åŒä¸€è£ç½®ä¸Šï¼Œä¸ç„¶æœƒå ±éŒ¯ã€‚\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "\n",
    "            #5ï¸âƒ£ logits = m(xb)\n",
    "            #m â†’ ä½ çš„æ¨¡å‹ (ä¾‹å¦‚ä¸€å€‹ nn.Module å®šç¾©çš„ç¥ç¶“ç¶²è·¯)ã€‚\n",
    "            # xb â†’ ä¸€å€‹ batch çš„è¼¸å…¥è³‡æ–™ (features)ï¼Œé€šå¸¸ shape = [batch_size, in_dim]ã€‚\n",
    "            # logits = æ¨¡å‹è¼¸å‡ºçš„ã€ŒåŸå§‹åˆ†æ•¸ã€(linear output)ï¼Œé‚„æ²’æœ‰ç¶“é softmax è½‰æ›æˆæ©Ÿç‡ã€‚\n",
    "            # å‰å‘å‚³æ’­ (forward pass)ï¼Œè¼¸å‡º logitsï¼ˆæœªç¶“ softmax çš„åˆ†æ•¸ï¼‰ã€‚\n",
    "            # shape é€šå¸¸æ˜¯ [batch_size, num_classes]ã€‚\n",
    "            logits = m(xb)\n",
    "\n",
    "            print('logits',logits)\n",
    "            print('yb',yb)\n",
    "\n",
    "            # 7ï¸âƒ£ æå¤±è¨ˆç®—\n",
    "            # loss_sum += criterion(logits, yb).item() * xb.size(0)\n",
    "            # criterion(logits, yb) = ç®—é€™å€‹ batch çš„å¹³å‡ lossã€‚\n",
    "            # .item() å–å‡º Python floatã€‚\n",
    "            # ä¹˜ä¸Š xb.size(0)ï¼ˆbatch sizeï¼‰ï¼Œè½‰æˆã€Œç¸½ lossã€ï¼Œæ–¹ä¾¿æœ€å¾ŒåšåŠ æ¬Šå¹³å‡ã€‚\n",
    "            loss_sum += criterion(logits, yb).item() * xb.size(0)\n",
    "\n",
    "            # 8ï¸âƒ£ æ­£ç¢ºç‡è¨ˆç®—\n",
    "            # correct += (logits.argmax(1) == yb).sum().item()\n",
    "            # argmax = argument of the maximum\n",
    "            # å°±æ˜¯ã€Œæ‰¾å‡ºä¸€å€‹åºåˆ—è£¡ æœ€å¤§å€¼çš„ä½ç½® (ç´¢å¼•)ã€ã€‚\n",
    "            # logits.argmax(1) â†’ æ‰¾å‡ºæ¯ä¸€ç­†è³‡æ–™é æ¸¬çš„é¡åˆ¥ç´¢å¼•ã€‚\n",
    "            # == yb â†’ å’ŒçœŸå¯¦æ¨™ç±¤æ¯”å°ï¼Œå›å‚³ True/False tensorã€‚\n",
    "            # .sum() â†’ è¨ˆç®—é€™å€‹ batch é æ¸¬æ­£ç¢ºçš„æ•¸é‡ã€‚\n",
    "            correct += (logits.argmax(1) == yb).sum().item()\n",
    "\n",
    "            #9ï¸âƒ£ ç´¯ç©æ¨£æœ¬æ•¸\n",
    "            # total += yb.size(0)\n",
    "            # ç´€éŒ„ä¸€å…±çœ‹éå¤šå°‘ç­†æ¨£æœ¬ã€‚\n",
    "            # æœ€å¾Œæ‰èƒ½ç®—å¹³å‡ loss å’Œæ­£ç¢ºç‡ã€‚\n",
    "            total += yb.size(0)\n",
    "    return loss_sum / total, correct / total\n",
    "\n",
    "\n",
    "# å„è®Šæ•¸çš„ç”¨é€”ï¼š\n",
    "# best_state = None\n",
    "# ç”¨ä¾†å­˜æ¨¡å‹ç›®å‰ã€Œæœ€ä½³ç‹€æ…‹ã€(é€šå¸¸æ˜¯ state_dict() çš„è¤‡æœ¬)ã€‚\n",
    "# å‰›é–‹å§‹é‚„æ²’æœ‰æœ€ä½³æ¨¡å‹ï¼Œæ‰€ä»¥æ˜¯ Noneã€‚\n",
    "\n",
    "# best_val = -1.0\n",
    "# ç”¨ä¾†è¨˜éŒ„ã€Œé©—è­‰é›† (validation) çš„æœ€ä½³è¡¨ç¾ã€\n",
    "# è¨­æˆ -1.0 æ˜¯å› ç‚ºæˆ‘å€‘å¸Œæœ›å¾Œé¢ç¬¬ä¸€æ¬¡é©—è­‰çµæœä¸€å®šæœƒæ¯”å®ƒå¥½ï¼ˆå‡è¨­æº–ç¢ºç‡ â‰¥ 0ï¼‰ã€‚\n",
    "\n",
    "# patience = 15\n",
    "# ä»£è¡¨ã€Œå®¹å¿é€£çºŒå¤šå°‘æ¬¡è¡¨ç¾æ²’æœ‰é€²æ­¥ã€\n",
    "# å¦‚æœè¶…é 15 æ¬¡ epoch éƒ½æ²’æœ‰æ”¹å–„ï¼Œå°±è§¸ç™¼ early stoppingï¼Œåœæ­¢è¨“ç·´ã€‚\n",
    "\n",
    "# bad = 0\n",
    "# è¨˜éŒ„ã€Œå·²ç¶“é€£çºŒå¹¾æ¬¡æ²’æœ‰é€²æ­¥ã€\n",
    "# æ¯ç•¶ val_acc æ²’æœ‰è®Šå¥½ï¼Œå°± bad += 1ï¼›æœ‰è®Šå¥½å°± bad = 0ã€‚\n",
    "best_state, best_val, patience, bad = None, -1.0, 15, 0\n",
    "hist = {\"tr_loss\": [], \"tr_acc\": [], \"va_loss\": [], \"va_acc\": []}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "34056bd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits tensor([[-0.3058, -0.0048,  0.1059],\n",
      "        [-0.1441,  0.0519, -0.0442],\n",
      "        [-0.1442,  0.0864, -0.0293],\n",
      "        [-0.2625, -0.0158,  0.0164],\n",
      "        [ 0.3275, -0.0046, -0.1956],\n",
      "        [-0.3512, -0.0322,  0.1340],\n",
      "        [ 0.3038, -0.0171, -0.1763],\n",
      "        [-0.2666,  0.0081,  0.0818],\n",
      "        [ 0.1845,  0.0703, -0.1612],\n",
      "        [-0.1050,  0.0419, -0.0637],\n",
      "        [-0.0844,  0.0860, -0.1062],\n",
      "        [-0.1384,  0.0228, -0.0238],\n",
      "        [ 0.2918, -0.0391, -0.1539],\n",
      "        [-0.5315, -0.0209, -0.0087],\n",
      "        [-0.0315,  0.1546, -0.0969],\n",
      "        [-0.4474, -0.0102, -0.0110]])\n",
      "yb tensor([2, 1, 2, 1, 0, 2, 0, 2, 0, 1, 1, 1, 0, 2, 1, 2])\n",
      "logits tensor([[ 0.2448,  0.0699, -0.1733],\n",
      "        [ 0.2152,  0.0338, -0.1785],\n",
      "        [-0.2530,  0.0175,  0.0403],\n",
      "        [-0.2758, -0.0062,  0.0820],\n",
      "        [-0.1488,  0.0265, -0.0156],\n",
      "        [ 0.2631,  0.0658, -0.1719],\n",
      "        [ 0.2426,  0.0263, -0.1847],\n",
      "        [-0.0220,  0.1573, -0.0973]])\n",
      "yb tensor([0, 0, 2, 2, 1, 0, 0, 1])\n",
      "Epoch 001 | train_loss=1.0005 acc=0.729 | val_loss=0.9564 acc=0.875\n",
      "logits tensor([[-4.1246e-01, -2.6632e-02,  2.2820e-01],\n",
      "        [-2.0052e-01,  4.1729e-02,  2.4316e-04],\n",
      "        [-2.2426e-01,  7.7610e-02,  3.7291e-02],\n",
      "        [-3.5801e-01, -3.5631e-02,  9.5741e-02],\n",
      "        [ 5.2452e-01, -5.7623e-02, -2.0238e-01],\n",
      "        [-4.8212e-01, -6.1363e-02,  2.5748e-01],\n",
      "        [ 5.0381e-01, -7.6007e-02, -1.7706e-01],\n",
      "        [-3.5713e-01, -1.1838e-02,  1.8663e-01],\n",
      "        [ 3.1380e-01,  4.4402e-02, -1.9359e-01],\n",
      "        [-1.4186e-01,  3.3297e-02, -2.8266e-02],\n",
      "        [-1.1309e-01,  8.8289e-02, -9.0672e-02],\n",
      "        [-1.7580e-01,  7.3823e-03,  2.5179e-02],\n",
      "        [ 4.9367e-01, -1.0574e-01, -1.4729e-01],\n",
      "        [-7.1076e-01, -5.6086e-02,  1.4681e-01],\n",
      "        [-5.0184e-02,  1.6689e-01, -1.0364e-01],\n",
      "        [-6.0225e-01, -4.2490e-02,  1.1564e-01]])\n",
      "yb tensor([2, 1, 2, 1, 0, 2, 0, 2, 0, 1, 1, 1, 0, 2, 1, 2])\n",
      "logits tensor([[ 0.3870,  0.0385, -0.1826],\n",
      "        [ 0.3510,  0.0022, -0.1950],\n",
      "        [-0.3533, -0.0099,  0.1234],\n",
      "        [-0.3882, -0.0396,  0.1878],\n",
      "        [-0.2127,  0.0084,  0.0484],\n",
      "        [ 0.4173,  0.0356, -0.1956],\n",
      "        [ 0.3842, -0.0068, -0.1991],\n",
      "        [-0.0461,  0.1695, -0.1012]])\n",
      "yb tensor([0, 0, 2, 2, 1, 0, 0, 1])\n",
      "Epoch 002 | train_loss=0.9412 acc=0.802 | val_loss=0.8868 acc=0.833\n",
      "logits tensor([[-0.5652, -0.0561,  0.3857],\n",
      "        [-0.2821,  0.0276,  0.0598],\n",
      "        [-0.3312,  0.0661,  0.1244],\n",
      "        [-0.4904, -0.0577,  0.1977],\n",
      "        [ 0.7352, -0.1071, -0.2085],\n",
      "        [-0.6457, -0.0828,  0.4280],\n",
      "        [ 0.7157, -0.1273, -0.1828],\n",
      "        [-0.4797, -0.0410,  0.3221],\n",
      "        [ 0.4407,  0.0209, -0.2331],\n",
      "        [-0.2040,  0.0223,  0.0250],\n",
      "        [-0.1535,  0.0852, -0.0611],\n",
      "        [-0.2390, -0.0126,  0.0983],\n",
      "        [ 0.7039, -0.1665, -0.1410],\n",
      "        [-0.9108, -0.0724,  0.3691],\n",
      "        [-0.0827,  0.1827, -0.1011],\n",
      "        [-0.7829, -0.0624,  0.2982]])\n",
      "yb tensor([2, 1, 2, 1, 0, 2, 0, 2, 0, 1, 1, 1, 0, 2, 1, 2])\n",
      "logits tensor([[ 0.5366,  0.0229, -0.2089],\n",
      "        [ 0.5011, -0.0151, -0.2210],\n",
      "        [-0.4826, -0.0421,  0.2238],\n",
      "        [-0.5237, -0.0620,  0.3267],\n",
      "        [-0.3003, -0.0124,  0.1308],\n",
      "        [ 0.5587,  0.0096, -0.2371],\n",
      "        [ 0.5476, -0.0273, -0.2237],\n",
      "        [-0.0846,  0.1850, -0.0951]])\n",
      "yb tensor([0, 0, 2, 2, 1, 0, 0, 1])\n",
      "Epoch 003 | train_loss=0.8684 acc=0.885 | val_loss=0.8150 acc=0.792\n",
      "logits tensor([[-0.7317, -0.0789,  0.5623],\n",
      "        [-0.3703,  0.0150,  0.1212],\n",
      "        [-0.4486,  0.0524,  0.2092],\n",
      "        [-0.6330, -0.0867,  0.3017],\n",
      "        [ 0.9814, -0.1570, -0.2411],\n",
      "        [-0.8228, -0.1117,  0.6080],\n",
      "        [ 0.9609, -0.1826, -0.2130],\n",
      "        [-0.6205, -0.0694,  0.4664],\n",
      "        [ 0.5886, -0.0024, -0.2879],\n",
      "        [-0.2772,  0.0109,  0.0755],\n",
      "        [-0.1990,  0.0863, -0.0334],\n",
      "        [-0.3124, -0.0385,  0.1711],\n",
      "        [ 0.9478, -0.2306, -0.1585],\n",
      "        [-1.1423, -0.0920,  0.5924],\n",
      "        [-0.1210,  0.1998, -0.1049],\n",
      "        [-0.9888, -0.0791,  0.4847]])\n",
      "yb tensor([2, 1, 2, 1, 0, 2, 0, 2, 0, 1, 1, 1, 0, 2, 1, 2])\n",
      "logits tensor([[ 7.1354e-01, -6.2500e-04, -2.4826e-01],\n",
      "        [ 6.7707e-01, -3.9392e-02, -2.5772e-01],\n",
      "        [-6.0751e-01, -6.5241e-02,  3.3343e-01],\n",
      "        [-6.7170e-01, -8.1316e-02,  4.7578e-01],\n",
      "        [-4.0045e-01, -3.0277e-02,  2.1488e-01],\n",
      "        [ 7.1876e-01, -1.6472e-02, -2.9580e-01],\n",
      "        [ 7.3529e-01, -5.4818e-02, -2.6123e-01],\n",
      "        [-1.2993e-01,  2.0194e-01, -9.4982e-02]])\n",
      "yb tensor([0, 0, 2, 2, 1, 0, 0, 1])\n",
      "Epoch 004 | train_loss=0.8078 acc=0.875 | val_loss=0.7442 acc=0.792\n",
      "logits tensor([[-0.9125, -0.1122,  0.7397],\n",
      "        [-0.4629,  0.0118,  0.1911],\n",
      "        [-0.5665,  0.0445,  0.2975],\n",
      "        [-0.7832, -0.1011,  0.4223],\n",
      "        [ 1.2683, -0.2124, -0.3037],\n",
      "        [-1.0212, -0.1470,  0.7989],\n",
      "        [ 1.2503, -0.2456, -0.2761],\n",
      "        [-0.7685, -0.1036,  0.6271],\n",
      "        [ 0.7534, -0.0319, -0.3546],\n",
      "        [-0.3536,  0.0080,  0.1315],\n",
      "        [-0.2525,  0.0908, -0.0034],\n",
      "        [-0.3850, -0.0726,  0.2460],\n",
      "        [ 1.2271, -0.3044, -0.2043],\n",
      "        [-1.4019, -0.1212,  0.8031],\n",
      "        [-0.1677,  0.2163, -0.1119],\n",
      "        [-1.2153, -0.1014,  0.6666]])\n",
      "yb tensor([2, 1, 2, 1, 0, 2, 0, 2, 0, 1, 1, 1, 0, 2, 1, 2])\n",
      "logits tensor([[ 0.9107, -0.0465, -0.3072],\n",
      "        [ 0.8713, -0.0824, -0.3151],\n",
      "        [-0.7455, -0.0829,  0.4572],\n",
      "        [-0.8371, -0.1058,  0.6294],\n",
      "        [-0.4951, -0.0457,  0.3124],\n",
      "        [ 0.9094, -0.0574, -0.3646],\n",
      "        [ 0.9528, -0.0956, -0.3172],\n",
      "        [-0.1840,  0.2187, -0.0973]])\n",
      "yb tensor([0, 0, 2, 2, 1, 0, 0, 1])\n",
      "Epoch 005 | train_loss=0.7509 acc=0.865 | val_loss=0.6767 acc=0.792\n",
      "logits tensor([[-1.1186, -0.1444,  0.9306],\n",
      "        [-0.5714,  0.0137,  0.2585],\n",
      "        [-0.7042,  0.0419,  0.3914],\n",
      "        [-0.9425, -0.1069,  0.5419],\n",
      "        [ 1.5692, -0.2773, -0.3824],\n",
      "        [-1.2483, -0.1761,  1.0035],\n",
      "        [ 1.5591, -0.3105, -0.3664],\n",
      "        [-0.9395, -0.1334,  0.7926],\n",
      "        [ 0.9261, -0.0714, -0.4257],\n",
      "        [-0.4405,  0.0136,  0.1900],\n",
      "        [-0.3178,  0.1007,  0.0265],\n",
      "        [-0.4630, -0.0896,  0.3301],\n",
      "        [ 1.5296, -0.3817, -0.2789],\n",
      "        [-1.6926, -0.1386,  1.0249],\n",
      "        [-0.2220,  0.2353, -0.1186],\n",
      "        [-1.4684, -0.1144,  0.8516]])\n",
      "yb tensor([2, 1, 2, 1, 0, 2, 0, 2, 0, 1, 1, 1, 0, 2, 1, 2])\n",
      "logits tensor([[ 1.1189, -0.1016, -0.3746],\n",
      "        [ 1.0756, -0.1347, -0.3810],\n",
      "        [-0.9033, -0.0955,  0.5890],\n",
      "        [-1.0274, -0.1253,  0.7949],\n",
      "        [-0.6051, -0.0567,  0.4128],\n",
      "        [ 1.1185, -0.1118, -0.4393],\n",
      "        [ 1.1747, -0.1521, -0.3868],\n",
      "        [-0.2467,  0.2380, -0.0990]])\n",
      "yb tensor([0, 0, 2, 2, 1, 0, 0, 1])\n",
      "Epoch 006 | train_loss=0.6856 acc=0.802 | val_loss=0.6159 acc=0.792\n",
      "logits tensor([[-1.3498, -0.1718,  1.1289],\n",
      "        [-0.6980,  0.0208,  0.3184],\n",
      "        [-0.8678,  0.0473,  0.4822],\n",
      "        [-1.1280, -0.1030,  0.6583],\n",
      "        [ 1.8912, -0.3439, -0.4859],\n",
      "        [-1.4990, -0.2035,  1.2161],\n",
      "        [ 1.8845, -0.3770, -0.4789],\n",
      "        [-1.1312, -0.1584,  0.9631],\n",
      "        [ 1.1046, -0.1178, -0.5103],\n",
      "        [-0.5405,  0.0252,  0.2456],\n",
      "        [-0.4021,  0.1177,  0.0504],\n",
      "        [-0.5479, -0.0985,  0.4165],\n",
      "        [ 1.8617, -0.4586, -0.3867],\n",
      "        [-2.0386, -0.1470,  1.2464],\n",
      "        [-0.3014,  0.2640, -0.1321],\n",
      "        [-1.7656, -0.1178,  1.0373]])\n",
      "yb tensor([2, 1, 2, 1, 0, 2, 0, 2, 0, 1, 1, 1, 0, 2, 1, 2])\n",
      "logits tensor([[ 1.3337, -0.1633, -0.4611],\n",
      "        [ 1.2877, -0.1922, -0.4623],\n",
      "        [-1.0870, -0.1013,  0.7229],\n",
      "        [-1.2433, -0.1386,  0.9657],\n",
      "        [-0.7298, -0.0638,  0.5143],\n",
      "        [ 1.3331, -0.1726, -0.5333],\n",
      "        [ 1.4066, -0.2140, -0.4711],\n",
      "        [-0.3355,  0.2671, -0.1060]])\n",
      "yb tensor([0, 0, 2, 2, 1, 0, 0, 1])\n",
      "Epoch 007 | train_loss=0.6122 acc=0.823 | val_loss=0.5622 acc=0.792\n",
      "logits tensor([[-1.5889, -0.1769,  1.3159],\n",
      "        [-0.8285,  0.0483,  0.3602],\n",
      "        [-1.0452,  0.0775,  0.5478],\n",
      "        [-1.3331, -0.0717,  0.7502],\n",
      "        [ 2.2181, -0.4278, -0.6247],\n",
      "        [-1.7762, -0.2024,  1.4064],\n",
      "        [ 2.2188, -0.4564, -0.6070],\n",
      "        [-1.3272, -0.1660,  1.1222],\n",
      "        [ 1.3032, -0.1724, -0.6109],\n",
      "        [-0.6474,  0.0494,  0.2823],\n",
      "        [-0.4841,  0.1511,  0.0555],\n",
      "        [-0.6379, -0.0921,  0.4903],\n",
      "        [ 2.2063, -0.5412, -0.5197],\n",
      "        [-2.4214, -0.1057,  1.4215],\n",
      "        [-0.3694,  0.3124, -0.1720],\n",
      "        [-2.0948, -0.0772,  1.1799]])\n",
      "yb tensor([2, 1, 2, 1, 0, 2, 0, 2, 0, 1, 1, 1, 0, 2, 1, 2])\n",
      "logits tensor([[ 1.5634, -0.2348, -0.5658],\n",
      "        [ 1.5126, -0.2605, -0.5648],\n",
      "        [-1.2894, -0.0825,  0.8414],\n",
      "        [-1.4789, -0.1338,  1.1163],\n",
      "        [-0.8615, -0.0554,  0.6004],\n",
      "        [ 1.5685, -0.2433, -0.6445],\n",
      "        [ 1.6499, -0.2882, -0.5805],\n",
      "        [-0.4219,  0.3122, -0.1344]])\n",
      "yb tensor([0, 0, 2, 2, 1, 0, 0, 1])\n",
      "Epoch 008 | train_loss=0.5624 acc=0.823 | val_loss=0.5160 acc=0.792\n",
      "logits tensor([[-1.8584, -0.1491,  1.4648],\n",
      "        [-0.9575,  0.0878,  0.3857],\n",
      "        [-1.2247,  0.1220,  0.5951],\n",
      "        [-1.5393, -0.0245,  0.8197],\n",
      "        [ 2.5678, -0.5339, -0.7974],\n",
      "        [-2.0703, -0.1706,  1.5624],\n",
      "        [ 2.5766, -0.5538, -0.7800],\n",
      "        [-1.5412, -0.1426,  1.2498],\n",
      "        [ 1.5267, -0.2400, -0.7304],\n",
      "        [-0.7551,  0.0880,  0.3018],\n",
      "        [-0.5623,  0.1928,  0.0453],\n",
      "        [-0.7319, -0.0665,  0.5400],\n",
      "        [ 2.5804, -0.6355, -0.7032],\n",
      "        [-2.8102, -0.0386,  1.5630],\n",
      "        [-0.4282,  0.3691, -0.2314],\n",
      "        [-2.4305, -0.0151,  1.2949]])\n",
      "yb tensor([2, 1, 2, 1, 0, 2, 0, 2, 0, 1, 1, 1, 0, 2, 1, 2])\n",
      "logits tensor([[ 1.8182, -0.3192, -0.6897],\n",
      "        [ 1.7611, -0.3413, -0.6863],\n",
      "        [-1.4982, -0.0510,  0.9348],\n",
      "        [-1.7298, -0.1023,  1.2354],\n",
      "        [-1.0035, -0.0295,  0.6647],\n",
      "        [ 1.8314, -0.3279, -0.7762],\n",
      "        [ 1.9178, -0.3759, -0.7101],\n",
      "        [-0.4924,  0.3685, -0.1844]])\n",
      "yb tensor([0, 0, 2, 2, 1, 0, 0, 1])\n",
      "Epoch 009 | train_loss=0.5193 acc=0.854 | val_loss=0.4759 acc=0.792\n",
      "logits tensor([[-2.1419, -0.1236,  1.6229],\n",
      "        [-1.0947,  0.1262,  0.4168],\n",
      "        [-1.4188,  0.1622,  0.6581],\n",
      "        [-1.7498,  0.0250,  0.8927],\n",
      "        [ 2.9019, -0.6498, -0.9678],\n",
      "        [-2.3803, -0.1393,  1.7253],\n",
      "        [ 2.9155, -0.6645, -0.9497],\n",
      "        [-1.7774, -0.1200,  1.3777],\n",
      "        [ 1.7426, -0.3093, -0.8551],\n",
      "        [-0.8693,  0.1252,  0.3235],\n",
      "        [-0.6483,  0.2344,  0.0390],\n",
      "        [-0.8324, -0.0378,  0.5835],\n",
      "        [ 2.9419, -0.7326, -0.9052],\n",
      "        [-3.2101,  0.0260,  1.7244],\n",
      "        [-0.5013,  0.4271, -0.2764],\n",
      "        [-2.7772,  0.0445,  1.4281]])\n",
      "yb tensor([2, 1, 2, 1, 0, 2, 0, 2, 0, 1, 1, 1, 0, 2, 1, 2])\n",
      "logits tensor([[ 2.0667, -0.4046, -0.8187],\n",
      "        [ 2.0024, -0.4231, -0.8129],\n",
      "        [-1.7189, -0.0188,  1.0334],\n",
      "        [-1.9919, -0.0719,  1.3656],\n",
      "        [-1.1599, -0.0041,  0.7317],\n",
      "        [ 2.0884, -0.4155, -0.9123],\n",
      "        [ 2.1778, -0.4638, -0.8456],\n",
      "        [-0.5790,  0.4269, -0.2243]])\n",
      "yb tensor([0, 0, 2, 2, 1, 0, 0, 1])\n",
      "Epoch 010 | train_loss=0.5008 acc=0.865 | val_loss=0.4433 acc=0.792\n",
      "logits tensor([[-2.4135, -0.1127,  1.7830],\n",
      "        [-1.2312,  0.1618,  0.4484],\n",
      "        [-1.6191,  0.1962,  0.7282],\n",
      "        [-1.9458,  0.0715,  0.9589],\n",
      "        [ 3.2232, -0.7613, -1.1429],\n",
      "        [-2.6730, -0.1223,  1.8874],\n",
      "        [ 3.2372, -0.7710, -1.1247],\n",
      "        [-2.0010, -0.1100,  1.5079],\n",
      "        [ 1.9426, -0.3744, -0.9882],\n",
      "        [-0.9754,  0.1623,  0.3396],\n",
      "        [-0.7348,  0.2779,  0.0294],\n",
      "        [-0.9282, -0.0138,  0.6216],\n",
      "        [ 3.2816, -0.8332, -1.1022],\n",
      "        [-3.5937,  0.0825,  1.8812],\n",
      "        [-0.5856,  0.4909, -0.3228],\n",
      "        [-3.1101,  0.0965,  1.5586]])\n",
      "yb tensor([2, 1, 2, 1, 0, 2, 0, 2, 0, 1, 1, 1, 0, 2, 1, 2])\n",
      "logits tensor([[ 2.3039, -0.4867, -0.9516],\n",
      "        [ 2.2317, -0.5016, -0.9434],\n",
      "        [-1.9291,  0.0046,  1.1320],\n",
      "        [-2.2422, -0.0531,  1.4966],\n",
      "        [-1.3087,  0.0146,  0.8001],\n",
      "        [ 2.3327, -0.4998, -1.0527],\n",
      "        [ 2.4258, -0.5481, -0.9853],\n",
      "        [-0.6746,  0.4916, -0.2637]])\n",
      "yb tensor([0, 0, 2, 2, 1, 0, 0, 1])\n",
      "Epoch 011 | train_loss=0.4487 acc=0.885 | val_loss=0.4167 acc=0.792\n",
      "logits tensor([[-2.6752, -0.0941,  1.9190],\n",
      "        [-1.3586,  0.2055,  0.4652],\n",
      "        [-1.8162,  0.2349,  0.7847],\n",
      "        [-2.1178,  0.1342,  0.9965],\n",
      "        [ 3.5282, -0.8673, -1.3208],\n",
      "        [-2.9537, -0.0962,  2.0229],\n",
      "        [ 3.5363, -0.8733, -1.3042],\n",
      "        [-2.2141, -0.0925,  1.6137],\n",
      "        [ 2.1366, -0.4382, -1.1232],\n",
      "        [-1.0742,  0.2051,  0.3430],\n",
      "        [-0.8178,  0.3264,  0.0103],\n",
      "        [-1.0168,  0.0172,  0.6441],\n",
      "        [ 3.5972, -0.9345, -1.2917],\n",
      "        [-3.9705,  0.1550,  2.0000],\n",
      "        [-0.6683,  0.5577, -0.3859],\n",
      "        [-3.4386,  0.1630,  1.6544]])\n",
      "yb tensor([2, 1, 2, 1, 0, 2, 0, 2, 0, 1, 1, 1, 0, 2, 1, 2])\n",
      "logits tensor([[ 2.5298, -0.5652, -1.0870],\n",
      "        [ 2.4497, -0.5765, -1.0761],\n",
      "        [-2.1317,  0.0353,  1.2119],\n",
      "        [-2.4833, -0.0270,  1.6058],\n",
      "        [-1.4477,  0.0407,  0.8542],\n",
      "        [ 2.5667, -0.5809, -1.1959],\n",
      "        [ 2.6610, -0.6284, -1.1273],\n",
      "        [-0.7681,  0.5620, -0.3148]])\n",
      "yb tensor([0, 0, 2, 2, 1, 0, 0, 1])\n",
      "Epoch 012 | train_loss=0.4315 acc=0.812 | val_loss=0.3935 acc=0.792\n",
      "logits tensor([[-2.9110, -0.0943,  2.0609],\n",
      "        [-1.4653,  0.2477,  0.4739],\n",
      "        [-1.9989,  0.2664,  0.8436],\n",
      "        [-2.2729,  0.1876,  1.0283],\n",
      "        [ 3.8178, -0.9615, -1.5014],\n",
      "        [-3.2085, -0.0901,  2.1662],\n",
      "        [ 3.8219, -0.9643, -1.4895],\n",
      "        [-2.4048, -0.0910,  1.7239],\n",
      "        [ 2.3267, -0.4909, -1.2616],\n",
      "        [-1.1610,  0.2444,  0.3437],\n",
      "        [-0.8944,  0.3783, -0.0186],\n",
      "        [-1.0924,  0.0413,  0.6665],\n",
      "        [ 3.8884, -1.0245, -1.4834],\n",
      "        [-4.3251,  0.2072,  2.1215],\n",
      "        [-0.7508,  0.6292, -0.4611],\n",
      "        [-3.7483,  0.2129,  1.7516]])\n",
      "yb tensor([2, 1, 2, 1, 0, 2, 0, 2, 0, 1, 1, 1, 0, 2, 1, 2])\n",
      "logits tensor([[ 2.7483, -0.6349, -1.2258],\n",
      "        [ 2.6602, -0.6423, -1.2122],\n",
      "        [-2.3171,  0.0531,  1.2967],\n",
      "        [-2.7006, -0.0159,  1.7168],\n",
      "        [-1.5631,  0.0616,  0.9017],\n",
      "        [ 2.7945, -0.6521, -1.3429],\n",
      "        [ 2.8871, -0.6994, -1.2727],\n",
      "        [-0.8610,  0.6342, -0.3824]])\n",
      "yb tensor([0, 0, 2, 2, 1, 0, 0, 1])\n",
      "Epoch 013 | train_loss=0.3885 acc=0.854 | val_loss=0.3728 acc=0.792\n",
      "logits tensor([[-3.1341, -0.0897,  2.1954],\n",
      "        [-1.5689,  0.2996,  0.4674],\n",
      "        [-2.1762,  0.3058,  0.8945],\n",
      "        [-2.4197,  0.2537,  1.0320],\n",
      "        [ 4.0615, -1.0300, -1.6722],\n",
      "        [-3.4476, -0.0788,  2.3006],\n",
      "        [ 4.0609, -1.0287, -1.6655],\n",
      "        [-2.5808, -0.0823,  1.8244],\n",
      "        [ 2.4897, -0.5284, -1.3899],\n",
      "        [-1.2441,  0.2906,  0.3368],\n",
      "        [-0.9749,  0.4353, -0.0590],\n",
      "        [-1.1677,  0.0747,  0.6811],\n",
      "        [ 4.1316, -1.0876, -1.6642],\n",
      "        [-4.6597,  0.2795,  2.2118],\n",
      "        [-0.8354,  0.7065, -0.5411],\n",
      "        [-4.0323,  0.2844,  1.8136]])\n",
      "yb tensor([2, 1, 2, 1, 0, 2, 0, 2, 0, 1, 1, 1, 0, 2, 1, 2])\n",
      "logits tensor([[ 2.9339e+00, -6.8632e-01, -1.3567e+00],\n",
      "        [ 2.8381e+00, -6.8986e-01, -1.3408e+00],\n",
      "        [-2.4924e+00,  8.0635e-02,  1.3698e+00],\n",
      "        [-2.9062e+00, -4.2219e-04,  1.8216e+00],\n",
      "        [-1.6701e+00,  9.0476e-02,  9.3859e-01],\n",
      "        [ 2.9882e+00, -7.0453e-01, -1.4807e+00],\n",
      "        [ 3.0781e+00, -7.5058e-01, -1.4103e+00],\n",
      "        [-9.5603e-01,  7.1185e-01, -4.5467e-01]])\n",
      "yb tensor([0, 0, 2, 2, 1, 0, 0, 1])\n",
      "Epoch 014 | train_loss=0.3913 acc=0.896 | val_loss=0.3532 acc=0.792\n",
      "logits tensor([[-3.3553, -0.0743,  2.3145],\n",
      "        [-1.6762,  0.3560,  0.4510],\n",
      "        [-2.3590,  0.3510,  0.9320],\n",
      "        [-2.5521,  0.3302,  1.0115],\n",
      "        [ 4.2805, -1.0851, -1.8315],\n",
      "        [-3.6748, -0.0537,  2.4120],\n",
      "        [ 4.2757, -1.0799, -1.8287],\n",
      "        [-2.7525, -0.0631,  1.9082],\n",
      "        [ 2.6328, -0.5554, -1.5063],\n",
      "        [-1.3290,  0.3432,  0.3219],\n",
      "        [-1.0569,  0.4957, -0.1026],\n",
      "        [-1.2429,  0.1167,  0.6871],\n",
      "        [ 4.3509, -1.1369, -1.8331],\n",
      "        [-4.9598,  0.3685,  2.2682],\n",
      "        [-0.9255,  0.7901, -0.6187],\n",
      "        [-4.2945,  0.3673,  1.8532]])\n",
      "yb tensor([2, 1, 2, 1, 0, 2, 0, 2, 0, 1, 1, 1, 0, 2, 1, 2])\n",
      "logits tensor([[ 3.0992, -0.7268, -1.4778],\n",
      "        [ 2.9961, -0.7266, -1.4597],\n",
      "        [-2.6582,  0.1180,  1.4246],\n",
      "        [-3.0973,  0.0289,  1.9017],\n",
      "        [-1.7791,  0.1256,  0.9688],\n",
      "        [ 3.1588, -0.7448, -1.6068],\n",
      "        [ 3.2485, -0.7906, -1.5379],\n",
      "        [-1.0550,  0.7951, -0.5242]])\n",
      "yb tensor([0, 0, 2, 2, 1, 0, 0, 1])\n",
      "Epoch 015 | train_loss=0.3737 acc=0.865 | val_loss=0.3348 acc=0.833\n",
      "logits tensor([[-3.5497, -0.0901,  2.4749],\n",
      "        [-1.7824,  0.3951,  0.4717],\n",
      "        [-2.5265,  0.3737,  1.0103],\n",
      "        [-2.6768,  0.3764,  1.0485],\n",
      "        [ 4.4900, -1.1365, -1.9820],\n",
      "        [-3.8847, -0.0678,  2.5796],\n",
      "        [ 4.4815, -1.1282, -1.9817],\n",
      "        [-2.9155, -0.0771,  2.0421],\n",
      "        [ 2.7575, -0.5721, -1.6284],\n",
      "        [-1.4137,  0.3805,  0.3344],\n",
      "        [-1.1409,  0.5457, -0.1234],\n",
      "        [-1.3116,  0.1416,  0.7213],\n",
      "        [ 4.5634, -1.1841, -1.9895],\n",
      "        [-5.2469,  0.4031,  2.4242],\n",
      "        [-1.0266,  0.8678, -0.6787],\n",
      "        [-4.5469,  0.4041,  1.9791]])\n",
      "yb tensor([2, 1, 2, 1, 0, 2, 0, 2, 0, 1, 1, 1, 0, 2, 1, 2])\n",
      "logits tensor([[ 3.2547, -0.7631, -1.5946],\n",
      "        [ 3.1446, -0.7593, -1.5738],\n",
      "        [-2.8072,  0.1300,  1.5164],\n",
      "        [-3.2815,  0.0226,  2.0368],\n",
      "        [-1.8839,  0.1389,  1.0338],\n",
      "        [ 3.3175, -0.7799, -1.7284],\n",
      "        [ 3.4092, -0.8266, -1.6600],\n",
      "        [-1.1622,  0.8702, -0.5744]])\n",
      "yb tensor([0, 0, 2, 2, 1, 0, 0, 1])\n",
      "Epoch 016 | train_loss=0.3245 acc=0.896 | val_loss=0.3215 acc=0.833\n",
      "æ—©åœï¼š15 epochs æœªæå‡\n"
     ]
    }
   ],
   "source": [
    "# è¨“ç·´è¿´åœˆ\n",
    "# for ep in range(1, 201):\n",
    "# è¨“ç·´æœ€å¤š 200 å€‹ epochã€‚\n",
    "# å¦‚æœææ—© early stoppingï¼Œå°±æœƒ breakã€‚\n",
    "for ep in range(1, 201):\n",
    "    # ğŸ‹ï¸â€â™‚ï¸ æ¨¡å‹è¨“ç·´ (train mode)\n",
    "    # model.train()\n",
    "    # total, correct, loss_sum = 0, 0, 0.0\n",
    "    # åˆ‡æ›åˆ°ã€Œè¨“ç·´æ¨¡å¼ã€(å•Ÿç”¨ dropoutã€BN ç­‰)ã€‚\n",
    "    # totalï¼šç´¯è¨ˆè¨“ç·´æ¨£æœ¬æ•¸\n",
    "    # correctï¼šç´¯è¨ˆé æ¸¬æ­£ç¢ºæ•¸\n",
    "    # loss_sumï¼šç´¯è¨ˆç¸½ loss\n",
    "    model.train()\n",
    "    total, correct, loss_sum = 0, 0, 0.0\n",
    "\n",
    "    # ä¸€å€‹ batch çš„è¨“ç·´\n",
    "    # for xb, yb in train_loader:\n",
    "    #     xb, yb = xb.to(device), yb.to(device)\n",
    "    #     logits = model(xb)\n",
    "    #     loss = criterion(logits, yb)\n",
    "\n",
    "    # å–ä¸€å€‹ batch (xb, yb)\n",
    "    # ä¸Ÿé€² GPU/CPU\n",
    "    # å‰å‘å‚³æ’­ â†’ å¾—åˆ° logits (åˆ†æ•¸)\n",
    "    # è¨ˆç®— loss (CrossEntropyLoss)\n",
    "    for xb, yb in train_loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        logits = model(xb)\n",
    "        loss = criterion(logits, yb)\n",
    "\n",
    "        #â¬…ï¸ åå‘å‚³æ’­ + åƒæ•¸æ›´æ–°\n",
    "        # optimizer.zero_grad()\n",
    "        #loss.backward()\n",
    "        #optimizer.step()\n",
    "\n",
    "        # zero_grad()ï¼šæ¸…æ‰ä¸Šä¸€è¼ªçš„æ¢¯åº¦\n",
    "        # loss.backward()ï¼šåå‘å‚³æ’­ï¼Œç®—å‡ºæ¢¯åº¦\n",
    "        # step()ï¼šç”¨ optimizer (Adam) æ›´æ–°åƒæ•¸\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        loss_sum += loss.item() * xb.size(0)\n",
    "        correct += (logits.argmax(1) == yb).sum().item()\n",
    "        total += yb.size(0)\n",
    "    \n",
    "    tr_loss, tr_acc = loss_sum / total, correct / total\n",
    "\n",
    "    # é©—è­‰ (validation)\n",
    "    # va_loss, va_acc = evaluate(model, val_loader)\n",
    "    # ç”¨å‰›æ‰å¯«çš„ evaluate å‡½æ•¸ç®— validation loss å’Œ accuracyã€‚\n",
    "    va_loss, va_acc = evaluate(model, val_loader)\n",
    "    \n",
    "    hist[\"tr_loss\"].append(tr_loss)\n",
    "    hist[\"tr_acc\"].append(tr_acc)\n",
    "    hist[\"va_loss\"].append(va_loss)\n",
    "    hist[\"va_acc\"].append(va_acc)\n",
    "    \n",
    "    print(f\"Epoch {ep:03d} | train_loss={tr_loss:.4f} acc={tr_acc:.3f} | val_loss={va_loss:.4f} acc={va_acc:.3f}\")\n",
    "\n",
    "\n",
    "    # Early Stopping\n",
    "    # è§£é‡‹ï¼š\n",
    "    # å¦‚æœ validation accuracy è®Šå¥½ â†’ æ›´æ–° best_valï¼Œå­˜ä¸‹æ¨¡å‹ç‹€æ…‹ best_stateï¼Œä¸¦ reset bad = 0ã€‚\n",
    "    # å¦‚æœæ²’æœ‰é€²æ­¥ â†’ bad += 1\n",
    "    # å¦‚æœé€£çºŒ patience (15) æ¬¡éƒ½æ²’é€²æ­¥ â†’ æ—©åœ (early stopping)ï¼Œç›´æ¥çµæŸè¨“ç·´ã€‚\n",
    "    #é€™ä¸€è¡Œï¼š\n",
    "    # {k: v.cpu() for k, v in model.state_dict().items()}\n",
    "    # ç­‰åƒ¹æ–¼ï¼š\n",
    "    # å–å‡ºæ¨¡å‹æ‰€æœ‰åƒæ•¸ (state_dict)\n",
    "    # æŠŠæ¯å€‹åƒæ•¸å¾ GPU æ¬åˆ° CPU\n",
    "    # å­˜æˆä¸€å€‹æ–°çš„å­—å…¸\n",
    "    # é€™å€‹æ–°å­—å…¸å°±è¢«å­˜åœ¨ best_state è£¡ï¼Œç”¨ä¾†ä¿å­˜ã€Œæœ€ä½³æ¨¡å‹ã€çš„åƒæ•¸ã€‚\n",
    "    if va_acc > best_val:\n",
    "     best_val, best_state,bad = va_acc, {k: v.cpu() for k, v in model.state_dict().items()},0\n",
    "    else:\n",
    "        bad += 1\n",
    "        if bad >= patience:\n",
    "            print(f\"æ—©åœï¼š{patience} epochs æœªæå‡\")\n",
    "            break\n",
    "\n",
    "\n",
    "# è¼‰å›æœ€ä½³\n",
    "if best_state is not None:\n",
    "    model.load_state_dict(best_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ecbd0f27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… å·²å­˜è¨“ç·´æ›²ç·š: iris_course\\plots\\curves.png\n",
      "âœ… å·²å­˜æœ€ä½³æ¬Šé‡: iris_course\\models\\best.pt\n",
      "STEP 6 âœ… å®Œæˆ\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# === ç•«è¨“ç·´/é©—è­‰æ›²ç·š ===\n",
    "xs = np.arange(1, len(hist[\"tr_loss\"]) + 1)\n",
    "plt.figure(figsize=(8, 4))\n",
    "\n",
    "# loss æ›²ç·š\n",
    "plt.plot(xs, hist[\"tr_loss\"], label=\"train_loss\")\n",
    "plt.plot(xs, hist[\"va_loss\"], label=\"val_loss\")\n",
    "\n",
    "# acc æ›²ç·š\n",
    "plt.plot(xs, hist[\"tr_acc\"], label=\"train_acc\")\n",
    "plt.plot(xs, hist[\"va_acc\"], label=\"val_acc\")\n",
    "\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"value\")\n",
    "plt.title(\"Training Curves\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "\n",
    "PLOTS = os.path.join(ROOT, \"plots\")\n",
    "\n",
    "# ç¢ºä¿è³‡æ–™å¤¾å­˜åœ¨\n",
    "os.makedirs(PLOTS, exist_ok=True)\n",
    "\n",
    "# å„²å­˜åœ–è¡¨\n",
    "curve_path = os.path.join(PLOTS, \"curves.png\")\n",
    "plt.savefig(curve_path, dpi=150)\n",
    "plt.close()\n",
    "print(f\"âœ… å·²å­˜è¨“ç·´æ›²ç·š: {curve_path}\")\n",
    "\n",
    "# === å„²å­˜æœ€ä½³æ¨¡å‹æ¬Šé‡ ===\n",
    "best_path = os.path.join(MODELS, \"best.pt\")\n",
    "torch.save(model.state_dict(), best_path)\n",
    "print(f\"âœ… å·²å­˜æœ€ä½³æ¬Šé‡: {best_path}\")\n",
    "\n",
    "print(\"STEP 6 âœ… å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4037ebf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== STEP 7 æ¸¬è©¦é›†è©•ä¼° ===\n",
      "Test Accuracy = 0.800\n",
      "\n",
      "åˆ†é¡å ±å‘Šï¼š\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      setosa      1.000     1.000     1.000        10\n",
      "  versicolor      1.000     0.400     0.571        10\n",
      "   virginica      0.625     1.000     0.769        10\n",
      "\n",
      "    accuracy                          0.800        30\n",
      "   macro avg      0.875     0.800     0.780        30\n",
      "weighted avg      0.875     0.800     0.780        30\n",
      "\n",
      "->å·²å­˜æ··æ·†çŸ©é™£: iris_course\\plots\\confusion_matrix.png\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "print(\"\\n=== STEP 7 æ¸¬è©¦é›†è©•ä¼° ===\")\n",
    "\n",
    "# æŠŠæ¨¡å‹åˆ‡æ›åˆ° è©•ä¼°æ¨¡å¼ (evaluation mode)ã€‚\n",
    "# é—œæ‰ dropoutã€batch normalization çš„éš¨æ©Ÿè¡Œç‚ºï¼Œè®“æ¸¬è©¦çµæœç©©å®šã€‚\n",
    "# æ¨¡å‹è¨“ç·´å¥½ ä¸æœƒè‡ªå·±é—œæ‰ Dropout/BNï¼Œå› ç‚º PyTorch ä¸çŸ¥é“ä½ è¦åšã€Œé©—è­‰ã€é‚„æ˜¯ã€Œç¹¼çºŒè¨“ç·´ã€ã€‚\n",
    "model.eval()\n",
    "\n",
    "# é—œé–‰æ¢¯åº¦è¨ˆç®—ï¼Œç¯€çœè¨˜æ†¶é«”å’Œé‹ç®—é‡ã€‚\n",
    "# æ¸¬è©¦æ™‚åªè¦ forwardï¼Œä¸éœ€è¦ backwardã€‚\n",
    "with torch.no_grad():\n",
    "\n",
    "    # X_test_sc â†’ æ¸¬è©¦é›†çš„è¼¸å…¥ç‰¹å¾µ (é€šå¸¸æ˜¯æ¨™æº–åŒ–å¾Œçš„è³‡æ–™)ã€‚\n",
    "    # torch.tensor(..., dtype=torch.float32) â†’ æŠŠ numpy array è½‰æˆ PyTorch tensorï¼Œä¸¦æŒ‡å®šæµ®é»æ•¸å‹åˆ¥ã€‚\n",
    "    # .to(device) â†’ æŠŠè³‡æ–™æ¬åˆ°è·Ÿæ¨¡å‹ç›¸åŒçš„è£ç½®ï¼ˆCPU æˆ– GPUï¼‰ã€‚\n",
    "    # model(...) â†’ å‰å‘å‚³æ’­ï¼Œå¾—åˆ° logitsï¼ˆæœªç¶“ softmax çš„åˆ†æ•¸ï¼‰ã€‚\n",
    "    # å‡è¨­æ¸¬è©¦é›†æœ‰ 100 ç­†è³‡æ–™ï¼Œé¡åˆ¥æ•¸ = 3ï¼Œ\n",
    "    # é‚£ logits.shape = [100, 3]ã€‚\n",
    "    logits = model(torch.tensor(X_test_sc, dtype=torch.float32).to(device))\n",
    "\n",
    "\n",
    "    # logits.argmax(1) â†’ æ²¿è‘—é¡åˆ¥ç¶­åº¦æ‰¾æœ€å¤§å€¼çš„ç´¢å¼•ï¼Œä¹Ÿå°±æ˜¯ã€Œæ¨¡å‹é æ¸¬çš„é¡åˆ¥ã€ã€‚\n",
    "    # e.g. [2.0, 1.0, -0.5] â†’ argmax = 0 (é æ¸¬ class 0)\n",
    "    # .cpu() â†’ æŠŠçµæœç§»å› CPUï¼ˆå› ç‚ºæœ‰å¯èƒ½åœ¨ GPU ä¸Šé‹ç®—ï¼‰ã€‚\n",
    "    # .numpy() â†’ è½‰æˆ numpy arrayï¼Œæ–¹ä¾¿å¾ŒçºŒç”¨ sklearn.metrics è¨ˆç®— accuracyã€æ··æ·†çŸ©é™£ç­‰ã€‚\n",
    "    # æœ€å¾Œ y_pred æ˜¯ä¸€å€‹é•·åº¦ = æ¸¬è©¦è³‡æ–™ç­†æ•¸çš„ arrayï¼Œä¾‹å¦‚ï¼š\n",
    "    # array([0, 2, 1, 0, 1, 2, ...])\n",
    "    y_pred = logits.argmax(1).cpu().numpy()\n",
    "\n",
    "# è¨ˆç®—æº–ç¢ºç‡\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(f\"Test Accuracy = {acc:.3f}\\n\")\n",
    "\n",
    "# åˆ†é¡å ±å‘Š\n",
    "print(\"åˆ†é¡å ±å‘Šï¼š\")\n",
    "print(classification_report(y_test, y_pred, target_names=target_names, digits=3))\n",
    "    \n",
    "# === æ··æ·†çŸ©é™£ ===\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(4.5, 4))\n",
    "plt.imshow(cm, interpolation=\"nearest\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.colorbar()\n",
    "\n",
    "ticks = np.arange(len(target_names))\n",
    "plt.xticks(ticks, target_names, rotation=30)\n",
    "plt.yticks(ticks, target_names)\n",
    "\n",
    "# åœ¨çŸ©é™£æ ¼å­ä¸­å¡«ä¸Šæ•¸å­—\n",
    "for i in range(cm.shape[0]):\n",
    "    for j in range(cm.shape[1]):\n",
    "        plt.text(j, i, str(cm[i, j]), ha=\"center\", va=\"center\")\n",
    "\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.tight_layout()\n",
    "\n",
    "cm_path = os.path.join(PLOTS, \"confusion_matrix.png\")\n",
    "plt.savefig(cm_path, dpi=150)\n",
    "plt.close()\n",
    "print(f\"->å·²å­˜æ··æ·†çŸ©é™£: {cm_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "83ea9f23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   sepal_length  sepal_width  petal_length  petal_width        True  \\\n",
      "0           4.4          3.0           1.3          0.2      setosa   \n",
      "1           6.1          3.0           4.9          1.8   virginica   \n",
      "2           4.9          2.4           3.3          1.0  versicolor   \n",
      "3           5.0          2.3           3.3          1.0  versicolor   \n",
      "4           4.4          3.2           1.3          0.2      setosa   \n",
      "5           6.3          3.3           4.7          1.6  versicolor   \n",
      "6           4.6          3.6           1.0          0.2      setosa   \n",
      "7           5.4          3.4           1.7          0.2      setosa   \n",
      "8           6.5          3.0           5.2          2.0   virginica   \n",
      "9           5.4          3.0           4.5          1.5  versicolor   \n",
      "\n",
      "         Pred  \n",
      "0      setosa  \n",
      "1   virginica  \n",
      "2  versicolor  \n",
      "3  versicolor  \n",
      "4      setosa  \n",
      "5   virginica  \n",
      "6      setosa  \n",
      "7      setosa  \n",
      "8   virginica  \n",
      "9   virginica  \n",
      "å·²å­˜æ¨£æœ¬å°ç…§(åŸå–®ä½): iris_course\\artifacts\\test_samples.csv\n",
      "STEP 7 å®Œæˆ âœ… ï¼ˆå…¨æµç¨‹ï¼‰\n"
     ]
    }
   ],
   "source": [
    "# === å›åˆ°åŸå–®ä½é¡¯ç¤ºå¹¾ç­† ===\n",
    "\n",
    "# X_test_sc â†’ ä¹‹å‰ç¶“éæ¨™æº–åŒ– (scaler) çš„æ¸¬è©¦é›†ç‰¹å¾µã€‚\n",
    "# scaler.inverse_transform(...) â†’ æŠŠæ¨™æº–åŒ–å¾Œçš„è³‡æ–™é‚„åŸæˆåŸæœ¬çš„æ•¸å€¼å–®ä½ã€‚\n",
    "# ä¾‹å¦‚ï¼šèŠ±ç“£é•·åº¦åŸæœ¬æ˜¯ 3.5 cmï¼Œæ¨™æº–åŒ–å¾Œå¯èƒ½æ˜¯ -0.23ï¼Œç¾åœ¨é‚„åŸå› 3.5ã€‚\n",
    "X_test_orig = scaler.inverse_transform(X_test_sc)\n",
    "\n",
    "# æœ€å¤šé¡¯ç¤º 10 ç­†æ¨£æœ¬ã€‚\n",
    "# å¦‚æœæ¸¬è©¦è³‡æ–™å°‘æ–¼ 10ï¼Œå°±é¡¯ç¤ºå…¨éƒ¨ã€‚\n",
    "show_n = min(10, len(X_test_orig))\n",
    "\n",
    "tbl = pd.DataFrame(X_test_orig[:show_n], columns=feature_names)\n",
    "tbl[\"True\"]  = [target_names[i] for i in y_test[:show_n]]\n",
    "tbl[\"Pred\"]  = [target_names[i] for i in y_pred[:show_n]]\n",
    "print(tbl)\n",
    "samples_csv = os.path.join(ARTIFACTS, \"test_samples.csv\")\n",
    "tbl.to_csv(samples_csv, index=False)\n",
    "print(f\"å·²å­˜æ¨£æœ¬å°ç…§(åŸå–®ä½): {samples_csv}\")\n",
    "\n",
    "print(\"STEP 7 å®Œæˆ âœ… ï¼ˆå…¨æµç¨‹ï¼‰\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyTorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
