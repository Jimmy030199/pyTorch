{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Step1 載入資料與探索\n",
      "\n",
      " 前5筆資料\n",
      "   sepal_length  sepal_width  petal_length  petal_width  target\n",
      "0           5.1          3.5           1.4          0.2       0\n",
      "1           4.9          3.0           1.4          0.2       0\n",
      "2           4.7          3.2           1.3          0.2       0\n",
      "3           4.6          3.1           1.5          0.2       0\n",
      "4           5.0          3.6           1.4          0.2       0\n",
      "\n",
      " 類別分布:\n",
      "0=setosa     : 50 筆\n",
      "1=versicolor : 50 筆\n",
      "2=virginica  : 50 筆\n",
      "\n",
      "[原始資料(未標準化)]\n",
      "sepal_length   mean=  5.8433 std=  0.8253\n",
      "sepal_width    mean=  3.0573 std=  0.4344\n",
      "petal_length   mean=  3.7580 std=  1.7594\n",
      "petal_width    mean=  1.1993 std=  0.7597\n",
      "\n",
      "->已存取20筆預覽 :iris_course\\artifacts\\iris_preview.csv\n",
      "STEP1 完成\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing  import List\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "ROOT= \"iris_course\"\n",
    "ARTIFACTS = os.path.join(ROOT,'artifacts')\n",
    "os.makedirs(ARTIFACTS,exist_ok=True)\n",
    "\n",
    "# axis=0 與 axis=1 差別\n",
    "# print(\"axis=0：每欄平均\", X.mean(axis=0))\n",
    "# print(\"axis=1：每列平均\", X.mean(axis=1))\n",
    "\n",
    "# 這段的作用\n",
    "# 這段是 把特徵名稱 (names)、平均值 (m)、標準差 (s) 串在一起逐一輸出。\n",
    "\n",
    "# 📌 詳細拆解\n",
    "# zip(names, m, s)\n",
    "# names 是特徵名稱的字串清單，例如：\n",
    "# [\"sepal_length\",\"sepal_width\",\"petal_length\",\"petal_width\"]\n",
    "\n",
    "# m 是各欄的平均值陣列，例如：\n",
    "# [5.84, 3.05, 3.76, 1.20]\n",
    "\n",
    "# s 是各欄的標準差陣列，例如：\n",
    "# [0.82, 0.43, 1.76, 0.76]\n",
    "\n",
    "# zip() 會把它們一一對應打包成 tuple：\n",
    "# [\n",
    "#   (\"sepal_length\", 5.84, 0.82),\n",
    "#   (\"sepal_width\",  3.05, 0.43),\n",
    "#   ...\n",
    "# ]\n",
    "\n",
    "# 格式\t     說明\n",
    "# {n:<14s}\t左對齊字串，寬度 14\n",
    "# {mi:8.4f}\t浮點數，總寬度 8，顯示 4 位小數\n",
    "# {sd:8.4f}\t同上\n",
    "\n",
    "def describe_stats(X:np.ndarray,names:List[str],title:str):\n",
    "    m,s = X.mean(axis=0), X.std(axis=0)\n",
    "    print(f\"\\n[{title}]\")\n",
    "    for n, mi, sd in zip(names, m,s):\n",
    "        print(f\"{n:<14s} mean={mi:8.4f} std={sd:8.4f}\" )\n",
    "\n",
    "print(\"***Step1 載入資料與探索\")\n",
    "iris =load_iris()\n",
    "# print(iris)\n",
    "\n",
    "\n",
    "# 載入資料集：\n",
    "# x 是 (150,4) 的數值矩陣\n",
    "# y 是 (150,) 的標籤（0,1,2）\n",
    "# 這四個特徵分別是：花萼長寬、花瓣長寬\n",
    "x,y = iris.data,iris.target\n",
    "\n",
    "# sepal 萼片\n",
    "# petal 花瓣\n",
    "feature_names = [\"sepal_length\",\"sepal_width\",\"petal_length\",\"petal_width\"]\n",
    "target_names=iris.target_names.tolist()\n",
    "\n",
    "# 這行會用 pandas 把 x 轉成一個 DataFrame（表格格式），欄名就是特徵名稱：\n",
    "df=pd.DataFrame(x,columns=feature_names)\n",
    "df[\"target\"] = y\n",
    "print(\"\\n 前5筆資料\");print(df.head())\n",
    "print(\"\\n 類別分布:\")\n",
    "\n",
    "\n",
    "# enumerate() 是 Python 內建函式\n",
    "# 它的作用是：在迴圈中同時取得「索引」和「元素」\n",
    "\n",
    "# 小總結\n",
    "#表達式\t                           意思\t結果型態\n",
    "# y\t類別標籤陣列\t           ndarray of int\n",
    "# y == i\t          元素逐一是否等於 i\tndarray of bool\n",
    "# (y == i).sum()\t      有幾個等於 i\tint（計數結果）\n",
    "\n",
    "# ✅ 所以雖然 y 是個整數陣列 (ndarray)，\n",
    "# 用 == 去比對數值時，會自動對每個元素做比較，這就是 NumPy 的「向量化運算」。\n",
    "\n",
    "# target_names：只會是 三個獨特的名稱 → ['setosa','versicolor','virginica']\n",
    "for i ,name in enumerate(target_names):\n",
    "    print(f\"{i}={name:<10s} : {(y==i).sum()} 筆\")\n",
    "describe_stats(x,feature_names,\"原始資料(未標準化)\")\n",
    "\n",
    "out_csv=os.path.join(ARTIFACTS,\"iris_preview.csv\")\n",
    "# index=False 表示 不要輸出 DataFrame 的索引欄位（只保留資料本身）\n",
    "df.head(20).to_csv(out_csv,index=False)\n",
    "\n",
    "print(f\"\\n->已存取20筆預覽 :{out_csv}\")\n",
    "print(\"STEP1 完成\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2949c15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===STEP2 | 切分 Train/Val/Test===\n",
      "切分形狀: train=(96, 4) val=(24, 4) test=(30, 4)\n",
      "STEP2 完成\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "print(\"\\n===STEP2 | 切分 Train/Val/Test===\")\n",
    "\n",
    "\n",
    "\n",
    "# x：所有的特徵資料（numpy.ndarray，shape = (150, 4)）\n",
    "# y：所有的標籤（numpy.ndarray，shape = (150,)）\n",
    "# train_test_split 是 scikit-learn 提供的\n",
    "# train_test_split 函式，用來隨機把資料拆成兩組\n",
    "# 參數解釋\n",
    "# 參數\t                            用途\n",
    "# x, y\t             輸入資料與標籤\n",
    "# test_size=0.2\t    指定 20% 當「測試集」，剩下 80% 是「訓練+驗證」\n",
    "# random_state=42\t設定隨機種子，讓結果可重現\n",
    "# stratify(分層)=y\t    分層抽樣：確保三個類別在兩組中比例一致\n",
    "\n",
    "X_trainval,X_test,y_trainval,y_test =train_test_split(\n",
    "    x,y,test_size=0.2,random_state=42,stratify=y\n",
    ")\n",
    "X_train,X_val,y_train,y_val =train_test_split(\n",
    "    X_trainval,y_trainval,test_size=0.2,random_state=42,stratify=y_trainval\n",
    ")\n",
    "\n",
    "print(f\"切分形狀: train={X_train.shape} val={X_val.shape} test={X_test.shape}\")\n",
    "print(\"STEP2 完成\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18de2a5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\====STEP 3 | 標準化(只用訓練集fit)並存檔)\n",
      "\n",
      "[訓練集(標準化前)]\n",
      "sepal_length   mean=  5.8333 std=  0.8516\n",
      "sepal_width    mean=  3.0083 std=  0.4264\n",
      "petal_length   mean=  3.7500 std=  1.7530\n",
      "petal_width    mean=  1.1875 std=  0.7463\n",
      "\n",
      "[訓練集(標準化後)]\n",
      "sepal_length   mean=  0.0000 std=  1.0000\n",
      "sepal_width    mean=  0.0000 std=  1.0000\n",
      "petal_length   mean=  0.0000 std=  1.0000\n",
      "petal_width    mean=  0.0000 std=  1.0000\n",
      "->已存標準化資料:iris_course\\artifacts\\train_val_test_scaled.npz\n",
      "->已存標準化器:iris_course\\artifacts\\scaler.pkl\n",
      "STEP3 完成\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import joblib\n",
    "\n",
    "print('\\====STEP 3 | 標準化(只用訓練集fit)並存檔)')\n",
    "\n",
    "# StandardScaler 是一個「轉換器 (transformer)」物件\n",
    "# 它會計算：\n",
    "# 每個欄位的平均值 μ\n",
    "# 每個欄位的標準差 σ\n",
    "# 然後把資料套用公式：\n",
    "\n",
    "# 𝑧 = (x - 𝜇) / 𝜎\n",
    "# 讓 每個特徵（欄）都變成平均 0、標準差 1\t​\n",
    "\n",
    "\n",
    "# 第 1 行：先用訓練資料「學習平均與標準差」\n",
    "# .fit(X_train) 會計算：\n",
    "# 每一欄的平均值 mean_\n",
    "# 每一欄的標準差 scale_\n",
    "# 這一步「只用訓練集」是為了避免資料洩漏（不能偷看驗證或測試資料）\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "\n",
    "# 第 2 行：把訓練資料做標準化\n",
    "# 用剛剛算出的 mean_ 和 scale_ 把資料轉換成：\n",
    "# (原值 - 平均) / 標準差\n",
    "# 結果：每一欄的平均會變成 0、標準差變成 1\n",
    "X_train_sc = scaler.transform(X_train)\n",
    "\n",
    "# 第 3～4 行：用同一個 scaler 處理驗證與測試資料\n",
    "# 這裡 不能再 fit 一次，要用 訓練集的平均與標準差 來轉換\n",
    "# 這樣才確保模型在驗證/測試時使用完全相同的尺度\n",
    "X_val_sc = scaler.transform(X_val)\n",
    "X_test_sc = scaler.transform(X_test)\n",
    "\n",
    "describe_stats(X_train, feature_names,\"訓練集(標準化前)\")\n",
    "describe_stats(X_train_sc, feature_names,\"訓練集(標準化後)\")\n",
    "\n",
    "npz_path= os.path.join(ARTIFACTS,\"train_val_test_scaled.npz\")\n",
    "# .npz 就是 把很多 NumPy 陣列一起打包壓縮存檔\n",
    "# → 讓你之後可以 一次存、一包讀，很方便。\n",
    "\n",
    "# 這行會建立一個 train_val_test_scaled.npz 檔，裡面包含：\n",
    "\n",
    "# 存進去的名稱\t內容\n",
    "# X_train_sc\t標準化後的訓練特徵資料\n",
    "# y_train\t訓練標籤\n",
    "# X_val_sc\t標準化後的驗證特徵資料\n",
    "# y_val\t驗證標籤\n",
    "# X_test_sc\t標準化後的測試特徵資料\n",
    "# y_test\t測試標籤\n",
    "# feature_names\t特徵名稱清單（轉成陣列存）\n",
    "# target_names\t類別名稱清單（轉成陣列存）\n",
    "\n",
    "np.savez(npz_path,\n",
    "         X_train_sc=X_train_sc,y_train=y_train,\n",
    "         X_val_sc=X_val_sc,y_val=y_val,\n",
    "         X_test_sc=X_test_sc,y_test=y_test,\n",
    "         feature_names=np.array(feature_names,dtype=object),\n",
    "         target_names=np.array(target_names,dtype=object),\n",
    ")\n",
    "\n",
    "# 這兩行的目的\n",
    "\n",
    "# 把你訓練好的 StandardScaler 物件\n",
    "# 存成一個檔案（scaler.pkl），\n",
    "# 以後要用時可以直接載回來，不用重新 .fit() 一次。\n",
    "\n",
    "# 把物件存起來\n",
    "# joblib.dump(scaler, scaler_path)\n",
    "# 使用 joblib 的 dump 函式\n",
    "# 把 scaler（也就是你用 X_train .fit() 過的 StandardScaler）存成 .pkl 檔案\n",
    "# .pkl 是「pickle」格式，用來存整個 Python 物件\n",
    "\n",
    "scaler_path =os.path.join(ARTIFACTS,\"scaler.pkl\")\n",
    "joblib.dump(scaler,scaler_path)\n",
    "\n",
    "print(f\"->已存標準化資料:{npz_path}\")\n",
    "print(f\"->已存標準化器:{scaler_path}\")\n",
    "print(\"STEP3 完成\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f154bf7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\====STEP 4 | Tensor 與 DataLoader\n",
      "第一個 batch:xb.shape=torch.Size([16, 4]), yb.shape=torch.Size([16])\n",
      "   xb[0](標準化後)=[1.017750859260559, -1.1921887397766113, 1.1694414615631104, 0.8207424879074097]\n",
      "   yb[0](類別)=2\n",
      "->已存batch 預覽iris_course\\artifacts\\batch_preview.csv\n",
      "STEP4 完成\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset,DataLoader\n",
    "# 整體目標\n",
    "# 這段是 STEP4：把資料轉成 torch.Tensor，再包進 DataLoader，用來訓練模型\n",
    "# 是 PyTorch 的標準資料處理流程。\n",
    "# 簡單來說：\n",
    "# 把資料(numpy.ndarray) → 轉成 Tensor → 用 DataLoader 分成小批次（batch） → 之後可以丟給模型訓練\n",
    "print('\\====STEP 4 | Tensor 與 DataLoader')\n",
    "\n",
    "\n",
    "X_train_t = torch.tensor(X_train_sc,dtype=torch.float32)\n",
    "y_train_t = torch.tensor(y_train,dtype=torch.long)\n",
    "X_val_t = torch.tensor(X_val_sc,dtype=torch.float32)\n",
    "y_val_t = torch.tensor(y_val,dtype=torch.long)\n",
    "\n",
    "\n",
    "# TensorDataset(X, y)：把 X 和 y 包成一筆一筆的資料\n",
    "# DataLoader(..., batch_size=16)：每次會吐出 16 筆資料（小批次）\n",
    "# shuffle=True：訓練集會隨機打亂順序（避免模型記住順序）\n",
    "# shuffle=False：驗證集保持原順序\n",
    "# 為什麼驗證/測試集不要 shuffle\n",
    "# 驗證時只是「評估」模型表現，不需要也不應打亂\n",
    "# 保持固定順序 → 方便對照預測與真實標籤\n",
    "# 每次評估結果一致，避免隨機性影響評估\n",
    "train_loader =DataLoader(TensorDataset(X_train_t,y_train_t),batch_size=16,shuffle=True)\n",
    "val_loader =DataLoader(TensorDataset(X_val_t,y_val_t),batch_size=16,shuffle=False)\n",
    "\n",
    "\n",
    "# 目的\n",
    "\n",
    "# 從 DataLoader（PyTorch）\n",
    "# 取出「第一個 batch（小批次）」的資料，\n",
    "# 用來 檢查資料長相是否正確。\n",
    "\n",
    "\n",
    "# train_loader 是你剛建立的 DataLoader，裡面有所有訓練資料（已分好 batch）\n",
    "# iter(train_loader) → 建立一個「迭代器」\n",
    "# next(...) → 從迭代器中取出第一組 (特徵, 標籤)\n",
    "# 結果會是：\n",
    "# xb = 一個 batch 的特徵資料\n",
    "# shape 通常是 (batch_size, 特徵數)\n",
    "# 例如 (16, 4)\n",
    "# yb = 一個 batch 的標籤資料\n",
    "# shape 是 (batch_size,)\n",
    "# 例如 (16,)\n",
    "\n",
    "# 重點觀念\n",
    "# shuffle=True 的 打亂時機不是在你建立 DataLoader 的當下，\n",
    "# 而是在你第一次呼叫 iter(train_loader)（開始一個 epoch）時才打亂資料順序。\n",
    "\n",
    "xb,yb = next(iter(train_loader))\n",
    "print(f\"第一個 batch:xb.shape={xb.shape}, yb.shape={yb.shape}\")\n",
    "\n",
    "# 取出 batch 裡第一筆資料的標籤\n",
    "print(f\"   xb[0](標準化後)={xb[0].tolist()}\")\n",
    "print(f\"   yb[0](類別)={yb[0].item()}\")\n",
    "\n",
    "\n",
    "# 功能（一句話）\n",
    "# 把你剛從 DataLoader 取出的那一個 batch（小批次）\n",
    "# 轉成表格（pandas.DataFrame），加上標籤欄位，\n",
    "# 再存成 CSV 檔，方便你用 Excel 或其他工具檢查。\n",
    "batch_preview=os.path.join(ARTIFACTS,\"batch_preview.csv\")\n",
    "pd.DataFrame(xb.numpy(),columns=feature_names).assign(label=yb.numpy()).to_csv(batch_preview,index=False)\n",
    "print(f\"->已存batch 預覽{batch_preview}\")\n",
    "print(\"STEP4 完成\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "70183213",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\====STEP 5 | 定義模型與參數量\n",
      "IrisMLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=4, out_features=64, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.2, inplace=False)\n",
      "    (3): Linear(in_features=64, out_features=32, bias=True)\n",
      "    (4): ReLU()\n",
      "    (5): Dropout(p=0.2, inplace=False)\n",
      "    (6): Linear(in_features=32, out_features=3, bias=True)\n",
      "  )\n",
      ")\n",
      "可訓練參數量:2,499\n",
      "-> 已存結構描述:iris_course\\models\\model_arch.txt\n",
      "STEP5 完成\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\pyTorch\\lib\\site-packages\\torch\\cuda\\__init__.py:218: UserWarning: \n",
      "NVIDIA GeForce RTX 5080 with CUDA capability sm_120 is not compatible with the current PyTorch installation.\n",
      "The current PyTorch install supports CUDA capabilities sm_50 sm_60 sm_61 sm_70 sm_75 sm_80 sm_86 sm_90.\n",
      "If you want to use the NVIDIA GeForce RTX 5080 GPU with PyTorch, please check the instructions at https://pytorch.org/get-started/locally/\n",
      "\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from torch import nn \n",
    "MODELS = os.path.join(ROOT,\"models\")\n",
    "os.makedirs(MODELS,exist_ok=True)\n",
    "\n",
    "print('\\====STEP 5 | 定義模型與參數量')\n",
    "\n",
    "class IrisMLP(nn.Module):\n",
    "    def __init__(self, in_dim=4, hidden1=64,hidden2=32,out_dim=3,dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim,hidden1),nn.ReLU(),nn.Dropout(dropout),\n",
    "            nn.Linear(hidden1,hidden2),nn.ReLU(),nn.Dropout(dropout),\n",
    "            nn.Linear(hidden2,out_dim)\n",
    "        )\n",
    "\n",
    "\n",
    "    #     forward：定義前向傳遞\n",
    "    # def forward(self, x):\n",
    "    #     return self.net(x)\n",
    "\n",
    "\n",
    "    # 告訴 PyTorch 資料要怎麼經過網路\n",
    "\n",
    "    # 只要呼叫 model(x)，就會自動跑這段\n",
    "    def forward(self,x): return self.net(x)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 計算 PyTorch 模型中「可訓練參數」的總數\n",
    "# 也就是：這個模型裡 所有需要更新的權重參數 一共有幾個數值（weights / biases）。\n",
    "\n",
    "# 你問的 -> int 是 Type Hint（型別註解） 的一種，\n",
    "# 不是程式功能的一部分，只是「告訴人或工具：這個函式會回傳什麼型別」。\n",
    "def count_trainable_params(model: nn.Module) -> int:\n",
    "\n",
    "    #     p.numel()\n",
    "    # 回傳這個 Tensor 裡「有幾個元素」\n",
    "    # 例如：\n",
    "    # p.shape = (64, 4) → p.numel() = 256\n",
    "    # p.shape = (64,)   → p.numel() = 64\n",
    "\n",
    "    #     (p.numel() for p in ...) 是 生成器，不是陣列，\n",
    "    # 它會「一個一個產生數字」，讓 sum() 去加總，\n",
    "    # 而不會先把所有數字存在記憶體裡。\n",
    "\n",
    "    # requires_grad 是什麼\n",
    "    # 它的意思是：\n",
    "    # 這個張量是否要在反向傳播（backpropagation）時計算梯度\n",
    "    # 有些參數可能：\n",
    "    # 是凍結的（不想訓練）\n",
    "    # 是固定的 embedding 或預訓練權重\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "\n",
    "# 檢查你的電腦有沒有 NVIDIA GPU（CUDA）可以用，\n",
    "# 然後設定一個 torch.device 物件，\n",
    "# 讓你之後可以把模型或資料移到 GPU 或 CPU 執行。\n",
    "device =torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 建立模型物件\n",
    "# 把模型搬到指定的裝置（CPU / GPU）上\n",
    "model = IrisMLP().to(device)\n",
    "print(model)\n",
    "print(f\"可訓練參數量:{count_trainable_params(model):,}\")\n",
    "\n",
    "arch_txt =os.path.join(MODELS,\"model_arch.txt\")\n",
    "\n",
    "# 功能總覽\n",
    "\n",
    "# 把 模型的結構 和 可訓練參數總數\n",
    "# 寫進一個文字檔（arch_txt）\n",
    "\n",
    "# 用 Python 內建的檔案操作\n",
    "# \"w\" → 以「寫入模式」開啟檔案\n",
    "# encoding=\"utf-8\" → 用 UTF-8 編碼（可以正常寫中文）\n",
    "# as f → 建立檔案物件 f\n",
    "# with 會在寫完後自動關閉檔案\n",
    "\n",
    "with open(arch_txt,\"w\",encoding=\"utf-8\") as f:\n",
    "    f.write(str(model) + \"\\n\")\n",
    "    f.write(f\"trainable_params={count_trainable_params(model)}\\n\")\n",
    "print(f\"-> 已存結構描述:{arch_txt}\")\n",
    "print(\"STEP5 完成\") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyTorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
