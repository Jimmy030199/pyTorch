{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Step1 è¼‰å…¥è³‡æ–™èˆ‡æ¢ç´¢\n",
      "\n",
      " å‰5ç­†è³‡æ–™\n",
      "   sepal_length  sepal_width  petal_length  petal_width  target\n",
      "0           5.1          3.5           1.4          0.2       0\n",
      "1           4.9          3.0           1.4          0.2       0\n",
      "2           4.7          3.2           1.3          0.2       0\n",
      "3           4.6          3.1           1.5          0.2       0\n",
      "4           5.0          3.6           1.4          0.2       0\n",
      "\n",
      " é¡åˆ¥åˆ†å¸ƒ:\n",
      "0=setosa     : 50 ç­†\n",
      "1=versicolor : 50 ç­†\n",
      "2=virginica  : 50 ç­†\n",
      "\n",
      "[åŸå§‹è³‡æ–™(æœªæ¨™æº–åŒ–)]\n",
      "sepal_length   mean=  5.8433 std=  0.8253\n",
      "sepal_width    mean=  3.0573 std=  0.4344\n",
      "petal_length   mean=  3.7580 std=  1.7594\n",
      "petal_width    mean=  1.1993 std=  0.7597\n",
      "\n",
      "->å·²å­˜å–20ç­†é è¦½ :iris_course\\artifacts\\iris_preview.csv\n",
      "STEP1 å®Œæˆ\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing  import List\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "ROOT= \"iris_course\"\n",
    "ARTIFACTS = os.path.join(ROOT,'artifacts')\n",
    "os.makedirs(ARTIFACTS,exist_ok=True)\n",
    "\n",
    "# axis=0 èˆ‡ axis=1 å·®åˆ¥\n",
    "# print(\"axis=0ï¼šæ¯æ¬„å¹³å‡\", X.mean(axis=0))\n",
    "# print(\"axis=1ï¼šæ¯åˆ—å¹³å‡\", X.mean(axis=1))\n",
    "\n",
    "# é€™æ®µçš„ä½œç”¨\n",
    "# é€™æ®µæ˜¯ æŠŠç‰¹å¾µåç¨± (names)ã€å¹³å‡å€¼ (m)ã€æ¨™æº–å·® (s) ä¸²åœ¨ä¸€èµ·é€ä¸€è¼¸å‡ºã€‚\n",
    "\n",
    "# ğŸ“Œ è©³ç´°æ‹†è§£\n",
    "# zip(names, m, s)\n",
    "# names æ˜¯ç‰¹å¾µåç¨±çš„å­—ä¸²æ¸…å–®ï¼Œä¾‹å¦‚ï¼š\n",
    "# [\"sepal_length\",\"sepal_width\",\"petal_length\",\"petal_width\"]\n",
    "\n",
    "# m æ˜¯å„æ¬„çš„å¹³å‡å€¼é™£åˆ—ï¼Œä¾‹å¦‚ï¼š\n",
    "# [5.84, 3.05, 3.76, 1.20]\n",
    "\n",
    "# s æ˜¯å„æ¬„çš„æ¨™æº–å·®é™£åˆ—ï¼Œä¾‹å¦‚ï¼š\n",
    "# [0.82, 0.43, 1.76, 0.76]\n",
    "\n",
    "# zip() æœƒæŠŠå®ƒå€‘ä¸€ä¸€å°æ‡‰æ‰“åŒ…æˆ tupleï¼š\n",
    "# [\n",
    "#   (\"sepal_length\", 5.84, 0.82),\n",
    "#   (\"sepal_width\",  3.05, 0.43),\n",
    "#   ...\n",
    "# ]\n",
    "\n",
    "# æ ¼å¼\t     èªªæ˜\n",
    "# {n:<14s}\tå·¦å°é½Šå­—ä¸²ï¼Œå¯¬åº¦ 14\n",
    "# {mi:8.4f}\tæµ®é»æ•¸ï¼Œç¸½å¯¬åº¦ 8ï¼Œé¡¯ç¤º 4 ä½å°æ•¸\n",
    "# {sd:8.4f}\tåŒä¸Š\n",
    "\n",
    "def describe_stats(X:np.ndarray,names:List[str],title:str):\n",
    "    m,s = X.mean(axis=0), X.std(axis=0)\n",
    "    print(f\"\\n[{title}]\")\n",
    "    for n, mi, sd in zip(names, m,s):\n",
    "        print(f\"{n:<14s} mean={mi:8.4f} std={sd:8.4f}\" )\n",
    "\n",
    "print(\"***Step1 è¼‰å…¥è³‡æ–™èˆ‡æ¢ç´¢\")\n",
    "iris =load_iris()\n",
    "# print(iris)\n",
    "\n",
    "\n",
    "# è¼‰å…¥è³‡æ–™é›†ï¼š\n",
    "# x æ˜¯ (150,4) çš„æ•¸å€¼çŸ©é™£\n",
    "# y æ˜¯ (150,) çš„æ¨™ç±¤ï¼ˆ0,1,2ï¼‰\n",
    "# é€™å››å€‹ç‰¹å¾µåˆ†åˆ¥æ˜¯ï¼šèŠ±è¼é•·å¯¬ã€èŠ±ç“£é•·å¯¬\n",
    "x,y = iris.data,iris.target\n",
    "\n",
    "# sepal è¼ç‰‡\n",
    "# petal èŠ±ç“£\n",
    "feature_names = [\"sepal_length\",\"sepal_width\",\"petal_length\",\"petal_width\"]\n",
    "target_names=iris.target_names.tolist()\n",
    "\n",
    "# é€™è¡Œæœƒç”¨ pandas æŠŠ x è½‰æˆä¸€å€‹ DataFrameï¼ˆè¡¨æ ¼æ ¼å¼ï¼‰ï¼Œæ¬„åå°±æ˜¯ç‰¹å¾µåç¨±ï¼š\n",
    "df=pd.DataFrame(x,columns=feature_names)\n",
    "df[\"target\"] = y\n",
    "print(\"\\n å‰5ç­†è³‡æ–™\");print(df.head())\n",
    "print(\"\\n é¡åˆ¥åˆ†å¸ƒ:\")\n",
    "\n",
    "\n",
    "# enumerate() æ˜¯ Python å…§å»ºå‡½å¼\n",
    "# å®ƒçš„ä½œç”¨æ˜¯ï¼šåœ¨è¿´åœˆä¸­åŒæ™‚å–å¾—ã€Œç´¢å¼•ã€å’Œã€Œå…ƒç´ ã€\n",
    "\n",
    "# å°ç¸½çµ\n",
    "#è¡¨é”å¼\t                           æ„æ€\tçµæœå‹æ…‹\n",
    "# y\té¡åˆ¥æ¨™ç±¤é™£åˆ—\t           ndarray of int\n",
    "# y == i\t          å…ƒç´ é€ä¸€æ˜¯å¦ç­‰æ–¼ i\tndarray of bool\n",
    "# (y == i).sum()\t      æœ‰å¹¾å€‹ç­‰æ–¼ i\tintï¼ˆè¨ˆæ•¸çµæœï¼‰\n",
    "\n",
    "# âœ… æ‰€ä»¥é›–ç„¶ y æ˜¯å€‹æ•´æ•¸é™£åˆ— (ndarray)ï¼Œ\n",
    "# ç”¨ == å»æ¯”å°æ•¸å€¼æ™‚ï¼Œæœƒè‡ªå‹•å°æ¯å€‹å…ƒç´ åšæ¯”è¼ƒï¼Œé€™å°±æ˜¯ NumPy çš„ã€Œå‘é‡åŒ–é‹ç®—ã€ã€‚\n",
    "\n",
    "# target_namesï¼šåªæœƒæ˜¯ ä¸‰å€‹ç¨ç‰¹çš„åç¨± â†’ ['setosa','versicolor','virginica']\n",
    "for i ,name in enumerate(target_names):\n",
    "    print(f\"{i}={name:<10s} : {(y==i).sum()} ç­†\")\n",
    "describe_stats(x,feature_names,\"åŸå§‹è³‡æ–™(æœªæ¨™æº–åŒ–)\")\n",
    "\n",
    "out_csv=os.path.join(ARTIFACTS,\"iris_preview.csv\")\n",
    "# index=False è¡¨ç¤º ä¸è¦è¼¸å‡º DataFrame çš„ç´¢å¼•æ¬„ä½ï¼ˆåªä¿ç•™è³‡æ–™æœ¬èº«ï¼‰\n",
    "df.head(20).to_csv(out_csv,index=False)\n",
    "\n",
    "print(f\"\\n->å·²å­˜å–20ç­†é è¦½ :{out_csv}\")\n",
    "print(\"STEP1 å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2949c15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===STEP2 | åˆ‡åˆ† Train/Val/Test===\n",
      "åˆ‡åˆ†å½¢ç‹€: train=(96, 4) val=(24, 4) test=(30, 4)\n",
      "STEP2 å®Œæˆ\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "print(\"\\n===STEP2 | åˆ‡åˆ† Train/Val/Test===\")\n",
    "\n",
    "\n",
    "\n",
    "# xï¼šæ‰€æœ‰çš„ç‰¹å¾µè³‡æ–™ï¼ˆnumpy.ndarrayï¼Œshape = (150, 4)ï¼‰\n",
    "# yï¼šæ‰€æœ‰çš„æ¨™ç±¤ï¼ˆnumpy.ndarrayï¼Œshape = (150,)ï¼‰\n",
    "# train_test_split æ˜¯ scikit-learn æä¾›çš„\n",
    "# train_test_split å‡½å¼ï¼Œç”¨ä¾†éš¨æ©ŸæŠŠè³‡æ–™æ‹†æˆå…©çµ„\n",
    "# åƒæ•¸è§£é‡‹\n",
    "# åƒæ•¸\t                            ç”¨é€”\n",
    "# x, y\t             è¼¸å…¥è³‡æ–™èˆ‡æ¨™ç±¤\n",
    "# test_size=0.2\t    æŒ‡å®š 20% ç•¶ã€Œæ¸¬è©¦é›†ã€ï¼Œå‰©ä¸‹ 80% æ˜¯ã€Œè¨“ç·´+é©—è­‰ã€\n",
    "# random_state=42\tè¨­å®šéš¨æ©Ÿç¨®å­ï¼Œè®“çµæœå¯é‡ç¾\n",
    "# stratify(åˆ†å±¤)=y\t    åˆ†å±¤æŠ½æ¨£ï¼šç¢ºä¿ä¸‰å€‹é¡åˆ¥åœ¨å…©çµ„ä¸­æ¯”ä¾‹ä¸€è‡´\n",
    "\n",
    "X_trainval,X_test,y_trainval,y_test =train_test_split(\n",
    "    x,y,test_size=0.2,random_state=42,stratify=y\n",
    ")\n",
    "X_train,X_val,y_train,y_val =train_test_split(\n",
    "    X_trainval,y_trainval,test_size=0.2,random_state=42,stratify=y_trainval\n",
    ")\n",
    "\n",
    "print(f\"åˆ‡åˆ†å½¢ç‹€: train={X_train.shape} val={X_val.shape} test={X_test.shape}\")\n",
    "print(\"STEP2 å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18de2a5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\====STEP 3 | æ¨™æº–åŒ–(åªç”¨è¨“ç·´é›†fit)ä¸¦å­˜æª”)\n",
      "\n",
      "[è¨“ç·´é›†(æ¨™æº–åŒ–å‰)]\n",
      "sepal_length   mean=  5.8333 std=  0.8516\n",
      "sepal_width    mean=  3.0083 std=  0.4264\n",
      "petal_length   mean=  3.7500 std=  1.7530\n",
      "petal_width    mean=  1.1875 std=  0.7463\n",
      "\n",
      "[è¨“ç·´é›†(æ¨™æº–åŒ–å¾Œ)]\n",
      "sepal_length   mean=  0.0000 std=  1.0000\n",
      "sepal_width    mean=  0.0000 std=  1.0000\n",
      "petal_length   mean=  0.0000 std=  1.0000\n",
      "petal_width    mean=  0.0000 std=  1.0000\n",
      "->å·²å­˜æ¨™æº–åŒ–è³‡æ–™:iris_course\\artifacts\\train_val_test_scaled.npz\n",
      "->å·²å­˜æ¨™æº–åŒ–å™¨:iris_course\\artifacts\\scaler.pkl\n",
      "STEP3 å®Œæˆ\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import joblib\n",
    "\n",
    "print('\\====STEP 3 | æ¨™æº–åŒ–(åªç”¨è¨“ç·´é›†fit)ä¸¦å­˜æª”)')\n",
    "\n",
    "# StandardScaler æ˜¯ä¸€å€‹ã€Œè½‰æ›å™¨ (transformer)ã€ç‰©ä»¶\n",
    "# å®ƒæœƒè¨ˆç®—ï¼š\n",
    "# æ¯å€‹æ¬„ä½çš„å¹³å‡å€¼ Î¼\n",
    "# æ¯å€‹æ¬„ä½çš„æ¨™æº–å·® Ïƒ\n",
    "# ç„¶å¾ŒæŠŠè³‡æ–™å¥—ç”¨å…¬å¼ï¼š\n",
    "\n",
    "# ğ‘§ = (x - ğœ‡) / ğœ\n",
    "# è®“ æ¯å€‹ç‰¹å¾µï¼ˆæ¬„ï¼‰éƒ½è®Šæˆå¹³å‡ 0ã€æ¨™æº–å·® 1\tâ€‹\n",
    "\n",
    "\n",
    "# ç¬¬ 1 è¡Œï¼šå…ˆç”¨è¨“ç·´è³‡æ–™ã€Œå­¸ç¿’å¹³å‡èˆ‡æ¨™æº–å·®ã€\n",
    "# .fit(X_train) æœƒè¨ˆç®—ï¼š\n",
    "# æ¯ä¸€æ¬„çš„å¹³å‡å€¼ mean_\n",
    "# æ¯ä¸€æ¬„çš„æ¨™æº–å·® scale_\n",
    "# é€™ä¸€æ­¥ã€Œåªç”¨è¨“ç·´é›†ã€æ˜¯ç‚ºäº†é¿å…è³‡æ–™æ´©æ¼ï¼ˆä¸èƒ½å·çœ‹é©—è­‰æˆ–æ¸¬è©¦è³‡æ–™ï¼‰\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "\n",
    "# ç¬¬ 2 è¡Œï¼šæŠŠè¨“ç·´è³‡æ–™åšæ¨™æº–åŒ–\n",
    "# ç”¨å‰›å‰›ç®—å‡ºçš„ mean_ å’Œ scale_ æŠŠè³‡æ–™è½‰æ›æˆï¼š\n",
    "# (åŸå€¼ - å¹³å‡) / æ¨™æº–å·®\n",
    "# çµæœï¼šæ¯ä¸€æ¬„çš„å¹³å‡æœƒè®Šæˆ 0ã€æ¨™æº–å·®è®Šæˆ 1\n",
    "X_train_sc = scaler.transform(X_train)\n",
    "\n",
    "# ç¬¬ 3ï½4 è¡Œï¼šç”¨åŒä¸€å€‹ scaler è™•ç†é©—è­‰èˆ‡æ¸¬è©¦è³‡æ–™\n",
    "# é€™è£¡ ä¸èƒ½å† fit ä¸€æ¬¡ï¼Œè¦ç”¨ è¨“ç·´é›†çš„å¹³å‡èˆ‡æ¨™æº–å·® ä¾†è½‰æ›\n",
    "# é€™æ¨£æ‰ç¢ºä¿æ¨¡å‹åœ¨é©—è­‰/æ¸¬è©¦æ™‚ä½¿ç”¨å®Œå…¨ç›¸åŒçš„å°ºåº¦\n",
    "X_val_sc = scaler.transform(X_val)\n",
    "X_test_sc = scaler.transform(X_test)\n",
    "\n",
    "describe_stats(X_train, feature_names,\"è¨“ç·´é›†(æ¨™æº–åŒ–å‰)\")\n",
    "describe_stats(X_train_sc, feature_names,\"è¨“ç·´é›†(æ¨™æº–åŒ–å¾Œ)\")\n",
    "\n",
    "npz_path= os.path.join(ARTIFACTS,\"train_val_test_scaled.npz\")\n",
    "# .npz å°±æ˜¯ æŠŠå¾ˆå¤š NumPy é™£åˆ—ä¸€èµ·æ‰“åŒ…å£“ç¸®å­˜æª”\n",
    "# â†’ è®“ä½ ä¹‹å¾Œå¯ä»¥ ä¸€æ¬¡å­˜ã€ä¸€åŒ…è®€ï¼Œå¾ˆæ–¹ä¾¿ã€‚\n",
    "\n",
    "# é€™è¡Œæœƒå»ºç«‹ä¸€å€‹ train_val_test_scaled.npz æª”ï¼Œè£¡é¢åŒ…å«ï¼š\n",
    "\n",
    "# å­˜é€²å»çš„åç¨±\tå…§å®¹\n",
    "# X_train_sc\tæ¨™æº–åŒ–å¾Œçš„è¨“ç·´ç‰¹å¾µè³‡æ–™\n",
    "# y_train\tè¨“ç·´æ¨™ç±¤\n",
    "# X_val_sc\tæ¨™æº–åŒ–å¾Œçš„é©—è­‰ç‰¹å¾µè³‡æ–™\n",
    "# y_val\té©—è­‰æ¨™ç±¤\n",
    "# X_test_sc\tæ¨™æº–åŒ–å¾Œçš„æ¸¬è©¦ç‰¹å¾µè³‡æ–™\n",
    "# y_test\tæ¸¬è©¦æ¨™ç±¤\n",
    "# feature_names\tç‰¹å¾µåç¨±æ¸…å–®ï¼ˆè½‰æˆé™£åˆ—å­˜ï¼‰\n",
    "# target_names\té¡åˆ¥åç¨±æ¸…å–®ï¼ˆè½‰æˆé™£åˆ—å­˜ï¼‰\n",
    "\n",
    "np.savez(npz_path,\n",
    "         X_train_sc=X_train_sc,y_train=y_train,\n",
    "         X_val_sc=X_val_sc,y_val=y_val,\n",
    "         X_test_sc=X_test_sc,y_test=y_test,\n",
    "         feature_names=np.array(feature_names,dtype=object),\n",
    "         target_names=np.array(target_names,dtype=object),\n",
    ")\n",
    "\n",
    "# é€™å…©è¡Œçš„ç›®çš„\n",
    "\n",
    "# æŠŠä½ è¨“ç·´å¥½çš„ StandardScaler ç‰©ä»¶\n",
    "# å­˜æˆä¸€å€‹æª”æ¡ˆï¼ˆscaler.pklï¼‰ï¼Œ\n",
    "# ä»¥å¾Œè¦ç”¨æ™‚å¯ä»¥ç›´æ¥è¼‰å›ä¾†ï¼Œä¸ç”¨é‡æ–° .fit() ä¸€æ¬¡ã€‚\n",
    "\n",
    "# æŠŠç‰©ä»¶å­˜èµ·ä¾†\n",
    "# joblib.dump(scaler, scaler_path)\n",
    "# ä½¿ç”¨ joblib çš„ dump å‡½å¼\n",
    "# æŠŠ scalerï¼ˆä¹Ÿå°±æ˜¯ä½ ç”¨ X_train .fit() éçš„ StandardScalerï¼‰å­˜æˆ .pkl æª”æ¡ˆ\n",
    "# .pkl æ˜¯ã€Œpickleã€æ ¼å¼ï¼Œç”¨ä¾†å­˜æ•´å€‹ Python ç‰©ä»¶\n",
    "\n",
    "scaler_path =os.path.join(ARTIFACTS,\"scaler.pkl\")\n",
    "joblib.dump(scaler,scaler_path)\n",
    "\n",
    "print(f\"->å·²å­˜æ¨™æº–åŒ–è³‡æ–™:{npz_path}\")\n",
    "print(f\"->å·²å­˜æ¨™æº–åŒ–å™¨:{scaler_path}\")\n",
    "print(\"STEP3 å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f154bf7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\====STEP 4 | Tensor èˆ‡ DataLoader\n",
      "ç¬¬ä¸€å€‹ batch:xb.shape=torch.Size([16, 4]), yb.shape=torch.Size([16])\n",
      "   xb[0](æ¨™æº–åŒ–å¾Œ)=[1.017750859260559, -1.1921887397766113, 1.1694414615631104, 0.8207424879074097]\n",
      "   yb[0](é¡åˆ¥)=2\n",
      "->å·²å­˜batch é è¦½iris_course\\artifacts\\batch_preview.csv\n",
      "STEP4 å®Œæˆ\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset,DataLoader\n",
    "# æ•´é«”ç›®æ¨™\n",
    "# é€™æ®µæ˜¯ STEP4ï¼šæŠŠè³‡æ–™è½‰æˆ torch.Tensorï¼Œå†åŒ…é€² DataLoaderï¼Œç”¨ä¾†è¨“ç·´æ¨¡å‹\n",
    "# æ˜¯ PyTorch çš„æ¨™æº–è³‡æ–™è™•ç†æµç¨‹ã€‚\n",
    "# ç°¡å–®ä¾†èªªï¼š\n",
    "# æŠŠè³‡æ–™(numpy.ndarray) â†’ è½‰æˆ Tensor â†’ ç”¨ DataLoader åˆ†æˆå°æ‰¹æ¬¡ï¼ˆbatchï¼‰ â†’ ä¹‹å¾Œå¯ä»¥ä¸Ÿçµ¦æ¨¡å‹è¨“ç·´\n",
    "print('\\====STEP 4 | Tensor èˆ‡ DataLoader')\n",
    "\n",
    "\n",
    "X_train_t = torch.tensor(X_train_sc,dtype=torch.float32)\n",
    "y_train_t = torch.tensor(y_train,dtype=torch.long)\n",
    "X_val_t = torch.tensor(X_val_sc,dtype=torch.float32)\n",
    "y_val_t = torch.tensor(y_val,dtype=torch.long)\n",
    "\n",
    "\n",
    "# TensorDataset(X, y)ï¼šæŠŠ X å’Œ y åŒ…æˆä¸€ç­†ä¸€ç­†çš„è³‡æ–™\n",
    "# DataLoader(..., batch_size=16)ï¼šæ¯æ¬¡æœƒåå‡º 16 ç­†è³‡æ–™ï¼ˆå°æ‰¹æ¬¡ï¼‰\n",
    "# shuffle=Trueï¼šè¨“ç·´é›†æœƒéš¨æ©Ÿæ‰“äº‚é †åºï¼ˆé¿å…æ¨¡å‹è¨˜ä½é †åºï¼‰\n",
    "# shuffle=Falseï¼šé©—è­‰é›†ä¿æŒåŸé †åº\n",
    "# ç‚ºä»€éº¼é©—è­‰/æ¸¬è©¦é›†ä¸è¦ shuffle\n",
    "# é©—è­‰æ™‚åªæ˜¯ã€Œè©•ä¼°ã€æ¨¡å‹è¡¨ç¾ï¼Œä¸éœ€è¦ä¹Ÿä¸æ‡‰æ‰“äº‚\n",
    "# ä¿æŒå›ºå®šé †åº â†’ æ–¹ä¾¿å°ç…§é æ¸¬èˆ‡çœŸå¯¦æ¨™ç±¤\n",
    "# æ¯æ¬¡è©•ä¼°çµæœä¸€è‡´ï¼Œé¿å…éš¨æ©Ÿæ€§å½±éŸ¿è©•ä¼°\n",
    "train_loader =DataLoader(TensorDataset(X_train_t,y_train_t),batch_size=16,shuffle=True)\n",
    "val_loader =DataLoader(TensorDataset(X_val_t,y_val_t),batch_size=16,shuffle=False)\n",
    "\n",
    "\n",
    "# ç›®çš„\n",
    "\n",
    "# å¾ DataLoaderï¼ˆPyTorchï¼‰\n",
    "# å–å‡ºã€Œç¬¬ä¸€å€‹ batchï¼ˆå°æ‰¹æ¬¡ï¼‰ã€çš„è³‡æ–™ï¼Œ\n",
    "# ç”¨ä¾† æª¢æŸ¥è³‡æ–™é•·ç›¸æ˜¯å¦æ­£ç¢ºã€‚\n",
    "\n",
    "\n",
    "# train_loader æ˜¯ä½ å‰›å»ºç«‹çš„ DataLoaderï¼Œè£¡é¢æœ‰æ‰€æœ‰è¨“ç·´è³‡æ–™ï¼ˆå·²åˆ†å¥½ batchï¼‰\n",
    "# iter(train_loader) â†’ å»ºç«‹ä¸€å€‹ã€Œè¿­ä»£å™¨ã€\n",
    "# next(...) â†’ å¾è¿­ä»£å™¨ä¸­å–å‡ºç¬¬ä¸€çµ„ (ç‰¹å¾µ, æ¨™ç±¤)\n",
    "# çµæœæœƒæ˜¯ï¼š\n",
    "# xb = ä¸€å€‹ batch çš„ç‰¹å¾µè³‡æ–™\n",
    "# shape é€šå¸¸æ˜¯ (batch_size, ç‰¹å¾µæ•¸)\n",
    "# ä¾‹å¦‚ (16, 4)\n",
    "# yb = ä¸€å€‹ batch çš„æ¨™ç±¤è³‡æ–™\n",
    "# shape æ˜¯ (batch_size,)\n",
    "# ä¾‹å¦‚ (16,)\n",
    "\n",
    "# é‡é»è§€å¿µ\n",
    "# shuffle=True çš„ æ‰“äº‚æ™‚æ©Ÿä¸æ˜¯åœ¨ä½ å»ºç«‹ DataLoader çš„ç•¶ä¸‹ï¼Œ\n",
    "# è€Œæ˜¯åœ¨ä½ ç¬¬ä¸€æ¬¡å‘¼å« iter(train_loader)ï¼ˆé–‹å§‹ä¸€å€‹ epochï¼‰æ™‚æ‰æ‰“äº‚è³‡æ–™é †åºã€‚\n",
    "\n",
    "xb,yb = next(iter(train_loader))\n",
    "print(f\"ç¬¬ä¸€å€‹ batch:xb.shape={xb.shape}, yb.shape={yb.shape}\")\n",
    "\n",
    "# å–å‡º batch è£¡ç¬¬ä¸€ç­†è³‡æ–™çš„æ¨™ç±¤\n",
    "print(f\"   xb[0](æ¨™æº–åŒ–å¾Œ)={xb[0].tolist()}\")\n",
    "print(f\"   yb[0](é¡åˆ¥)={yb[0].item()}\")\n",
    "\n",
    "\n",
    "# åŠŸèƒ½ï¼ˆä¸€å¥è©±ï¼‰\n",
    "# æŠŠä½ å‰›å¾ DataLoader å–å‡ºçš„é‚£ä¸€å€‹ batchï¼ˆå°æ‰¹æ¬¡ï¼‰\n",
    "# è½‰æˆè¡¨æ ¼ï¼ˆpandas.DataFrameï¼‰ï¼ŒåŠ ä¸Šæ¨™ç±¤æ¬„ä½ï¼Œ\n",
    "# å†å­˜æˆ CSV æª”ï¼Œæ–¹ä¾¿ä½ ç”¨ Excel æˆ–å…¶ä»–å·¥å…·æª¢æŸ¥ã€‚\n",
    "batch_preview=os.path.join(ARTIFACTS,\"batch_preview.csv\")\n",
    "pd.DataFrame(xb.numpy(),columns=feature_names).assign(label=yb.numpy()).to_csv(batch_preview,index=False)\n",
    "print(f\"->å·²å­˜batch é è¦½{batch_preview}\")\n",
    "print(\"STEP4 å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "70183213",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\====STEP 5 | å®šç¾©æ¨¡å‹èˆ‡åƒæ•¸é‡\n",
      "IrisMLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=4, out_features=64, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.2, inplace=False)\n",
      "    (3): Linear(in_features=64, out_features=32, bias=True)\n",
      "    (4): ReLU()\n",
      "    (5): Dropout(p=0.2, inplace=False)\n",
      "    (6): Linear(in_features=32, out_features=3, bias=True)\n",
      "  )\n",
      ")\n",
      "å¯è¨“ç·´åƒæ•¸é‡:2,499\n",
      "-> å·²å­˜çµæ§‹æè¿°:iris_course\\models\\model_arch.txt\n",
      "STEP5 å®Œæˆ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\pyTorch\\lib\\site-packages\\torch\\cuda\\__init__.py:218: UserWarning: \n",
      "NVIDIA GeForce RTX 5080 with CUDA capability sm_120 is not compatible with the current PyTorch installation.\n",
      "The current PyTorch install supports CUDA capabilities sm_50 sm_60 sm_61 sm_70 sm_75 sm_80 sm_86 sm_90.\n",
      "If you want to use the NVIDIA GeForce RTX 5080 GPU with PyTorch, please check the instructions at https://pytorch.org/get-started/locally/\n",
      "\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from torch import nn \n",
    "MODELS = os.path.join(ROOT,\"models\")\n",
    "os.makedirs(MODELS,exist_ok=True)\n",
    "\n",
    "print('\\====STEP 5 | å®šç¾©æ¨¡å‹èˆ‡åƒæ•¸é‡')\n",
    "\n",
    "class IrisMLP(nn.Module):\n",
    "    def __init__(self, in_dim=4, hidden1=64,hidden2=32,out_dim=3,dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim,hidden1),nn.ReLU(),nn.Dropout(dropout),\n",
    "            nn.Linear(hidden1,hidden2),nn.ReLU(),nn.Dropout(dropout),\n",
    "            nn.Linear(hidden2,out_dim)\n",
    "        )\n",
    "\n",
    "\n",
    "    #     forwardï¼šå®šç¾©å‰å‘å‚³é\n",
    "    # def forward(self, x):\n",
    "    #     return self.net(x)\n",
    "\n",
    "\n",
    "    # å‘Šè¨´ PyTorch è³‡æ–™è¦æ€éº¼ç¶“éç¶²è·¯\n",
    "\n",
    "    # åªè¦å‘¼å« model(x)ï¼Œå°±æœƒè‡ªå‹•è·‘é€™æ®µ\n",
    "    def forward(self,x): return self.net(x)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# è¨ˆç®— PyTorch æ¨¡å‹ä¸­ã€Œå¯è¨“ç·´åƒæ•¸ã€çš„ç¸½æ•¸\n",
    "# ä¹Ÿå°±æ˜¯ï¼šé€™å€‹æ¨¡å‹è£¡ æ‰€æœ‰éœ€è¦æ›´æ–°çš„æ¬Šé‡åƒæ•¸ ä¸€å…±æœ‰å¹¾å€‹æ•¸å€¼ï¼ˆweights / biasesï¼‰ã€‚\n",
    "\n",
    "# ä½ å•çš„ -> int æ˜¯ Type Hintï¼ˆå‹åˆ¥è¨»è§£ï¼‰ çš„ä¸€ç¨®ï¼Œ\n",
    "# ä¸æ˜¯ç¨‹å¼åŠŸèƒ½çš„ä¸€éƒ¨åˆ†ï¼Œåªæ˜¯ã€Œå‘Šè¨´äººæˆ–å·¥å…·ï¼šé€™å€‹å‡½å¼æœƒå›å‚³ä»€éº¼å‹åˆ¥ã€ã€‚\n",
    "def count_trainable_params(model: nn.Module) -> int:\n",
    "\n",
    "    #     p.numel()\n",
    "    # å›å‚³é€™å€‹ Tensor è£¡ã€Œæœ‰å¹¾å€‹å…ƒç´ ã€\n",
    "    # ä¾‹å¦‚ï¼š\n",
    "    # p.shape = (64, 4) â†’ p.numel() = 256\n",
    "    # p.shape = (64,)   â†’ p.numel() = 64\n",
    "\n",
    "    #     (p.numel() for p in ...) æ˜¯ ç”Ÿæˆå™¨ï¼Œä¸æ˜¯é™£åˆ—ï¼Œ\n",
    "    # å®ƒæœƒã€Œä¸€å€‹ä¸€å€‹ç”¢ç”Ÿæ•¸å­—ã€ï¼Œè®“ sum() å»åŠ ç¸½ï¼Œ\n",
    "    # è€Œä¸æœƒå…ˆæŠŠæ‰€æœ‰æ•¸å­—å­˜åœ¨è¨˜æ†¶é«”è£¡ã€‚\n",
    "\n",
    "    # requires_grad æ˜¯ä»€éº¼\n",
    "    # å®ƒçš„æ„æ€æ˜¯ï¼š\n",
    "    # é€™å€‹å¼µé‡æ˜¯å¦è¦åœ¨åå‘å‚³æ’­ï¼ˆbackpropagationï¼‰æ™‚è¨ˆç®—æ¢¯åº¦\n",
    "    # æœ‰äº›åƒæ•¸å¯èƒ½ï¼š\n",
    "    # æ˜¯å‡çµçš„ï¼ˆä¸æƒ³è¨“ç·´ï¼‰\n",
    "    # æ˜¯å›ºå®šçš„ embedding æˆ–é è¨“ç·´æ¬Šé‡\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "\n",
    "# æª¢æŸ¥ä½ çš„é›»è…¦æœ‰æ²’æœ‰ NVIDIA GPUï¼ˆCUDAï¼‰å¯ä»¥ç”¨ï¼Œ\n",
    "# ç„¶å¾Œè¨­å®šä¸€å€‹ torch.device ç‰©ä»¶ï¼Œ\n",
    "# è®“ä½ ä¹‹å¾Œå¯ä»¥æŠŠæ¨¡å‹æˆ–è³‡æ–™ç§»åˆ° GPU æˆ– CPU åŸ·è¡Œã€‚\n",
    "device =torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# å»ºç«‹æ¨¡å‹ç‰©ä»¶\n",
    "# æŠŠæ¨¡å‹æ¬åˆ°æŒ‡å®šçš„è£ç½®ï¼ˆCPU / GPUï¼‰ä¸Š\n",
    "model = IrisMLP().to(device)\n",
    "print(model)\n",
    "print(f\"å¯è¨“ç·´åƒæ•¸é‡:{count_trainable_params(model):,}\")\n",
    "\n",
    "arch_txt =os.path.join(MODELS,\"model_arch.txt\")\n",
    "\n",
    "# åŠŸèƒ½ç¸½è¦½\n",
    "\n",
    "# æŠŠ æ¨¡å‹çš„çµæ§‹ å’Œ å¯è¨“ç·´åƒæ•¸ç¸½æ•¸\n",
    "# å¯«é€²ä¸€å€‹æ–‡å­—æª”ï¼ˆarch_txtï¼‰\n",
    "\n",
    "# ç”¨ Python å…§å»ºçš„æª”æ¡ˆæ“ä½œ\n",
    "# \"w\" â†’ ä»¥ã€Œå¯«å…¥æ¨¡å¼ã€é–‹å•Ÿæª”æ¡ˆ\n",
    "# encoding=\"utf-8\" â†’ ç”¨ UTF-8 ç·¨ç¢¼ï¼ˆå¯ä»¥æ­£å¸¸å¯«ä¸­æ–‡ï¼‰\n",
    "# as f â†’ å»ºç«‹æª”æ¡ˆç‰©ä»¶ f\n",
    "# with æœƒåœ¨å¯«å®Œå¾Œè‡ªå‹•é—œé–‰æª”æ¡ˆ\n",
    "\n",
    "with open(arch_txt,\"w\",encoding=\"utf-8\") as f:\n",
    "    f.write(str(model) + \"\\n\")\n",
    "    f.write(f\"trainable_params={count_trainable_params(model)}\\n\")\n",
    "print(f\"-> å·²å­˜çµæ§‹æè¿°:{arch_txt}\")\n",
    "print(\"STEP5 å®Œæˆ\") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyTorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
